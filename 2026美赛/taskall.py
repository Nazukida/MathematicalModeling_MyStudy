"""
============================================================
AI èŒä¸šæ¼”åŒ–ç»¼åˆæ¨¡å‹ - å®Œæ•´å·¥ä½œæµ
(AI Career Evolution Comprehensive Models - Complete Workflow)
============================================================
åŠŸèƒ½ï¼šå®ç°å››ä¸ªé«˜çº§æ¨¡å‹ç”¨äºAIå¯¹èŒä¸šçš„å½±å“åˆ†æ
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================

æ¨¡å‹æ¡†æ¶ï¼š
1. æ¨¡å‹ Iï¼šèŒä¸šæ¼”åŒ–åŠ¨æ€æ¨¡å‹ï¼ˆSD + è´å¶æ–¯ç½‘ç»œï¼‰
2. æ¨¡å‹ IIï¼šæ•™è‚²å†³ç­–ä¼˜åŒ–æ¨¡å‹ (AHP + MOEA/D)
3. æ¨¡å‹ IIIï¼šç»¼åˆæˆåŠŸè¯„ä»·æ¨¡å‹ï¼ˆæ¨¡ç³Šè¯„ä»· + ç›¸å…³æ€§åˆ†æï¼‰
4. æ¨¡å‹ IVï¼šæ³›åŒ–ä¸æ¨å¹¿æ¨¡å‹ (CBR + GWR)
============================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
from scipy.optimize import curve_fit
from scipy.integrate import odeint
import warnings
import os

warnings.filterwarnings('ignore')

# ============================================================
# å›¾è¡¨é…ç½®ï¼ˆå†…è”ç‰ˆæœ¬ï¼Œé¿å…å¯¼å…¥é—®é¢˜ï¼‰
# ============================================================

class PlotStyleConfig:
    """å›¾è¡¨ç¾åŒ–é…ç½®ç±»"""

    COLORS = {
        'primary': '#1f77b4',  # æ·±è“ - å†å²/åŸºå‡†
        'secondary': '#ff7f0e',  # æ©™è‰² - AIå½±å“/é¢„æµ‹
        'accent': '#2ca02c',  # ç»¿è‰² - æˆåŠŸ/çªå‡º
        'danger': '#d62728',  # çº¢è‰² - å±é™©/èµ·å§‹ç‚¹
        'neutral': '#7f7f7f',  # ç°è‰² - ä¸­æ€§
        'background': '#f8f9fa',  # ææµ…ç°èƒŒæ™¯
        'grid': '#e9ecef'  # æµ…ç°ç½‘æ ¼
    }

    PALETTE = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#d62728', '#ff9896']

    @staticmethod
    def setup_style(style='academic'):
        """è®¾ç½®å­¦æœ¯é£æ ¼"""
        plt.style.use('default')  # ä½¿ç”¨é»˜è®¤é£æ ¼ä½œä¸ºåŸºç¡€
        rcParams['font.family'] = 'DejaVu Sans'  # æˆ–è€… 'SimHei' å¦‚æœæ”¯æŒä¸­æ–‡
        rcParams['font.size'] = 12
        rcParams['axes.labelsize'] = 14
        rcParams['axes.titlesize'] = 16
        rcParams['xtick.labelsize'] = 12
        rcParams['ytick.labelsize'] = 12
        rcParams['legend.fontsize'] = 12
        rcParams['figure.titlesize'] = 18

        # ç½‘æ ¼å’ŒèƒŒæ™¯
        rcParams['axes.grid'] = True
        rcParams['grid.alpha'] = 0.3
        rcParams['axes.facecolor'] = PlotStyleConfig.COLORS['background']

    @staticmethod
    def get_palette(n=None):
        """è·å–è°ƒè‰²æ¿"""
        if n is None:
            return PlotStyleConfig.PALETTE
        return PlotStyleConfig.PALETTE[:n] if n <= len(PlotStyleConfig.PALETTE) else PlotStyleConfig.PALETTE * (n // len(PlotStyleConfig.PALETTE)) + PlotStyleConfig.PALETTE[:n % len(PlotStyleConfig.PALETTE)]


class FigureSaver:
    """å›¾è¡¨ä¿å­˜å·¥å…·ç±»"""

    def __init__(self, save_dir='./figures', format='png'):
        self.save_dir = save_dir
        self.format = format
        os.makedirs(save_dir, exist_ok=True)

    def save(self, fig, filename, formats=None, tight=True):
        if formats is None:
            formats = [self.format]
        if tight:
            plt.tight_layout()
        paths = []
        for fmt in formats:
            path = os.path.join(self.save_dir, f"{filename}.{fmt}")
            fig.savefig(path, dpi=300, bbox_inches='tight')
            paths.append(path)
        plt.close(fig)  # å…³é—­å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜
        return paths


# è®¾ç½®ç»˜å›¾é£æ ¼
PlotStyleConfig.setup_style('academic')


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹ I - èŒä¸šæ¼”åŒ–åŠ¨æ€æ¨¡å‹ï¼ˆSD + è´å¶æ–¯ç½‘ç»œï¼‰
# ============================================================

class SDParams:
    """
    ç³»ç»ŸåŠ¨åŠ›å­¦æ¨¡å‹å‚æ•°é…ç½®ç±»
    """

    def __init__(self, occupation_name='chef'):
        self.occupation_name = occupation_name

        # èŒä¸šç‰¹å®šå‚æ•°
        self.params = {
            'chef': {
                'alpha': 0.1,  # åˆ›é€ æ•ˆåº”ç³»æ•°
                'beta': 0.15,  # æ›¿ä»£æ•ˆåº”ç³»æ•° (å¨å¸ˆè¾ƒä½ï¼Œå› ä¸ºç‰©ç†æ“ä½œ)
                'gamma': 0.2,  # æƒé‡ç³»æ•°
                'initial_L': 100,  # åˆå§‹åŠ³åŠ¨åŠ›éœ€æ±‚
                'initial_T': 0.1,  # åˆå§‹æŠ€æœ¯æˆç†Ÿåº¦
                'initial_S': 0.8,  # åˆå§‹æŠ€èƒ½åŒ¹é…åº¦ (å¨å¸ˆè¾ƒé«˜)
                'initial_H': 0.9   # åˆå§‹äººç±»æ ¸å¿ƒç´ å…» (å¨å¸ˆè¾ƒé«˜)
            },
            'software_engineer': {
                'alpha': 0.2,
                'beta': 0.4,
                'gamma': 0.3,
                'initial_L': 150,
                'initial_T': 0.1,
                'initial_S': 0.9,
                'initial_H': 0.85
            },
            'graphic_designer': {
                'alpha': 0.15,
                'beta': 0.25,
                'gamma': 0.25,
                'initial_L': 80,
                'initial_T': 0.1,
                'initial_S': 0.85,
                'initial_H': 0.8
            }
        }

        # è®¾ç½®å½“å‰èŒä¸šå‚æ•°
        if self.occupation_name in self.params:
            p = self.params[self.occupation_name]
            self.alpha = p['alpha']
            self.beta = p['beta']
            self.gamma = p['gamma']
            self.initial_L = p['initial_L']
            self.initial_T = p['initial_T']
            self.initial_S = p['initial_S']
            self.initial_H = p['initial_H']
        else:
            self.alpha = 0.1
            self.beta = 0.15
            self.gamma = 0.2
            self.initial_L = 100
            self.initial_T = 0.1
            self.initial_S = 0.8
            self.initial_H = 0.9


class SDModel:
    """
    ç³»ç»ŸåŠ¨åŠ›å­¦æ¨¡å‹ç±»
    """

    def __init__(self, params: SDParams):
        self.params = params

    def system_dynamics(self, y, t):
        """
        ç³»ç»ŸåŠ¨åŠ›å­¦å¾®åˆ†æ–¹ç¨‹ (å¢å¼ºç‰ˆï¼Œé’ˆå¯¹ Gen-AI)

        dy/dt = f(y, t)

        y = [L, T, S, H]  # L: åŠ³åŠ¨åŠ›éœ€æ±‚, T: æŠ€æœ¯æˆç†Ÿåº¦, S: æŠ€èƒ½åŒ¹é…åº¦, H: äººç±»æ ¸å¿ƒç´ å…»
        """
        L, T, S, H = y
        p = self.params

        # åŠ³åŠ¨åŠ›éœ€æ±‚ï¼šå¼•å…¥é€»è¾‘æ–¯è°›å¢é•¿æ¨¡æ‹Ÿéçº¿æ€§å†²å‡»
        K = 200  # æ‰¿è½½å®¹é‡ (æœ€å¤§åŠ³åŠ¨åŠ›éœ€æ±‚)
        dL_dt = p.alpha * T * S * (1 - L/K) - p.beta * T * L

        # æŠ€æœ¯æˆç†Ÿåº¦ï¼šé€»è¾‘æ–¯è°›å¢é•¿
        dT_dt = p.gamma * T * (1 - T)

        # æŠ€èƒ½åŒ¹é…åº¦ï¼šéšæ—¶é—´æå‡ï¼Œä½†éšæŠ€æœ¯è¿›æ­¥ä¸‹é™
        dS_dt = 0.05 * (1 - S) - 0.1 * dT_dt

        # äººç±»æ ¸å¿ƒç´ å…»ï¼šåˆ›é€ åŠ›æº¢ä»· vs æŠ€èƒ½èç¼©
        creativity_premium = 0.1 * T * H  # åˆ›é€ åŠ›æº¢ä»·
        skill_atrophy = 0.05 * T * (1 - H)  # æŠ€èƒ½èç¼©
        dH_dt = creativity_premium - skill_atrophy

        return [dL_dt, dT_dt, dS_dt, dH_dt]

    def simulate(self, t_span):
        """
        æ¨¡æ‹Ÿç³»ç»ŸåŠ¨åŠ›å­¦

        :param t_span: æ—¶é—´è·¨åº¦
        :return: æ¨¡æ‹Ÿç»“æœ
        """
        y0 = [self.params.initial_L, self.params.initial_T, self.params.initial_S, self.params.initial_H]
        t = np.linspace(0, t_span, 100)
        sol = odeint(self.system_dynamics, y0, t)

        return {
            'time': t,
            'labor_demand': sol[:, 0],
            'tech_maturity': sol[:, 1],
            'skill_matching': sol[:, 2],
            'human_core_competence': sol[:, 3]
        }


class BNParams:
    """
    è´å¶æ–¯ç½‘ç»œå‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # æ¡ä»¶æ¦‚ç‡è¡¨ (CPT)
        # P(High_Impact | Tech_Breakthrough, Policy)
        self.cpt_high_impact = {
            ('True', 'Supportive'): 0.8,
            ('True', 'Neutral'): 0.6,
            ('True', 'Restrictive'): 0.3,
            ('False', 'Supportive'): 0.4,
            ('False', 'Neutral'): 0.2,
            ('False', 'Restrictive'): 0.1
        }

        # å…ˆéªŒæ¦‚ç‡
        self.prior_tech_breakthrough = 0.3
        self.prior_policy = {'Supportive': 0.4, 'Neutral': 0.4, 'Restrictive': 0.2}


class BNModel:
    """
    è´å¶æ–¯ç½‘ç»œæ¨¡å‹ç±»
    """

    def __init__(self, params: BNParams):
        self.params = params

    def compute_probability(self, tech_breakthrough, policy):
        """
        è®¡ç®—é«˜å†²å‡»æ¦‚ç‡

        :param tech_breakthrough: æŠ€æœ¯çªç ´ (True/False)
        :param policy: æ”¿ç­– ('Supportive'/'Neutral'/'Restrictive')
        :return: é«˜å†²å‡»æ¦‚ç‡
        """
        key = (str(tech_breakthrough), policy)
        return self.params.cpt_high_impact.get(key, 0.5)

    def predict_impact(self, scenarios):
        """
        é¢„æµ‹ä¸åŒæƒ…æ™¯ä¸‹çš„å†²å‡»

        :param scenarios: æƒ…æ™¯åˆ—è¡¨ [(tech, policy), ...]
        :return: é¢„æµ‹ç»“æœ
        """
        results = []
        for tech, policy in scenarios:
            prob = self.compute_probability(tech, policy)
            results.append({
                'tech_breakthrough': tech,
                'policy': policy,
                'high_impact_prob': prob
            })
        return results


class SD_BN_Model:
    """
    ç»¼åˆSD + BNæ¨¡å‹
    """

    def __init__(self, sd_params=None, bn_params=None):
        self.sd_params = sd_params or SDParams()
        self.bn_params = bn_params or BNParams()
        self.sd_model = SDModel(self.sd_params)
        self.bn_model = BNModel(self.bn_params)

    def run_simulation(self, t_span=10):
        """
        è¿è¡Œå®Œæ•´æ¨¡æ‹Ÿ

        :param t_span: æ—¶é—´è·¨åº¦
        :return: ç»“æœå­—å…¸
        """
        # SDæ¨¡æ‹Ÿ
        sd_results = self.sd_model.simulate(t_span)

        # BNæƒ…æ™¯åˆ†æ
        scenarios = [
            (True, 'Supportive'),
            (True, 'Neutral'),
            (True, 'Restrictive'),
            (False, 'Supportive'),
            (False, 'Neutral'),
            (False, 'Restrictive')
        ]
        bn_results = self.bn_model.predict_impact(scenarios)

        return {
            'sd_results': sd_results,
            'bn_results': bn_results,
            'occupation': self.sd_params.occupation_name
        }


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹ II - æ•™è‚²å†³ç­–ä¼˜åŒ–æ¨¡å‹ (AHP + MOEA/D)
# ============================================================

class AHPParams:
    """
    AHPå‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # åˆ¤æ–­çŸ©é˜µ (é’ˆå¯¹å››ä¸ªå‡†åˆ™: å°±ä¸šç«äº‰åŠ›, æŠ€è‰ºç‹¬ç‰¹æ€§, æ•™å­¦æˆæœ¬, ä¼¦ç†åˆè§„æ€§)
        # å‡†åˆ™: C1=å°±ä¸šç«äº‰åŠ›, C2=æŠ€è‰ºç‹¬ç‰¹æ€§, C3=æ•™å­¦æˆæœ¬, C4=ä¼¦ç†åˆè§„æ€§
        self.judgment_matrix = np.array([
            [1, 3, 1/2, 2],    # C1 vs others
            [1/3, 1, 1/4, 1/2], # C2 vs others
            [2, 4, 1, 3],      # C3 vs others
            [1/2, 2, 1/3, 1]   # C4 vs others
        ])

        self.criteria = ['Employment Competitiveness', 'Artistic Uniqueness', 'Teaching Cost', 'Ethical Compliance']


class AHPModel:
    """
    AHPæ¨¡å‹ç±»
    """

    def __init__(self, params: AHPParams):
        self.params = params

    def calculate_weights(self):
        """
        è®¡ç®—å‡†åˆ™æƒé‡

        :return: æƒé‡å‘é‡å’Œä¸€è‡´æ€§æ¯”ç‡
        """
        A = self.params.judgment_matrix
        n = A.shape[0]

        # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        eigenvals, eigenvecs = np.linalg.eig(A)
        max_eigenval = np.max(eigenvals.real)
        weights = eigenvecs[:, np.argmax(eigenvals.real)].real
        weights = weights / np.sum(weights)

        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        CI = (max_eigenval - n) / (n - 1)
        RI = [0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45][n-1]  # RIè¡¨
        CR = CI / RI if RI > 0 else 0

        return weights, CR

    def sensitivity_analysis(self, perturbation=0.1):
        """
        AHPæƒé‡çµæ•åº¦åˆ†æ

        :param perturbation: æƒé‡æ³¢åŠ¨å¹…åº¦ (Â±10%)
        :return: çµæ•åº¦åˆ†æç»“æœ
        """
        original_weights, original_CR = self.calculate_weights()
        results = []

        for i in range(len(original_weights)):
            # å¢åŠ æƒé‡
            perturbed_matrix = self.params.judgment_matrix.copy()
            perturbed_matrix[i, :] *= (1 + perturbation)
            perturbed_matrix[:, i] /= (1 + perturbation)

            # é‡æ–°è®¡ç®—æƒé‡
            A = perturbed_matrix
            n = A.shape[0]
            eigenvals, eigenvecs = np.linalg.eig(A)
            max_eigenval = np.max(eigenvals.real)
            new_weights = eigenvecs[:, np.argmax(eigenvals.real)].real
            new_weights = new_weights / np.sum(new_weights)
            CI = (max_eigenval - n) / (n - 1)
            RI = [0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45][n-1]
            new_CR = CI / RI if RI > 0 else 0

            weight_change = np.abs(new_weights - original_weights)
            max_change = np.max(weight_change)

            results.append({
                'criterion': self.params.criteria[i],
                'original_weight': original_weights[i],
                'new_weight': new_weights.tolist(),
                'max_weight_change': max_change,
                'new_CR': new_CR
            })

        return results


class MOEADParams:
    """
    MOEA/Då‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        self.population_size = 50
        self.max_generations = 10  # å‡å°‘ä»£æ•°ä»¥ä¾¿å±•ç¤º
        self.neighborhood_size = 10
        self.crossover_rate = 0.8
        self.mutation_rate = 0.1

        # ç›®æ ‡å‡½æ•°æƒé‡
        self.weights = np.random.rand(self.population_size, 3)  # ä¸‰ä¸ªç›®æ ‡
        self.weights = self.weights / np.sum(self.weights, axis=1, keepdims=True)


class MOEADModel:
    """
    MOEA/Då¤šç›®æ ‡ä¼˜åŒ–æ¨¡å‹
    """

    def __init__(self, params: MOEADParams):
        self.params = params

    def objective_functions(self, x):
        """
        ç›®æ ‡å‡½æ•°

        :param x: å†³ç­–å˜é‡ [basic_enrollment, ai_enrollment, course_reform_rate]
        :return: ç›®æ ‡å€¼ [f1, f2, f3]
        """
        basic_enrollment, ai_enrollment, course_reform_rate = x

        # f1: å°±ä¸šç‡ç›®æ ‡ (æœ€å¤§åŒ–)
        f1 = - (0.5 * basic_enrollment + 0.7 * ai_enrollment + 0.3 * course_reform_rate)

        # f2: è½¬å‹æˆæœ¬ç›®æ ‡ (æœ€å°åŒ–)
        f2 = 0.4 * basic_enrollment + 0.6 * ai_enrollment + 0.5 * course_reform_rate

        # f3: ç¢³è¶³è¿¹/ç¯å¢ƒå½±å“ç›®æ ‡ (æœ€å°åŒ–ï¼ŒAIå¯é™ä½é£Ÿææµªè´¹)
        f3 = 0.3 * basic_enrollment - 0.2 * ai_enrollment - 0.1 * course_reform_rate

        return [f1, f2, f3]

    def optimize(self):
        """
        æ‰§è¡ŒMOEA/Dä¼˜åŒ–

        :return: å¸•ç´¯æ‰˜å‰æ²¿å’Œæ¼”åŒ–å†å²
        """
        # åˆå§‹åŒ–ç§ç¾¤
        population = []
        for i in range(self.params.population_size):
            x = np.random.rand(3)  # [basic_enrollment, ai_enrollment, course_reform_rate]
            f = self.objective_functions(x)
            population.append({'x': x, 'f': f})

        evolution_history = [population.copy()]

        # è¿›åŒ–è¿‡ç¨‹
        for gen in range(self.params.max_generations):
            new_population = []
            for i, ind in enumerate(population):
                # é€‰æ‹©é‚»åŸŸ
                neighbors = self._get_neighbors(i, population)

                # äº¤å‰
                if np.random.rand() < self.params.crossover_rate:
                    parent1 = ind
                    parent2 = np.random.choice(neighbors)
                    offspring_x = self._crossover(parent1['x'], parent2['x'])
                else:
                    offspring_x = ind['x'].copy()

                # å˜å¼‚
                offspring_x = self._mutate(offspring_x)

                # è¯„ä¼°
                offspring_f = self.objective_functions(offspring_x)
                offspring = {'x': offspring_x, 'f': offspring_f}

                # æ›´æ–°é‚»åŸŸ
                for neighbor in neighbors:
                    if self._dominates(offspring, neighbor):
                        neighbor.update(offspring)

                new_population.append(offspring)

            population = new_population
            evolution_history.append(population.copy())

        # æå–å¸•ç´¯æ‰˜å‰æ²¿
        pareto_front = []
        for ind in population:
            dominated = False
            for other in population:
                if self._dominates(other, ind):
                    dominated = True
                    break
            if not dominated:
                pareto_front.append(ind)

        return pareto_front, evolution_history

    def _get_neighbors(self, index, population):
        """è·å–é‚»åŸŸä¸ªä½“"""
        start = max(0, index - self.params.neighborhood_size // 2)
        end = min(len(population), index + self.params.neighborhood_size // 2 + 1)
        neighbors = population[start:end]
        if len(neighbors) < self.params.neighborhood_size:
            neighbors.extend(population[:self.params.neighborhood_size - len(neighbors)])
        return neighbors

    def _crossover(self, x1, x2):
        """ç®€å•äº¤å‰"""
        alpha = np.random.rand()
        return alpha * x1 + (1 - alpha) * x2

    def _mutate(self, x):
        """ç®€å•å˜å¼‚"""
        for i in range(len(x)):
            if np.random.rand() < self.params.mutation_rate:
                x[i] += np.random.normal(0, 0.1)
                x[i] = np.clip(x[i], 0, 1)  # ä¿æŒåœ¨[0,1]èŒƒå›´å†…
        return x

    def _dominates(self, ind1, ind2):
        """æ£€æŸ¥ind1æ˜¯å¦æ”¯é…ind2"""
        better_or_equal = all(f1 <= f2 for f1, f2 in zip(ind1['f'], ind2['f']))
        strictly_better = any(f1 < f2 for f1, f2 in zip(ind1['f'], ind2['f']))
        return better_or_equal and strictly_better


class AHP_MOEAD_Model:
    """
    ç»¼åˆAHP + MOEA/Dæ¨¡å‹
    """

    def __init__(self, ahp_params=None, moead_params=None):
        self.ahp_params = ahp_params or AHPParams()
        self.moead_params = moead_params or MOEADParams()
        self.ahp_model = AHPModel(self.ahp_params)
        self.moead_model = MOEADModel(self.moead_params)

    def run_optimization(self):
        """
        è¿è¡Œå®Œæ•´ä¼˜åŒ–

        :return: ç»“æœå­—å…¸
        """
        # AHPæƒé‡è®¡ç®—
        weights, CR = self.ahp_model.calculate_weights()

        # AHPçµæ•åº¦åˆ†æ
        sensitivity_results = self.ahp_model.sensitivity_analysis()

        # MOEA/Dä¼˜åŒ–
        pareto_front, evolution_history = self.moead_model.optimize()

        # è†ç›–ç‚¹åˆ†æ
        knee_point = self._find_knee_point(pareto_front)

        return {
            'ahp_weights': weights,
            'ahp_CR': CR,
            'sensitivity_results': sensitivity_results,
            'pareto_front': pareto_front,
            'evolution_history': evolution_history,
            'knee_point': knee_point,
            'criteria': self.ahp_params.criteria
        }

    def _find_knee_point(self, pareto_front):
        """
        å¯»æ‰¾å¸•ç´¯æ‰˜å‰æ²¿çš„è†ç›–ç‚¹

        :param pareto_front: å¸•ç´¯æ‰˜å‰æ²¿
        :return: è†ç›–ç‚¹ç´¢å¼•å’Œå…·ä½“å»ºè®®
        """
        if len(pareto_front) < 3:
            return None

        # è®¡ç®—æ¯ä¸ªç‚¹çš„"è†ç›–åº¦" (ä¸ç†æƒ³ç‚¹çš„è·ç¦» + ä¸é‚»ç‚¹çš„è§’åº¦)
        points = np.array([ind['f'] for ind in pareto_front])
        ideal_point = np.min(points, axis=0)

        max_distances = np.max(points - ideal_point, axis=0)
        normalized_points = (points - ideal_point) / max_distances

        knee_scores = []
        for i, point in enumerate(normalized_points):
            # è®¡ç®—åˆ°ç†æƒ³ç‚¹çš„è·ç¦»
            distance = np.linalg.norm(point)

            # è®¡ç®—ä¸å‰åç‚¹çš„è§’åº¦å˜åŒ–
            if i == 0:
                prev_vector = point - normalized_points[i+1]
            elif i == len(normalized_points) - 1:
                prev_vector = normalized_points[i-1] - point
            else:
                prev_vector = normalized_points[i-1] - point

            if i == 0:
                next_vector = normalized_points[i+1] - point
            elif i == len(normalized_points) - 1:
                next_vector = point - normalized_points[i-1]
            else:
                next_vector = point - normalized_points[i+1]

            # è®¡ç®—è§’åº¦
            cos_angle = np.dot(prev_vector, next_vector) / (np.linalg.norm(prev_vector) * np.linalg.norm(next_vector))
            angle = np.arccos(np.clip(cos_angle, -1, 1))

            # è†ç›–åº¦ = è·ç¦» + è§’åº¦æƒé‡
            knee_score = distance + 0.5 * angle
            knee_scores.append(knee_score)

        knee_index = np.argmin(knee_scores)
        knee_solution = pareto_front[knee_index]

        # ç”Ÿæˆæ”¿ç­–å»ºè®®
        basic_enrollment, ai_enrollment, course_reform_rate = knee_solution['x']
        policy_recommendation = f"""
        å»ºè®®æŸå¨å¸ˆå­¦æ ¡æŠ•å…¥:
        - ä¼ ç»ŸæŠ€è‰ºæ•™å­¦é¢„ç®—: {basic_enrollment*100:.0f}%
        - AIç›¸å…³æ•™å­¦é¢„ç®—: {ai_enrollment*100:.0f}%
        - è¯¾ç¨‹æ”¹é©æŠ•å…¥: {course_reform_rate*100:.0f}%

        è¿™å°†å®ç°å°±ä¸šç‡æå‡ {(1-knee_solution['f'][0])*100:.1f}%,
        è½¬å‹æˆæœ¬æ§åˆ¶åœ¨ {knee_solution['f'][1]:.2f},
        ç¯å¢ƒå½±å“æœ€å°åŒ–è‡³ {knee_solution['f'][2]:.2f}
        """

        return {
            'index': knee_index,
            'solution': knee_solution,
            'policy_recommendation': policy_recommendation.strip()
        }


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹ III - ç»¼åˆæˆåŠŸè¯„ä»·æ¨¡å‹ï¼ˆæ¨¡ç³Šè¯„ä»· + ç›¸å…³æ€§åˆ†æï¼‰
# ============================================================

class FCEParams:
    """
    æ¨¡ç³Šç»¼åˆè¯„ä»·å‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # è¯„ä»·é›† U = {ä¼˜ç§€, è‰¯å¥½, ä¸­ç­‰, è¾ƒå·®}
        self.evaluation_set = ['ä¼˜ç§€', 'è‰¯å¥½', 'ä¸­ç­‰', 'è¾ƒå·®']

        # æƒé‡å‘é‡ (é’ˆå¯¹å‡†åˆ™: ä¼¦ç†ç´ å…», å®¡ç¾èƒ½åŠ›, æŠ€æœ¯æŠ€èƒ½, å°±ä¸šå‰æ™¯, äººç±»è‡ªä¸»æ€§, æ–‡åŒ–ä¼ æ‰¿)
        self.weights = np.array([0.2, 0.15, 0.2, 0.15, 0.15, 0.15])

        # éš¶å±åº¦çŸ©é˜µç¤ºä¾‹ (é’ˆå¯¹ä¸åŒæ”¿ç­–)
        self.membership_matrix = {
            'ç¦ç”¨AI': np.array([
                [0.8, 0.15, 0.05, 0.0],   # ä¼¦ç†ç´ å…»
                [0.7, 0.2, 0.1, 0.0],     # å®¡ç¾èƒ½åŠ›
                [0.1, 0.3, 0.4, 0.2],     # æŠ€æœ¯æŠ€èƒ½
                [0.2, 0.4, 0.3, 0.1],     # å°±ä¸šå‰æ™¯
                [0.9, 0.08, 0.02, 0.0],   # äººç±»è‡ªä¸»æ€§
                [0.8, 0.15, 0.05, 0.0]    # æ–‡åŒ–ä¼ æ‰¿
            ]),
            'å…¨å‘˜æ‹¥æŠ±AI': np.array([
                [0.1, 0.2, 0.4, 0.3],    # ä¼¦ç†ç´ å…»
                [0.2, 0.3, 0.3, 0.2],    # å®¡ç¾èƒ½åŠ›
                [0.9, 0.08, 0.02, 0.0],  # æŠ€æœ¯æŠ€èƒ½
                [0.8, 0.15, 0.05, 0.0],  # å°±ä¸šå‰æ™¯
                [0.3, 0.3, 0.3, 0.1],    # äººç±»è‡ªä¸»æ€§
                [0.2, 0.3, 0.3, 0.2]     # æ–‡åŒ–ä¼ æ‰¿
            ])
        }


class FCEModel:
    """
    æ¨¡ç³Šç»¼åˆè¯„ä»·æ¨¡å‹
    """

    def __init__(self, params: FCEParams):
        self.params = params

    def evaluate_policy(self, policy_name):
        """
        è¯„ä»·ç‰¹å®šæ”¿ç­–

        :param policy_name: æ”¿ç­–åç§°
        :return: ç»¼åˆè¯„ä»·å‘é‡
        """
        if policy_name not in self.params.membership_matrix:
            raise ValueError(f"Policy {policy_name} not found")

        R = self.params.membership_matrix[policy_name]
        W = self.params.weights

        # ç»¼åˆè¯„ä»·: B = W * R
        B = np.dot(W, R)

        return B


class CorrelationParams:
    """
    ç›¸å…³æ€§åˆ†æå‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # æ¨¡æ‹Ÿæ•°æ®: AIèå…¥åº¦ vs æ ¸å¿ƒç«äº‰åŠ›
        np.random.seed(42)
        self.ai_integration = np.random.rand(100) * 100  # 0-100%
        self.core_competence = 50 + 0.5 * self.ai_integration + np.random.normal(0, 10, 100)


class CorrelationModel:
    """
    ç›¸å…³æ€§åˆ†ææ¨¡å‹
    """

    def __init__(self, params: CorrelationParams):
        self.params = params

    def analyze_correlation(self):
        """
        åˆ†æAIèå…¥åº¦ä¸æ ¸å¿ƒç«äº‰åŠ›çš„ç›¸å…³æ€§

        :return: ç›¸å…³ç³»æ•°å’Œpå€¼
        """
        from scipy.stats import pearsonr

        corr, p_value = pearsonr(self.params.ai_integration, self.params.core_competence)

        return {
            'correlation_coefficient': corr,
            'p_value': p_value,
            'ai_integration': self.params.ai_integration,
            'core_competence': self.params.core_competence
        }


class FCE_Correlation_Model:
    """
    ç»¼åˆæ¨¡ç³Šè¯„ä»· + ç›¸å…³æ€§åˆ†ææ¨¡å‹
    """

    def __init__(self, fce_params=None, corr_params=None, ahp_weights=None):
        self.fce_params = fce_params or FCEParams()
        self.corr_params = corr_params or CorrelationParams()
        self.fce_model = FCEModel(self.fce_params)
        self.corr_model = CorrelationModel(self.corr_params)
        # ä»AHPè·å–æƒé‡ï¼Œå½¢æˆé—­ç¯ (æ³¨é‡Šæ‰ï¼Œå› ä¸ºFCEç°åœ¨æœ‰6ä¸ªå‡†åˆ™ï¼Œè€ŒAHPæœ‰4ä¸ª)
        # if ahp_weights is not None:
        #     self.fce_params.weights = ahp_weights[:4]  # å–å‰å››ä¸ªæƒé‡å¯¹åº”FCEçš„å››ä¸ªå‡†åˆ™

    def run_evaluation(self):
        """
        è¿è¡Œå®Œæ•´è¯„ä»·

        :return: ç»“æœå­—å…¸
        """
        # æ¨¡ç³Šè¯„ä»·
        policies = list(self.fce_params.membership_matrix.keys())
        fce_results = {}
        for policy in policies:
            fce_results[policy] = self.fce_model.evaluate_policy(policy)

        # ç›¸å…³æ€§åˆ†æ
        corr_results = self.corr_model.analyze_correlation()

        return {
            'fce_results': fce_results,
            'corr_results': corr_results,
            'evaluation_set': self.fce_params.evaluation_set,
            'weights': self.fce_params.weights
        }


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹ IV - æ³›åŒ–ä¸æ¨å¹¿æ¨¡å‹ (CBR + GWR)
# ============================================================

class CBRParams:
    """
    æ¡ˆä¾‹æ¨ç†å‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # æ¡ˆä¾‹åº“
        self.case_base = [
            {
                'id': 1,
                'name': 'CIA Culinary Institute',
                'digital_level': 0.7,
                'budget_per_student': 50000,
                'ai_integration': 0.3,
                'outcome': 'success'
            },
            {
                'id': 2,
                'name': 'Cloud Kitchen Startup',
                'digital_level': 0.9,
                'budget_per_student': 20000,
                'ai_integration': 0.8,
                'outcome': 'success'
            },
            {
                'id': 3,
                'name': 'Michelin Traditional Restaurant',
                'digital_level': 0.4,
                'budget_per_student': 10000,
                'ai_integration': 0.1,
                'outcome': 'moderate'
            },
            {
                'id': 4,
                'name': 'Silicon Valley Tech-Focused Culinary School',
                'digital_level': 0.95,
                'budget_per_student': 60000,
                'ai_integration': 0.9,
                'outcome': 'success'
            }
        ]


class CBRModel:
    """
    æ¡ˆä¾‹æ¨ç†æ¨¡å‹
    """

    def __init__(self, params: CBRParams):
        self.params = params

    def calculate_similarity(self, query_case, base_case):
        """
        è®¡ç®—ç›¸ä¼¼åº¦

        :param query_case: æŸ¥è¯¢æ¡ˆä¾‹
        :param base_case: åŸºå‡†æ¡ˆä¾‹
        :return: ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # ç®€å•æ¬§å‡ é‡Œå¾—è·ç¦»
        features = ['digital_level', 'budget_per_student', 'ai_integration']
        distance = 0
        for feature in features:
            distance += (query_case[feature] - base_case[feature]) ** 2
        similarity = 1 / (1 + np.sqrt(distance))
        return similarity

    def retrieve_similar_cases(self, query_case, top_k=2):
        """
        æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹

        :param query_case: æŸ¥è¯¢æ¡ˆä¾‹
        :param top_k: è¿”å›æœ€ç›¸ä¼¼æ¡ˆä¾‹æ•°é‡
        :return: ç›¸ä¼¼æ¡ˆä¾‹åˆ—è¡¨
        """
        similarities = []
        for case in self.params.case_base:
            sim = self.calculate_similarity(query_case, case)
            similarities.append((case, sim))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]


class GWRParams:
    """
    åœ°ç†åŠ æƒå›å½’å‚æ•°é…ç½®ç±»
    """

    def __init__(self):
        # çœŸå®åŸå¸‚ç»çº¬åº¦æ•°æ® (ç¤ºä¾‹)
        self.cities = {
            'New York': {'lat': 40.7128, 'lon': -74.0060},
            'Paris': {'lat': 48.8566, 'lon': 2.3522},
            'Tokyo': {'lat': 35.6762, 'lon': 139.6503},
            'London': {'lat': 51.5074, 'lon': -0.1278},
            'Beijing': {'lat': 39.9042, 'lon': 116.4074},
            'San Francisco': {'lat': 37.7749, 'lon': -122.4194},
            'Berlin': {'lat': 52.5200, 'lon': 13.4050},
            'Sydney': {'lat': -33.8688, 'lon': 151.2093}
        }

        # è½¬æ¢ä¸ºåæ ‡æ•°ç»„
        self.coordinates = np.array([[city['lat'], city['lon']] for city in self.cities.values()])
        self.city_names = list(self.cities.keys())

        # æ¨¡æ‹ŸAIèå…¥åº¦å’Œæœ¬åœ°å‚æ•°
        np.random.seed(42)
        self.ai_integration = np.random.rand(len(self.cities)) * 100
        self.local_parameters = np.random.rand(len(self.cities), 3)


class GWRModel:
    """
    åœ°ç†åŠ æƒå›å½’æ¨¡å‹
    """

    def __init__(self, params: GWRParams):
        self.params = params

    def local_regression(self, target_point, bandwidth=10):
        """
        å±€éƒ¨å›å½’

        :param target_point: ç›®æ ‡ç‚¹åæ ‡
        :param bandwidth: å¸¦å®½
        :return: å±€éƒ¨å‚æ•°
        """
        # è®¡ç®—æƒé‡ (é«˜æ–¯æ ¸)
        distances = np.linalg.norm(self.params.coordinates - target_point, axis=1)
        weights = np.exp(-distances ** 2 / (2 * bandwidth ** 2))
        weights = weights / np.sum(weights)

        # åŠ æƒå›å½’ (ç®€åŒ–)
        X = np.column_stack([np.ones(len(weights)), self.params.ai_integration])
        y = self.params.local_parameters[:, 0]  # ç¤ºä¾‹ç›®æ ‡å˜é‡

        # åŠ æƒæœ€å°äºŒä¹˜
        W = np.diag(weights)
        beta = np.linalg.inv(X.T @ W @ X) @ X.T @ W @ y

        return beta

    def predict_local_parameters(self, new_points):
        """
        é¢„æµ‹æ–°ç‚¹çš„å±€éƒ¨å‚æ•°

        :param new_points: æ–°ç‚¹åæ ‡
        :return: é¢„æµ‹å‚æ•°
        """
        predictions = []
        for point in new_points:
            beta = self.local_regression(point)
            predictions.append(beta)
        return np.array(predictions)


class CBR_GWR_Model:
    """
    ç»¼åˆCBR + GWRæ¨¡å‹
    """

    def __init__(self, cbr_params=None, gwr_params=None):
        self.cbr_params = cbr_params or CBRParams()
        self.gwr_params = gwr_params or GWRParams()
        self.cbr_model = CBRModel(self.cbr_params)
        self.gwr_model = GWRModel(self.gwr_params)

    def generalize_solution(self, query_case, new_locations):
        """
        æ³›åŒ–è§£å†³æ–¹æ¡ˆ

        :param query_case: æŸ¥è¯¢æ¡ˆä¾‹
        :param new_locations: æ–°ä½ç½®åæ ‡
        :return: æ³›åŒ–ç»“æœ
        """
        # CBRæ£€ç´¢
        similar_cases = self.cbr_model.retrieve_similar_cases(query_case)

        # GWRé¢„æµ‹
        local_params = self.gwr_model.predict_local_parameters(new_locations)

        return {
            'similar_cases': similar_cases,
            'local_parameters': local_params,
            'query_case': query_case,
            'new_locations': new_locations
        }


# ============================================================
# å¯è§†åŒ–æ¨¡å—
# ============================================================

class ComprehensiveVisualization:
    """
    ç»¼åˆæ¨¡å‹å¯è§†åŒ–ç±»
    """

    def __init__(self, save_dir='./figures'):
        self.saver = FigureSaver(save_dir)

    def plot_sd_results(self, results):
        """
        ç»˜åˆ¶SDæ¨¡å‹ç»“æœ
        """
        fig, axes = plt.subplots(2, 2, figsize=(18, 10))

        sd_res = results['sd_results']
        axes[0,0].plot(sd_res['time'], sd_res['labor_demand'], 'b-', linewidth=2)
        axes[0,0].set_title('Labor Demand Evolution')
        axes[0,0].set_xlabel('Time')
        axes[0,0].set_ylabel('Labor Demand')
        axes[0,0].grid(True)

        axes[0,1].plot(sd_res['time'], sd_res['tech_maturity'], 'r-', linewidth=2)
        axes[0,1].set_title('Technology Maturity Evolution')
        axes[0,1].set_xlabel('Time')
        axes[0,1].set_ylabel('Technology Maturity')
        axes[0,1].grid(True)

        axes[1,0].plot(sd_res['time'], sd_res['skill_matching'], 'g-', linewidth=2)
        axes[1,0].set_title('Skill Matching Degree')
        axes[1,0].set_xlabel('Time')
        axes[1,0].set_ylabel('Skill Matching')
        axes[1,0].grid(True)

        axes[1,1].plot(sd_res['time'], sd_res['human_core_competence'], 'm-', linewidth=2)
        axes[1,1].set_title('Human Core Competence Evolution')
        axes[1,1].set_xlabel('Time')
        axes[1,1].set_ylabel('Human Core Competence')
        axes[1,1].grid(True)

        plt.suptitle(f'System Dynamics Model - {results["occupation"]}')
        paths = self.saver.save(fig, 'sd_model_results')
        print(f"SD model visualization saved: {paths[0]}")

    def plot_bn_results(self, results):
        """
        ç»˜åˆ¶BNæ¨¡å‹ç»“æœ
        """
        fig, ax = plt.subplots(figsize=(10, 6))

        bn_res = results['bn_results']
        scenarios = [f"{r['tech_breakthrough']}_{r['policy']}" for r in bn_res]
        probs = [r['high_impact_prob'] for r in bn_res]

        ax.bar(scenarios, probs, color='skyblue')
        ax.set_title('Bayesian Network - Impact Probability by Scenario')
        ax.set_ylabel('High Impact Probability')
        ax.tick_params(axis='x', rotation=45)

        paths = self.saver.save(fig, 'bn_model_results')
        print(f"BN model visualization saved: {paths[0]}")

    def plot_ahp_weights(self, results):
        """
        ç»˜åˆ¶AHPæƒé‡
        """
        fig, ax = plt.subplots(figsize=(8, 6))

        weights = results['ahp_weights']
        criteria = results['criteria']

        ax.bar(criteria, weights, color='lightgreen')
        ax.set_title(f'AHP Weights (CR = {results["ahp_CR"]:.3f})')
        ax.set_ylabel('Weight')
        ax.tick_params(axis='x', rotation=45)

        paths = self.saver.save(fig, 'ahp_weights')
        print(f"AHP weights visualization saved: {paths[0]}")

    def plot_pareto_front(self, results):
        """
        ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿
        """
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')

        front = results['pareto_front']
        f1_vals = [ind['f'][0] for ind in front]
        f2_vals = [ind['f'][1] for ind in front]
        f3_vals = [ind['f'][2] for ind in front]

        ax.scatter(f1_vals, f2_vals, f3_vals, c='red', s=50, alpha=0.7)
        ax.set_title('MOEA/D Pareto Front (3D)')
        ax.set_xlabel('Objective 1 (Employment Rate)')
        ax.set_ylabel('Objective 2 (Transition Cost)')
        ax.set_zlabel('Objective 3 (Carbon Footprint)')

        paths = self.saver.save(fig, 'pareto_front')
        print(f"Pareto front visualization saved: {paths[0]}")

    def plot_evolution_process(self, results):
        """
        ç»˜åˆ¶æ¼”åŒ–è¿‡ç¨‹
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        history = results['evolution_history']

        for gen in [0, len(history)//4, len(history)//2, 3*len(history)//4, -1]:
            pop = history[gen]
            f1 = [ind['f'][0] for ind in pop]
            f2 = [ind['f'][1] for ind in pop]
            f3 = [ind['f'][2] for ind in pop]

            row = gen // (len(history)//2)
            col = gen % 3

            axes[row, col].scatter(f1, f2, alpha=0.6)
            axes[row, col].set_title(f'Generation {gen}')
            axes[row, col].set_xlabel('Obj1')
            axes[row, col].set_ylabel('Obj2')
            axes[row, col].grid(True)

        plt.suptitle('MOEA/D Evolution Process')
        paths = self.saver.save(fig, 'evolution_process')
        print(f"Evolution process visualization saved: {paths[0]}")

    def plot_fce_results(self, results):
        """
        ç»˜åˆ¶æ¨¡ç³Šè¯„ä»·ç»“æœ
        """
        fig, ax = plt.subplots(figsize=(10, 6))

        fce_res = results['fce_results']
        eval_set = results['evaluation_set']

        policies = list(fce_res.keys())
        for i, policy in enumerate(policies):
            ax.bar(np.arange(len(eval_set)) + i*0.2, fce_res[policy],
                   width=0.2, label=policy, alpha=0.7)

        ax.set_xticks(np.arange(len(eval_set)) + 0.2)
        ax.set_xticklabels(eval_set)
        ax.set_title('Fuzzy Comprehensive Evaluation')
        ax.set_ylabel('Membership Degree')
        ax.legend()

        paths = self.saver.save(fig, 'fce_results')
        print(f"FCE results visualization saved: {paths[0]}")

    def plot_correlation_results(self, results):
        """
        ç»˜åˆ¶ç›¸å…³æ€§åˆ†æç»“æœ
        """
        fig, ax = plt.subplots(figsize=(8, 6))

        corr_res = results['corr_results']
        ax.scatter(corr_res['ai_integration'], corr_res['core_competence'], alpha=0.6)
        ax.set_title(f'Correlation Analysis (r = {corr_res["correlation_coefficient"]:.3f})')
        ax.set_xlabel('AI Integration Degree (%)')
        ax.set_ylabel('Core Competence')
        ax.grid(True)

        paths = self.saver.save(fig, 'correlation_analysis')
        print(f"Correlation analysis visualization saved: {paths[0]}")

    def plot_gwr_spatial_sensitivity(self, results):
        """
        ç»˜åˆ¶GWRç©ºé—´æ•æ„Ÿåº¦åˆ†å¸ƒ
        """
        fig, ax = plt.subplots(figsize=(10, 6))
        locations = results['new_locations']
        sensitivities = results['local_parameters'][:, 1]  # å‡è®¾ç¬¬äºŒåˆ—æ˜¯AIæ•æ„Ÿåº¦

        sc = ax.scatter(locations[:, 1], locations[:, 0], c=sensitivities,
                        cmap='viridis', s=100, edgecolor='black')
        plt.colorbar(sc, label='AI Sensitivity Coefficient')
        ax.set_title('Spatial Distribution of AI Integration Impact')
        ax.set_xlabel('Longitude')
        ax.set_ylabel('Latitude')
        ax.grid(True)

        paths = self.saver.save(fig, 'gwr_spatial_sensitivity')
        print(f"GWR spatial sensitivity visualization saved: {paths[0]}")


def sensitivity_analysis(sd_model, param_name, param_range, t_span=10):
    """
    çµæ•åº¦åˆ†æ

    :param sd_model: SDæ¨¡å‹å®ä¾‹
    :param param_name: å‚æ•°åç§°
    :param param_range: å‚æ•°èŒƒå›´
    :param t_span: æ—¶é—´è·¨åº¦
    :return: åˆ†æç»“æœ
    """
    results = []
    original_value = getattr(sd_model.params, param_name)

    for value in param_range:
        setattr(sd_model.params, param_name, value)
        sim_result = sd_model.simulate(t_span)
        results.append({
            'param_value': value,
            'final_labor_demand': sim_result['labor_demand'][-1],
            'final_tech_maturity': sim_result['tech_maturity'][-1],
            'final_skill_matching': sim_result['skill_matching'][-1]
        })

    # æ¢å¤åŸå§‹å€¼
    setattr(sd_model.params, param_name, original_value)

    return results

def run_comprehensive_models():
    """
    è¿è¡Œæ‰€æœ‰å››ä¸ªæ¨¡å‹çš„å®Œæ•´å·¥ä½œæµ
    """
    print("\n" + "â–ˆ"*70)
    print("â–ˆ" + " "*15 + "AIèŒä¸šæ¼”åŒ–ç»¼åˆæ¨¡å‹" + " "*16 + "â–ˆ")
    print("â–ˆ" + " "*10 + "Comprehensive AI Career Evolution Models" + " "*11 + "â–ˆ")
    print("â–ˆ"*70 + "\n")

    # åˆ›å»ºfiguresç›®å½•
    figures_dir = './figures'
    os.makedirs(figures_dir, exist_ok=True)

    viz = ComprehensiveVisualization(save_dir=figures_dir)

    # ========== æ¨¡å‹ I: SD + BN ==========
    print("ã€Model Iã€‘èŒä¸šæ¼”åŒ–åŠ¨æ€æ¨¡å‹ (SD + BN)...")
    sd_bn_model = SD_BN_Model()
    sd_bn_results = sd_bn_model.run_simulation()

    print("  ğŸ“Š ç»˜åˆ¶SDæ¨¡å‹ç»“æœ...")
    viz.plot_sd_results(sd_bn_results)

    print("  ğŸ“Š ç»˜åˆ¶BNæ¨¡å‹ç»“æœ...")
    viz.plot_bn_results(sd_bn_results)

    # ========== æ¨¡å‹ II: AHP + MOEA/D ==========
    print("\nã€Model IIã€‘æ•™è‚²å†³ç­–ä¼˜åŒ–æ¨¡å‹ (AHP + MOEA/D)...")
    ahp_moead_model = AHP_MOEAD_Model()
    ahp_moead_results = ahp_moead_model.run_optimization()

    print("  ğŸ“Š ç»˜åˆ¶AHPæƒé‡...")
    viz.plot_ahp_weights(ahp_moead_results)

    print("  ğŸ“Š ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿...")
    viz.plot_pareto_front(ahp_moead_results)

    print("  ğŸ“Š ç»˜åˆ¶æ¼”åŒ–è¿‡ç¨‹...")
    viz.plot_evolution_process(ahp_moead_results)

    # ========== æ¨¡å‹ III: æ¨¡ç³Šè¯„ä»· + ç›¸å…³æ€§åˆ†æ ==========
    print("\nã€Model IIIã€‘ç»¼åˆæˆåŠŸè¯„ä»·æ¨¡å‹ (FCE + Correlation)...")
    fce_corr_model = FCE_Correlation_Model(ahp_weights=ahp_moead_results['ahp_weights'])
    fce_corr_results = fce_corr_model.run_evaluation()

    print("  ğŸ“Š ç»˜åˆ¶æ¨¡ç³Šè¯„ä»·ç»“æœ...")
    viz.plot_fce_results(fce_corr_results)

    print("  ğŸ“Š ç»˜åˆ¶ç›¸å…³æ€§åˆ†æ...")
    viz.plot_correlation_results(fce_corr_results)

    # ========== æ¨¡å‹ IV: CBR + GWR ==========
    print("\nã€Model IVã€‘æ³›åŒ–ä¸æ¨å¹¿æ¨¡å‹ (CBR + GWR)...")
    cbr_gwr_model = CBR_GWR_Model()

    # ç¤ºä¾‹æŸ¥è¯¢
    query_case = {
        'digital_level': 0.6,
        'budget_per_student': 40000,
        'ai_integration': 0.4
    }
    new_locations = np.array([[50, 50], [70, 30]])

    cbr_gwr_results = cbr_gwr_model.generalize_solution(query_case, new_locations)

    print(f"  ğŸ“Š æ‰¾åˆ° {len(cbr_gwr_results['similar_cases'])} ä¸ªç›¸ä¼¼æ¡ˆä¾‹")
    print(f"  ğŸ“Š ä¸º {len(new_locations)} ä¸ªæ–°ä½ç½®é¢„æµ‹äº†å±€éƒ¨å‚æ•°")

    print("  ğŸ“Š ç»˜åˆ¶GWRç©ºé—´æ•æ„Ÿåº¦...")
    viz.plot_gwr_spatial_sensitivity(cbr_gwr_results)

    # çµæ•åº¦åˆ†æ
    print("\nã€Sensitivity Analysisã€‘å‚æ•°çµæ•åº¦åˆ†æ...")
    sd_model = SDModel(SDParams('chef'))
    beta_range = np.linspace(0.05, 0.25, 10)
    sensitivity_results = sensitivity_analysis(sd_model, 'beta', beta_range)

    # ç»˜åˆ¶çµæ•åº¦åˆ†æç»“æœ
    fig, ax = plt.subplots(figsize=(10, 6))
    param_values = [r['param_value'] for r in sensitivity_results]
    labor_demands = [r['final_labor_demand'] for r in sensitivity_results]
    ax.plot(param_values, labor_demands, 'bo-', linewidth=2)
    ax.set_title('Sensitivity Analysis: Beta Parameter vs Final Labor Demand')
    ax.set_xlabel('Beta (Substitution Rate)')
    ax.set_ylabel('Final Labor Demand')
    ax.grid(True)
    paths = viz.saver.save(fig, 'sensitivity_analysis')
    print(f"Sensitivity analysis visualization saved: {paths[0]}")

    print("\n" + "â–ˆ"*70)
    print("â–ˆ" + " "*23 + "ç»¼åˆæ¨¡å‹æ‰§è¡Œå®Œæˆ!" + " "*24 + "â–ˆ")
    print("â–ˆ"*70 + "\n")

    return {
        'model_I': sd_bn_results,
        'model_II': ahp_moead_results,
        'model_III': fce_corr_results,
        'model_IV': cbr_gwr_results,
        'sensitivity': sensitivity_results
    }


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æ¨¡å‹
    all_results = run_comprehensive_models()

    # ç¤ºä¾‹ï¼šè®¿é—®ç‰¹å®šæ¨¡å‹ç»“æœ
    print("\nç¤ºä¾‹ç»“æœè®¿é—®:")
    print(f"SDæ¨¡å‹æœ€ç»ˆåŠ³åŠ¨åŠ›éœ€æ±‚: {all_results['model_I']['sd_results']['labor_demand'][-1]:.2f}")
    print(f"äººç±»æ ¸å¿ƒç´ å…»æœ€ç»ˆå€¼: {all_results['model_I']['sd_results']['human_core_competence'][-1]:.2f}")
    print(f"AHPä¸€è‡´æ€§æ¯”ç‡: {all_results['model_II']['ahp_CR']:.3f}")
    print(f"æƒé‡çµæ•åº¦æœ€å¤§å˜åŒ–: {max(r['max_weight_change'] for r in all_results['model_II']['sensitivity_results']):.3f}")
    print(f"è†ç›–ç‚¹æ”¿ç­–å»ºè®®: {all_results['model_II']['knee_point']['policy_recommendation'][:100]}...")
    print(f"ç›¸å…³ç³»æ•°: {all_results['model_III']['corr_results']['correlation_coefficient']:.3f}")
