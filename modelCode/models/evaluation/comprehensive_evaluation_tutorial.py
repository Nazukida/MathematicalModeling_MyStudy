"""
============================================================
ç»¼åˆè¯„ä»·æ¨¡å‹å®Œæ•´æ•™ç¨‹ (Comprehensive Evaluation Tutorial)
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
æœ¬æ•™ç¨‹å±•ç¤ºå¦‚ä½•å°†æ•°æ®é¢„å¤„ç†ã€èµ‹æƒæ¨¡å‹ã€ç»¼åˆè¯„ä»·ã€å¯è§†åŒ–å®Œæ•´ä¸²è”èµ·æ¥

åŒ…å«å†…å®¹ï¼š
1. æ•°æ®é¢„å¤„ç†æ¨¡å— (Data Preprocessing)
2. èµ‹æƒæ–¹æ³• (Weighting Methods)
   - ç†µæƒæ³• (Entropy Weight)
   - CRITICæ³• (CRITIC Method)
   - ç»„åˆèµ‹æƒæ³• (Combined Weighting)
3. ç»¼åˆè¯„ä»·æ–¹æ³• (Evaluation Methods)
   - TOPSISæ³• (TOPSIS)
   - ç°è‰²å…³è”åˆ†æ (Grey Relational Analysis)
4. å¯è§†åŒ–æ¨¡å— (Visualization)
5. çµæ•åº¦åˆ†æ (Sensitivity Analysis)
6. å®Œæ•´æ¡ˆä¾‹æ¼”ç¤º

ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ20æ—¥
============================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå®Œæ•´å·¥ä½œæµç¨‹æ¦‚è§ˆ
# ============================================================

def print_workflow():
    """æ‰“å°å®Œæ•´å·¥ä½œæµç¨‹"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              ç»¼åˆè¯„ä»·æ¨¡å‹å®Œæ•´å·¥ä½œæµç¨‹ (Complete Workflow)                  â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                          â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 1: æ•°æ®å‡†å¤‡ (Data Preparation)                            â”‚    â•‘
    â•‘   â”‚  â”œâ”€ ä»CSV/DataFrame/æ•°ç»„åŠ è½½æ•°æ®                                 â”‚    â•‘
    â•‘   â”‚  â”œâ”€ ç¡®å®šè¯„ä»·å¯¹è±¡å’Œè¯„ä»·æŒ‡æ ‡                                       â”‚    â•‘
    â•‘   â”‚  â””â”€ ç¡®å®šæŒ‡æ ‡ç±»å‹ï¼ˆæ­£å‘/è´Ÿå‘ï¼‰                                    â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                            â†“                                             â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 2: æ•°æ®é¢„å¤„ç† (Data Preprocessing)                        â”‚    â•‘
    â•‘   â”‚  â”œâ”€ ç¼ºå¤±å€¼å¤„ç†ï¼ˆå‡å€¼/ä¸­å€¼å¡«å……ï¼‰                                  â”‚    â•‘
    â•‘   â”‚  â”œâ”€ å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†                                             â”‚    â•‘
    â•‘   â”‚  â””â”€ æ•°æ®æ ‡å‡†åŒ–ï¼ˆæå·®æ³•/Z-scoreï¼‰                                 â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                            â†“                                             â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 3: ç¡®å®šæƒé‡ (Weight Determination)                        â”‚    â•‘
    â•‘   â”‚  â”œâ”€ ç†µæƒæ³• (é€‚ç”¨äºæŒ‡æ ‡ç‹¬ç«‹çš„æƒ…å†µ)                                â”‚    â•‘
    â•‘   â”‚  â”œâ”€ CRITICæ³• (é€‚ç”¨äºæŒ‡æ ‡ç›¸å…³çš„æƒ…å†µ)                              â”‚    â•‘
    â•‘   â”‚  â””â”€ ç»„åˆèµ‹æƒ (ä¸»è§‚+å®¢è§‚ç»“åˆ)                                     â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                            â†“                                             â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 4: ç»¼åˆè¯„ä»· (Comprehensive Evaluation)                    â”‚    â•‘
    â•‘   â”‚  â”œâ”€ TOPSISæ³• (é€¼è¿‘ç†æƒ³è§£æ’åº)                                    â”‚    â•‘
    â•‘   â”‚  â””â”€ ç°è‰²å…³è”åˆ†æ (å½¢çŠ¶ç›¸ä¼¼åº¦è¯„ä»·)                                â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                            â†“                                             â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 5: å¯è§†åŒ–åˆ†æ (Visualization)                             â”‚    â•‘
    â•‘   â”‚  â”œâ”€ æƒé‡åˆ†å¸ƒå›¾ï¼ˆæ¡å½¢å›¾/é¥¼å›¾ï¼‰                                    â”‚    â•‘
    â•‘   â”‚  â”œâ”€ è¯„ä»·ç»“æœæ’åºå›¾                                               â”‚    â•‘
    â•‘   â”‚  â”œâ”€ é›·è¾¾å›¾ï¼ˆå¤šç»´åº¦å¯¹æ¯”ï¼‰                                         â”‚    â•‘
    â•‘   â”‚  â””â”€ çƒ­åŠ›å›¾ï¼ˆè¯„ä»·çŸ©é˜µï¼‰                                           â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                            â†“                                             â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘   â”‚  Step 6: çµæ•åº¦åˆ†æ (Sensitivity Analysis)                      â”‚    â•‘
    â•‘   â”‚  â””â”€ æƒé‡æ‰°åŠ¨å¯¹ç»“æœçš„å½±å“åˆ†æ                                     â”‚    â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                                                                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†ç±»
# ============================================================

class DataPreprocessor:
    """
    ç»¼åˆè¯„ä»·æ•°æ®é¢„å¤„ç†å™¨
    åŠŸèƒ½ï¼šæ•°æ®åŠ è½½ã€ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€æ•°æ®æ ‡å‡†åŒ–
    """
    
    def __init__(self):
        self.raw_data = None
        self.processed_data = None
        self.indicator_names = None
        self.object_names = None
        self.indicator_types = None
        self.preprocessing_log = []
    
    def load_data(self, data, indicator_names=None, object_names=None, indicator_types=None):
        """
        åŠ è½½æ•°æ®
        
        :param data: numpyæ•°ç»„ã€DataFrameæˆ–CSVæ–‡ä»¶è·¯å¾„
        :param indicator_names: æŒ‡æ ‡åç§°åˆ—è¡¨
        :param object_names: è¯„ä»·å¯¹è±¡åç§°åˆ—è¡¨
        :param indicator_types: æŒ‡æ ‡ç±»å‹ ['positive', 'negative', ...]
        :return: self
        """
        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
        if isinstance(data, str):
            df = pd.read_csv(data, index_col=0)
            self.raw_data = df.values.astype(float)
            self.indicator_names = list(df.columns)
            self.object_names = list(df.index)
        # å¦‚æœæ˜¯DataFrame
        elif isinstance(data, pd.DataFrame):
            self.raw_data = data.values.astype(float)
            self.indicator_names = indicator_names or list(data.columns)
            self.object_names = object_names or list(data.index)
        # å¦‚æœæ˜¯numpyæ•°ç»„
        else:
            self.raw_data = np.array(data).astype(float)
            n_objects, n_indicators = self.raw_data.shape
            self.indicator_names = indicator_names or [f"æŒ‡æ ‡{i+1}" for i in range(n_indicators)]
            self.object_names = object_names or [f"æ–¹æ¡ˆ{i+1}" for i in range(n_objects)]
        
        self.indicator_types = indicator_types
        self.processed_data = self.raw_data.copy()
        self.preprocessing_log.append("æ•°æ®åŠ è½½å®Œæˆ")
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼š{len(self.object_names)}ä¸ªè¯„ä»·å¯¹è±¡ï¼Œ{len(self.indicator_names)}ä¸ªè¯„ä»·æŒ‡æ ‡")
        return self
    
    def check_missing_values(self):
        """æ£€æŸ¥ç¼ºå¤±å€¼"""
        missing_count = np.isnan(self.processed_data).sum()
        if missing_count > 0:
            print(f"âš ï¸  å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼")
            return True
        else:
            print("âœ… æ— ç¼ºå¤±å€¼")
            return False
    
    def handle_missing_values(self, method='mean'):
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        :param method: 'mean'(å‡å€¼å¡«å……) / 'median'(ä¸­å€¼å¡«å……) / 'drop'(åˆ é™¤)
        """
        for j in range(self.processed_data.shape[1]):
            col = self.processed_data[:, j]
            mask = np.isnan(col)
            if mask.any():
                if method == 'mean':
                    fill_value = np.nanmean(col)
                elif method == 'median':
                    fill_value = np.nanmedian(col)
                else:
                    continue
                col[mask] = fill_value
        
        self.preprocessing_log.append(f"ç¼ºå¤±å€¼å¤„ç†ï¼š{method}")
        print(f"âœ… ç¼ºå¤±å€¼å·²ä½¿ç”¨ {method} æ–¹æ³•å¡«å……")
        return self
    
    def detect_outliers(self, method='iqr', threshold=1.5):
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        
        :param method: 'iqr'(å››åˆ†ä½è·æ³•) / 'zscore'(Zåˆ†æ•°æ³•)
        :param threshold: é˜ˆå€¼ï¼ˆIQRæ³•é»˜è®¤1.5ï¼ŒZåˆ†æ•°æ³•é»˜è®¤3ï¼‰
        """
        outliers = {}
        
        for j, name in enumerate(self.indicator_names):
            col = self.processed_data[:, j]
            
            if method == 'iqr':
                Q1, Q3 = np.percentile(col, [25, 75])
                IQR = Q3 - Q1
                lower = Q1 - threshold * IQR
                upper = Q3 + threshold * IQR
                outlier_mask = (col < lower) | (col > upper)
            elif method == 'zscore':
                z_scores = np.abs((col - np.mean(col)) / np.std(col))
                outlier_mask = z_scores > threshold
            
            if outlier_mask.any():
                outliers[name] = np.where(outlier_mask)[0].tolist()
        
        if outliers:
            print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼š")
            for indicator, indices in outliers.items():
                print(f"    {indicator}: ç¬¬ {indices} è¡Œ")
        else:
            print("âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸å€¼")
        
        return outliers
    
    def normalize(self, method='minmax'):
        """
        æ•°æ®æ ‡å‡†åŒ–
        
        :param method: 'minmax'(æå·®æ³•) / 'zscore'(Zåˆ†æ•°æ³•)
        """
        if method == 'minmax':
            data_min = self.processed_data.min(axis=0)
            data_max = self.processed_data.max(axis=0)
            self.processed_data = (self.processed_data - data_min) / (data_max - data_min + 1e-10)
        elif method == 'zscore':
            mean = self.processed_data.mean(axis=0)
            std = self.processed_data.std(axis=0)
            self.processed_data = (self.processed_data - mean) / (std + 1e-10)
        
        self.preprocessing_log.append(f"æ•°æ®æ ‡å‡†åŒ–ï¼š{method}")
        print(f"âœ… æ•°æ®å·²ä½¿ç”¨ {method} æ–¹æ³•æ ‡å‡†åŒ–")
        return self
    
    def transform_negative_indicators(self):
        """
        å°†è´Ÿå‘æŒ‡æ ‡è½¬ä¸ºæ­£å‘
        """
        if self.indicator_types is None:
            print("âš ï¸  æœªè®¾ç½®æŒ‡æ ‡ç±»å‹ï¼Œè·³è¿‡è´Ÿå‘æŒ‡æ ‡è½¬æ¢")
            return self
        
        for j, ind_type in enumerate(self.indicator_types):
            if ind_type == 'negative':
                self.processed_data[:, j] = 1 - self.processed_data[:, j]
        
        self.preprocessing_log.append("è´Ÿå‘æŒ‡æ ‡å·²è½¬ä¸ºæ­£å‘")
        print("âœ… è´Ÿå‘æŒ‡æ ‡å·²è½¬ä¸ºæ­£å‘")
        return self
    
    def get_dataframe(self):
        """è¿”å›å¤„ç†åçš„DataFrame"""
        return pd.DataFrame(
            self.processed_data,
            index=self.object_names,
            columns=self.indicator_names
        )
    
    def summary(self):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®é¢„å¤„ç†æ‘˜è¦")
        print("="*60)
        print(f"è¯„ä»·å¯¹è±¡: {self.object_names}")
        print(f"è¯„ä»·æŒ‡æ ‡: {self.indicator_names}")
        print(f"æŒ‡æ ‡ç±»å‹: {self.indicator_types}")
        print(f"é¢„å¤„ç†æ­¥éª¤: {self.preprocessing_log}")
        print("\nå¤„ç†åæ•°æ®:")
        print(self.get_dataframe().round(4))
        print("="*60)


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šèµ‹æƒæ–¹æ³•
# ============================================================

class EntropyWeight:
    """ç†µæƒæ³•"""
    
    def __init__(self):
        self.weights = None
        self.entropy = None
        self.indicator_names = None
    
    def fit(self, data):
        """
        è®¡ç®—ç†µæƒæ³•æƒé‡
        
        :param data: æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆDataFrameæˆ–æ•°ç»„ï¼‰
        """
        if isinstance(data, pd.DataFrame):
            self.indicator_names = list(data.columns)
            data = data.values
        else:
            self.indicator_names = [f"æŒ‡æ ‡{i+1}" for i in range(data.shape[1])]
        
        n, m = data.shape
        
        # è®¡ç®—æ¯”ä¾‹çŸ©é˜µ
        data = np.clip(data, 1e-10, None)  # é¿å…0å€¼
        p = data / data.sum(axis=0)
        p = np.where(p == 0, 1e-10, p)
        
        # è®¡ç®—ç†µå€¼
        k = 1 / np.log(n)
        self.entropy = -k * (p * np.log(p)).sum(axis=0)
        
        # è®¡ç®—æƒé‡
        d = 1 - self.entropy
        self.weights = d / d.sum()
        
        return self
    
    def get_weights(self):
        """è¿”å›æƒé‡Series"""
        return pd.Series(self.weights, index=self.indicator_names)


class CRITIC:
    """CRITICæ³•"""
    
    def __init__(self):
        self.weights = None
        self.std = None
        self.conflict = None
        self.correlation_matrix = None
        self.indicator_names = None
    
    def fit(self, data):
        """
        è®¡ç®—CRITICæ³•æƒé‡
        
        :param data: æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆDataFrameæˆ–æ•°ç»„ï¼‰
        """
        if isinstance(data, pd.DataFrame):
            self.indicator_names = list(data.columns)
            data = data.values
        else:
            self.indicator_names = [f"æŒ‡æ ‡{i+1}" for i in range(data.shape[1])]
        
        # è®¡ç®—æ ‡å‡†å·®
        self.std = np.std(data, axis=0, ddof=1)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        self.correlation_matrix = np.corrcoef(data.T)
        self.correlation_matrix = np.nan_to_num(self.correlation_matrix, nan=1.0)
        
        # è®¡ç®—å†²çªæ€§
        self.conflict = np.sum(1 - self.correlation_matrix, axis=1)
        
        # è®¡ç®—ä¿¡æ¯é‡
        information = self.std * self.conflict
        
        # è®¡ç®—æƒé‡
        self.weights = information / information.sum()
        
        return self
    
    def get_weights(self):
        """è¿”å›æƒé‡Series"""
        return pd.Series(self.weights, index=self.indicator_names)


class CombinedWeight:
    """ç»„åˆèµ‹æƒæ³•"""
    
    def __init__(self, alpha=0.5):
        """
        :param alpha: ç†µæƒæ³•çš„æƒé‡ç³»æ•°ï¼ŒCRITICæ³•ç³»æ•°ä¸º 1-alpha
        """
        self.alpha = alpha
        self.weights = None
        self.entropy_weights = None
        self.critic_weights = None
    
    def fit(self, data):
        """è®¡ç®—ç»„åˆæƒé‡"""
        # ç†µæƒæ³•
        entropy = EntropyWeight()
        entropy.fit(data)
        self.entropy_weights = entropy.get_weights()
        
        # CRITICæ³•
        critic = CRITIC()
        critic.fit(data)
        self.critic_weights = critic.get_weights()
        
        # ç»„åˆ
        self.weights = self.alpha * self.entropy_weights + (1 - self.alpha) * self.critic_weights
        
        return self
    
    def get_weights(self):
        """è¿”å›æƒé‡Series"""
        return self.weights


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆè¯„ä»·æ–¹æ³•
# ============================================================

class TOPSIS:
    """TOPSISæ³•"""
    
    def __init__(self):
        self.closeness = None
        self.rankings = None
        self.distances_positive = None
        self.distances_negative = None
        self.object_names = None
    
    def fit(self, data, weights):
        """
        æ‰§è¡ŒTOPSISè¯„ä»·
        
        :param data: æ ‡å‡†åŒ–åçš„æ•°æ®
        :param weights: æƒé‡å‘é‡
        """
        if isinstance(data, pd.DataFrame):
            self.object_names = list(data.index)
            data = data.values
        else:
            self.object_names = [f"æ–¹æ¡ˆ{i+1}" for i in range(data.shape[0])]
        
        if isinstance(weights, pd.Series):
            weights = weights.values
        
        # åŠ æƒæ ‡å‡†åŒ–
        data_weighted = data * weights
        
        # ç†æƒ³è§£å’Œè´Ÿç†æƒ³è§£
        ideal_positive = data_weighted.max(axis=0)
        ideal_negative = data_weighted.min(axis=0)
        
        # è®¡ç®—è·ç¦»
        self.distances_positive = np.sqrt(((data_weighted - ideal_positive) ** 2).sum(axis=1))
        self.distances_negative = np.sqrt(((data_weighted - ideal_negative) ** 2).sum(axis=1))
        
        # è®¡ç®—è´´è¿‘åº¦
        self.closeness = self.distances_negative / (self.distances_positive + self.distances_negative + 1e-10)
        
        # æ’å
        self.rankings = np.argsort(-self.closeness) + 1
        
        return self
    
    def get_results(self):
        """è¿”å›ç»“æœDataFrame"""
        return pd.DataFrame({
            'è¯„ä»·å¯¹è±¡': self.object_names,
            'D+': self.distances_positive.round(4),
            'D-': self.distances_negative.round(4),
            'è´´è¿‘åº¦': self.closeness.round(4),
            'æ’å': [np.where(np.argsort(-self.closeness) == i)[0][0] + 1 for i in range(len(self.object_names))]
        }).sort_values('æ’å')


class GreyRelationalAnalysis:
    """ç°è‰²å…³è”åˆ†æ"""
    
    def __init__(self, rho=0.5):
        self.rho = rho
        self.relational_degrees = None
        self.rankings = None
        self.object_names = None
    
    def fit(self, data, weights=None):
        """
        æ‰§è¡Œç°è‰²å…³è”åˆ†æ
        
        :param data: æ ‡å‡†åŒ–åçš„æ•°æ®
        :param weights: å¯é€‰çš„æƒé‡å‘é‡
        """
        if isinstance(data, pd.DataFrame):
            self.object_names = list(data.index)
            data = data.values
        else:
            self.object_names = [f"æ–¹æ¡ˆ{i+1}" for i in range(data.shape[0])]
        
        n, m = data.shape
        
        # å‚è€ƒåºåˆ—ï¼ˆæœ€ä¼˜å€¼ï¼‰
        reference = data.max(axis=0)
        
        # å·®åºåˆ—
        delta = np.abs(data - reference)
        delta_min = delta.min()
        delta_max = delta.max()
        
        # å…³è”ç³»æ•°
        xi = (delta_min + self.rho * delta_max) / (delta + self.rho * delta_max)
        
        # å…³è”åº¦
        if weights is not None:
            if isinstance(weights, pd.Series):
                weights = weights.values
            self.relational_degrees = (xi * weights).sum(axis=1)
        else:
            self.relational_degrees = xi.mean(axis=1)
        
        # æ’å
        self.rankings = np.argsort(-self.relational_degrees) + 1
        
        return self
    
    def get_results(self):
        """è¿”å›ç»“æœDataFrame"""
        return pd.DataFrame({
            'è¯„ä»·å¯¹è±¡': self.object_names,
            'ç°è‰²å…³è”åº¦': self.relational_degrees.round(4),
            'æ’å': [np.where(np.argsort(-self.relational_degrees) == i)[0][0] + 1 for i in range(len(self.object_names))]
        }).sort_values('æ’å')


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¯è§†åŒ–æ¨¡å—
# ============================================================

class EvaluationVisualizer:
    """ç»¼åˆè¯„ä»·å¯è§†åŒ–å™¨"""
    
    COLORS = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6B4C9A', '#1B998B']
    
    @staticmethod
    def plot_weights_comparison(entropy_weights, critic_weights, combined_weights=None, 
                                 save_path=None):
        """
        å¯¹æ¯”ä¸åŒèµ‹æƒæ–¹æ³•çš„æƒé‡
        """
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(entropy_weights))
        width = 0.25
        
        bars1 = ax.bar(x - width, entropy_weights.values, width, label='ç†µæƒæ³•', 
                       color='#2E86AB', edgecolor='white')
        bars2 = ax.bar(x, critic_weights.values, width, label='CRITICæ³•', 
                       color='#A23B72', edgecolor='white')
        
        if combined_weights is not None:
            bars3 = ax.bar(x + width, combined_weights.values, width, label='ç»„åˆèµ‹æƒ', 
                           color='#F18F01', edgecolor='white')
        
        ax.set_xlabel('è¯„ä»·æŒ‡æ ‡', fontsize=12, fontweight='bold')
        ax.set_ylabel('æƒé‡', fontsize=12, fontweight='bold')
        ax.set_title('ä¸åŒèµ‹æƒæ–¹æ³•æƒé‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(entropy_weights.index, rotation=15)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_topsis_results(results, title="TOPSISè¯„ä»·ç»“æœ", save_path=None):
        """ç»˜åˆ¶TOPSISç»“æœ"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # è´´è¿‘åº¦æ’åº
        ax1 = axes[0]
        sorted_results = results.sort_values('è´´è¿‘åº¦', ascending=True)
        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sorted_results)))
        ax1.barh(sorted_results['è¯„ä»·å¯¹è±¡'], sorted_results['è´´è¿‘åº¦'],
                color=colors, edgecolor='white', linewidth=2)
        ax1.set_xlabel('è´´è¿‘åº¦', fontweight='bold')
        ax1.set_title('(a) è´´è¿‘åº¦æ’åº', fontsize=12, fontweight='bold')
        ax1.set_xlim(0, 1)
        
        # æ·»åŠ æ’åæ ‡æ³¨
        for i, (_, row) in enumerate(sorted_results.iterrows()):
            ax1.text(row['è´´è¿‘åº¦'] + 0.02, i, f"#{int(row['æ’å'])}", 
                    va='center', fontweight='bold')
        
        # è·ç¦»å¯¹æ¯”
        ax2 = axes[1]
        x = np.arange(len(results))
        width = 0.35
        ax2.bar(x - width/2, results['D+'], width, label='D+ (åˆ°ç†æƒ³è§£)', 
               color='#2E86AB', edgecolor='white')
        ax2.bar(x + width/2, results['D-'], width, label='D- (åˆ°è´Ÿç†æƒ³è§£)', 
               color='#A23B72', edgecolor='white')
        ax2.set_xlabel('è¯„ä»·å¯¹è±¡', fontweight='bold')
        ax2.set_ylabel('è·ç¦»', fontweight='bold')
        ax2.set_title('(b) è·ç¦»å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(results['è¯„ä»·å¯¹è±¡'], rotation=15)
        ax2.legend()
        
        plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_radar(data, title="å¤šç»´åº¦é›·è¾¾å›¾", save_path=None):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        if isinstance(data, pd.DataFrame):
            indicators = list(data.columns)
            object_names = list(data.index)
            values = data.values
        else:
            raise ValueError("è¯·ä¼ å…¥DataFrameæ ¼å¼çš„æ•°æ®")
        
        n_indicators = len(indicators)
        angles = np.linspace(0, 2 * np.pi, n_indicators, endpoint=False).tolist()
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))
        
        colors = EvaluationVisualizer.COLORS
        for i, (name, row) in enumerate(zip(object_names, values)):
            row_values = row.tolist()
            row_values += row_values[:1]
            ax.plot(angles, row_values, 'o-', linewidth=2, label=name,
                   color=colors[i % len(colors)])
            ax.fill(angles, row_values, alpha=0.1, color=colors[i % len(colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(indicators, fontsize=11)
        ax.set_title(title, fontsize=14, fontweight='bold', y=1.08)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_full_report(raw_data, weights, topsis_results, gra_results=None, save_path=None):
        """ç”Ÿæˆå®Œæ•´è¯„ä»·æŠ¥å‘Š"""
        fig = plt.figure(figsize=(16, 12))
        
        # å­å›¾1: åŸå§‹æ•°æ®çƒ­åŠ›å›¾
        ax1 = fig.add_subplot(2, 2, 1)
        data_norm = (raw_data - raw_data.min()) / (raw_data.max() - raw_data.min())
        im1 = ax1.imshow(data_norm.values, cmap='RdYlGn', aspect='auto')
        ax1.set_xticks(np.arange(len(raw_data.columns)))
        ax1.set_yticks(np.arange(len(raw_data.index)))
        ax1.set_xticklabels(raw_data.columns, fontsize=9, rotation=15)
        ax1.set_yticklabels(raw_data.index, fontsize=9)
        ax1.set_title('(a) è¯„ä»·çŸ©é˜µçƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
        plt.colorbar(im1, ax=ax1, shrink=0.8)
        
        # å­å›¾2: æƒé‡åˆ†å¸ƒ
        ax2 = fig.add_subplot(2, 2, 2)
        colors = EvaluationVisualizer.COLORS[:len(weights)]
        bars = ax2.bar(weights.index, weights.values, color=colors, edgecolor='white')
        ax2.set_ylabel('æƒé‡', fontweight='bold')
        ax2.set_title('(b) æŒ‡æ ‡æƒé‡åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        for bar, val in zip(bars, weights.values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)
        ax2.tick_params(axis='x', rotation=15)
        
        # å­å›¾3: TOPSISæ’åº
        ax3 = fig.add_subplot(2, 2, 3)
        sorted_results = topsis_results.sort_values('è´´è¿‘åº¦', ascending=True)
        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sorted_results)))
        ax3.barh(sorted_results['è¯„ä»·å¯¹è±¡'], sorted_results['è´´è¿‘åº¦'],
                color=colors, edgecolor='white')
        ax3.set_xlabel('è´´è¿‘åº¦', fontweight='bold')
        ax3.set_title('(c) TOPSISè¯„ä»·æ’åº', fontsize=12, fontweight='bold')
        ax3.set_xlim(0, 1)
        for i, (_, row) in enumerate(sorted_results.iterrows()):
            ax3.text(row['è´´è¿‘åº¦'] + 0.02, i, f"#{int(row['æ’å'])}", 
                    va='center', fontweight='bold', fontsize=10)
        
        # å­å›¾4: é›·è¾¾å›¾
        ax4 = fig.add_subplot(2, 2, 4, polar=True)
        indicators = list(raw_data.columns)
        n_indicators = len(indicators)
        angles = np.linspace(0, 2 * np.pi, n_indicators, endpoint=False).tolist()
        angles += angles[:1]
        
        colors_radar = EvaluationVisualizer.COLORS
        for i, (name, row) in enumerate(data_norm.iterrows()):
            values = row.tolist()
            values += values[:1]
            ax4.plot(angles, values, 'o-', linewidth=2, label=name,
                    color=colors_radar[i % len(colors_radar)])
            ax4.fill(angles, values, alpha=0.1)
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(indicators, fontsize=9)
        ax4.set_title('(d) å¤šç»´åº¦å¯¹æ¯”', fontsize=12, fontweight='bold', y=1.08)
        ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=8)
        
        plt.suptitle('ç»¼åˆè¯„ä»·åˆ†ææŠ¥å‘Š (Comprehensive Evaluation Report)', 
                    fontsize=14, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´æ¡ˆä¾‹æ¼”ç¤º
# ============================================================

def run_complete_example():
    """è¿è¡Œå®Œæ•´çš„ç»¼åˆè¯„ä»·æ¡ˆä¾‹"""
    
    print_workflow()
    
    print("\n" + "="*70)
    print("ğŸ¯ ç»¼åˆè¯„ä»·å®Œæ•´æ¡ˆä¾‹ï¼šä¾›åº”å•†é€‰æ‹©é—®é¢˜")
    print("="*70)
    
    # ========================================
    # Step 1: æ•°æ®å‡†å¤‡
    # ========================================
    print("\n" + "-"*50)
    print("ğŸ“Š Step 1: æ•°æ®å‡†å¤‡")
    print("-"*50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼š5ä¸ªä¾›åº”å•†ï¼Œ4ä¸ªè¯„ä»·æŒ‡æ ‡
    np.random.seed(2026)
    data = pd.DataFrame({
        'è´¨é‡è¯„åˆ†': [85, 92, 78, 88, 95],
        'ä»·æ ¼(ä¸‡å…ƒ)': [120, 95, 150, 110, 85],    # è´Ÿå‘æŒ‡æ ‡
        'äº¤è´§æœŸ(å¤©)': [15, 20, 10, 25, 12],       # è´Ÿå‘æŒ‡æ ‡
        'æœåŠ¡è¯„åˆ†': [90, 85, 88, 92, 80]
    }, index=['ä¾›åº”å•†A', 'ä¾›åº”å•†B', 'ä¾›åº”å•†C', 'ä¾›åº”å•†D', 'ä¾›åº”å•†E'])
    
    indicator_types = ['positive', 'negative', 'negative', 'positive']
    
    print("\nåŸå§‹æ•°æ®ï¼š")
    print(data)
    print(f"\næŒ‡æ ‡ç±»å‹ï¼š{indicator_types}")
    
    # ========================================
    # Step 2: æ•°æ®é¢„å¤„ç†
    # ========================================
    print("\n" + "-"*50)
    print("ğŸ“Š Step 2: æ•°æ®é¢„å¤„ç†")
    print("-"*50)
    
    preprocessor = DataPreprocessor()
    preprocessor.load_data(data, indicator_types=indicator_types)
    preprocessor.check_missing_values()
    preprocessor.detect_outliers(method='iqr')
    preprocessor.normalize(method='minmax')
    preprocessor.transform_negative_indicators()
    
    processed_data = preprocessor.get_dataframe()
    print("\né¢„å¤„ç†åçš„æ•°æ®ï¼ˆæ ‡å‡†åŒ–+è´Ÿå‘è½¬æ­£å‘ï¼‰ï¼š")
    print(processed_data.round(4))
    
    # ========================================
    # Step 3: ç¡®å®šæƒé‡
    # ========================================
    print("\n" + "-"*50)
    print("ğŸ“Š Step 3: ç¡®å®šæƒé‡")
    print("-"*50)
    
    # 3.1 ç†µæƒæ³•
    entropy = EntropyWeight()
    entropy.fit(processed_data)
    entropy_weights = entropy.get_weights()
    print("\nç†µæƒæ³•æƒé‡ï¼š")
    print(entropy_weights.round(4))
    
    # 3.2 CRITICæ³•
    critic = CRITIC()
    critic.fit(processed_data)
    critic_weights = critic.get_weights()
    print("\nCRITICæ³•æƒé‡ï¼š")
    print(critic_weights.round(4))
    
    # 3.3 ç»„åˆèµ‹æƒ
    combined = CombinedWeight(alpha=0.5)
    combined.fit(processed_data)
    combined_weights = combined.get_weights()
    print("\nç»„åˆèµ‹æƒæƒé‡ï¼ˆÎ±=0.5ï¼‰ï¼š")
    print(combined_weights.round(4))
    
    # ========================================
    # Step 4: ç»¼åˆè¯„ä»·
    # ========================================
    print("\n" + "-"*50)
    print("ğŸ“Š Step 4: ç»¼åˆè¯„ä»·")
    print("-"*50)
    
    # 4.1 TOPSISæ³•
    topsis = TOPSIS()
    topsis.fit(processed_data, combined_weights)
    topsis_results = topsis.get_results()
    print("\nTOPSISè¯„ä»·ç»“æœï¼š")
    print(topsis_results)
    
    # 4.2 ç°è‰²å…³è”åˆ†æ
    gra = GreyRelationalAnalysis(rho=0.5)
    gra.fit(processed_data, combined_weights)
    gra_results = gra.get_results()
    print("\nç°è‰²å…³è”åˆ†æç»“æœï¼š")
    print(gra_results)
    
    # ========================================
    # Step 5: å¯è§†åŒ–åˆ†æ
    # ========================================
    print("\n" + "-"*50)
    print("ğŸ“Š Step 5: å¯è§†åŒ–åˆ†æ")
    print("-"*50)
    
    visualizer = EvaluationVisualizer()
    
    # æƒé‡å¯¹æ¯”å›¾
    visualizer.plot_weights_comparison(entropy_weights, critic_weights, combined_weights)
    
    # TOPSISç»“æœå›¾
    visualizer.plot_topsis_results(topsis_results, title="ä¾›åº”å•†TOPSISè¯„ä»·ç»“æœ")
    
    # é›·è¾¾å›¾
    visualizer.plot_radar(processed_data, title="ä¾›åº”å•†å¤šç»´åº¦å¯¹æ¯”é›·è¾¾å›¾")
    
    # å®Œæ•´æŠ¥å‘Š
    visualizer.plot_full_report(data, combined_weights, topsis_results)
    
    # ========================================
    # ç»“è®º
    # ========================================
    print("\n" + "="*70)
    print("ğŸ† è¯„ä»·ç»“è®º")
    print("="*70)
    
    best_topsis = topsis_results[topsis_results['æ’å'] == 1]['è¯„ä»·å¯¹è±¡'].values[0]
    best_gra = gra_results[gra_results['æ’å'] == 1]['è¯„ä»·å¯¹è±¡'].values[0]
    
    print(f"\nTOPSISæ³•æœ€ä¼˜æ–¹æ¡ˆ: {best_topsis}")
    print(f"ç°è‰²å…³è”åˆ†ææœ€ä¼˜æ–¹æ¡ˆ: {best_gra}")
    
    if best_topsis == best_gra:
        print(f"\nâœ… ä¸¤ç§æ–¹æ³•è¯„ä»·ç»“æœä¸€è‡´ï¼Œæœ€ç»ˆæ¨è: {best_topsis}")
    else:
        print(f"\nâš ï¸  ä¸¤ç§æ–¹æ³•è¯„ä»·ç»“æœä¸ä¸€è‡´ï¼Œå»ºè®®è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ")
    
    print("\n" + "="*70)
    print("   âœ… ç»¼åˆè¯„ä»·å®Œæˆï¼")
    print("="*70)
    
    return {
        'raw_data': data,
        'processed_data': processed_data,
        'entropy_weights': entropy_weights,
        'critic_weights': critic_weights,
        'combined_weights': combined_weights,
        'topsis_results': topsis_results,
        'gra_results': gra_results
    }


# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šä½¿ç”¨æŒ‡å—
# ============================================================

def print_usage_guide():
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                        ç»¼åˆè¯„ä»·æ¨¡å‹ä½¿ç”¨æŒ‡å—                               â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                          â•‘
    â•‘  ã€å¿«é€Ÿå¼€å§‹ã€‘                                                            â•‘
    â•‘                                                                          â•‘
    â•‘  from comprehensive_evaluation_tutorial import *                         â•‘
    â•‘                                                                          â•‘
    â•‘  # 1. å‡†å¤‡æ•°æ®                                                           â•‘
    â•‘  data = pd.DataFrame({                                                   â•‘
    â•‘      'æŒ‡æ ‡1': [85, 92, 78],                                              â•‘
    â•‘      'æŒ‡æ ‡2': [120, 95, 150],  # è´Ÿå‘                                    â•‘
    â•‘      'æŒ‡æ ‡3': [90, 85, 88]                                               â•‘
    â•‘  }, index=['æ–¹æ¡ˆA', 'æ–¹æ¡ˆB', 'æ–¹æ¡ˆC'])                                   â•‘
    â•‘  indicator_types = ['positive', 'negative', 'positive']                  â•‘
    â•‘                                                                          â•‘
    â•‘  # 2. é¢„å¤„ç†                                                             â•‘
    â•‘  preprocessor = DataPreprocessor()                                       â•‘
    â•‘  preprocessor.load_data(data, indicator_types=indicator_types)           â•‘
    â•‘  preprocessor.normalize('minmax')                                        â•‘
    â•‘  preprocessor.transform_negative_indicators()                            â•‘
    â•‘  processed_data = preprocessor.get_dataframe()                           â•‘
    â•‘                                                                          â•‘
    â•‘  # 3. è®¡ç®—æƒé‡                                                           â•‘
    â•‘  entropy = EntropyWeight()                                               â•‘
    â•‘  entropy.fit(processed_data)                                             â•‘
    â•‘  weights = entropy.get_weights()                                         â•‘
    â•‘                                                                          â•‘
    â•‘  # 4. TOPSISè¯„ä»·                                                         â•‘
    â•‘  topsis = TOPSIS()                                                       â•‘
    â•‘  topsis.fit(processed_data, weights)                                     â•‘
    â•‘  results = topsis.get_results()                                          â•‘
    â•‘                                                                          â•‘
    â•‘  # 5. å¯è§†åŒ–                                                             â•‘
    â•‘  visualizer = EvaluationVisualizer()                                     â•‘
    â•‘  visualizer.plot_topsis_results(results)                                 â•‘
    â•‘                                                                          â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ã€æŒ‡æ ‡ç±»å‹è¯´æ˜ã€‘                                                        â•‘
    â•‘  - 'positive': æ­£å‘æŒ‡æ ‡ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰å¦‚ï¼šæ”¶ç›Šã€è´¨é‡ã€æ•ˆç‡                  â•‘
    â•‘  - 'negative': è´Ÿå‘æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰å¦‚ï¼šæˆæœ¬ã€é£é™©ã€æ—¶é—´                  â•‘
    â•‘                                                                          â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ã€èµ‹æƒæ–¹æ³•é€‰æ‹©ã€‘                                                        â•‘
    â•‘  - ç†µæƒæ³•ï¼šé€‚ç”¨äºæŒ‡æ ‡é—´ç›¸å…³æ€§è¾ƒä½çš„æƒ…å†µ                                  â•‘
    â•‘  - CRITICæ³•ï¼šé€‚ç”¨äºæŒ‡æ ‡é—´å­˜åœ¨è¾ƒå¼ºç›¸å…³æ€§çš„æƒ…å†µ                            â•‘
    â•‘  - ç»„åˆèµ‹æƒï¼šç»¼åˆè€ƒè™‘ä¸¤ç§æ–¹æ³•ï¼Œæ›´åŠ ç¨³å¥                                  â•‘
    â•‘                                                                          â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ã€è®ºæ–‡å›¾è¡¨å»ºè®®ã€‘                                                        â•‘
    â•‘  Figure 1: è¯„ä»·æŒ‡æ ‡ä½“ç³»ï¼ˆæ ‘å½¢å›¾æˆ–è¡¨æ ¼ï¼‰                                  â•‘
    â•‘  Figure 2: ä¸åŒèµ‹æƒæ–¹æ³•æƒé‡å¯¹æ¯”                                          â•‘
    â•‘  Figure 3: TOPSISè¯„ä»·ç»“æœï¼ˆè´´è¿‘åº¦æ’åºï¼‰                                  â•‘
    â•‘  Figure 4: å¤šç»´åº¦é›·è¾¾å›¾å¯¹æ¯”                                              â•‘
    â•‘  Figure 5: ç»¼åˆè¯„ä»·æŠ¥å‘Š                                                  â•‘
    â•‘                                                                          â•‘
    â•‘  Table 1: åŸå§‹æ•°æ®çŸ©é˜µ                                                   â•‘
    â•‘  Table 2: æ ‡å‡†åŒ–çŸ©é˜µ                                                     â•‘
    â•‘  Table 3: æƒé‡è®¡ç®—è¿‡ç¨‹                                                   â•‘
    â•‘  Table 4: TOPSISè¯„ä»·ç»“æœ                                                 â•‘
    â•‘                                                                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


# ============================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´æ¡ˆä¾‹
    results = run_complete_example()
    
    # æ‰“å°ä½¿ç”¨æŒ‡å—
    print_usage_guide()
