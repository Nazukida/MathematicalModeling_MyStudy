"""
============================================================
ä¼˜åŒ–ç®—æ³•ï¼šç²’å­ç¾¤ä¼˜åŒ– (PSO) + é—ä¼ ç®—æ³• (GA) + å‚æ•°åæ¼”
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šå‡½æ•°ä¼˜åŒ–ã€å‚æ•°ä¼°è®¡ã€é€†é—®é¢˜æ±‚è§£ã€å¤æ‚æ–¹ç¨‹å‚æ•°åæ¼”
åŸç†ï¼šæ™ºèƒ½ä¼˜åŒ–ç®—æ³•æœç´¢æœ€ä¼˜å‚æ•°
ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ
============================================================

åº”ç”¨åœºæ™¯ï¼š
- å¤æ‚å‡½æ•°æœ€ä¼˜åŒ–
- æ¨¡å‹å‚æ•°æ ‡å®šï¼ˆé€†é—®é¢˜ï¼‰
- æœºå™¨å­¦ä¹ è¶…å‚æ•°è°ƒä¼˜
- å·¥ç¨‹è®¾è®¡ä¼˜åŒ–
- èµ„æºé…ç½®é—®é¢˜

æ ¸å¿ƒç®—æ³•ï¼š
1. PSOï¼šç¾¤ä½“æ™ºèƒ½ï¼Œæ¨¡æ‹Ÿé¸Ÿç¾¤è§…é£Ÿ
2. GAï¼šè¿›åŒ–ç®—æ³•ï¼Œæ¨¡æ‹Ÿè‡ªç„¶é€‰æ‹©
3. å·®åˆ†è¿›åŒ– (DE)ï¼šè¿ç»­ç©ºé—´ä¼˜åŒ–
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod
import warnings

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from visualization.plot_config import PlotStyleConfig, FigureSaver

PlotStyleConfig.setup_style()
warnings.filterwarnings('ignore')


class BaseOptimizer(ABC):
    """ä¼˜åŒ–ç®—æ³•åŸºç±»"""
    
    def __init__(self, objective_func, bounds, n_dims=None, 
                 max_iter=100, random_seed=42, verbose=True):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        :param objective_func: ç›®æ ‡å‡½æ•° f(x) -> scalarï¼ˆæœ€å°åŒ–ï¼‰
        :param bounds: å‚æ•°èŒƒå›´ [(low1, high1), (low2, high2), ...] æˆ– (low, high)
        :param n_dims: å‚æ•°ç»´åº¦ï¼ˆå¦‚æœboundsæ˜¯å…ƒç»„åˆ™éœ€è¦æŒ‡å®šï¼‰
        :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        :param random_seed: éšæœºç§å­
        :param verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.objective_func = objective_func
        self.max_iter = max_iter
        self.random_seed = random_seed
        self.verbose = verbose
        
        np.random.seed(random_seed)
        
        # å¤„ç†è¾¹ç•Œ
        if isinstance(bounds, tuple) and len(bounds) == 2:
            if n_dims is None:
                raise ValueError("ä½¿ç”¨(low, high)æ ¼å¼æ—¶éœ€è¦æŒ‡å®šn_dims")
            self.bounds = np.array([bounds] * n_dims)
        else:
            self.bounds = np.array(bounds)
        
        self.n_dims = len(self.bounds)
        self.lower_bounds = self.bounds[:, 0]
        self.upper_bounds = self.bounds[:, 1]
        
        # ç»“æœå­˜å‚¨
        self.best_solution = None
        self.best_value = np.inf
        self.history = {
            'best_values': [],
            'mean_values': [],
            'solutions': []
        }
        self.n_evaluations = 0
        
    @abstractmethod
    def optimize(self):
        """æ‰§è¡Œä¼˜åŒ–"""
        pass
    
    def _clip_to_bounds(self, x):
        """å°†è§£é™åˆ¶åœ¨è¾¹ç•Œå†…"""
        return np.clip(x, self.lower_bounds, self.upper_bounds)
    
    def _random_init(self, n_particles):
        """éšæœºåˆå§‹åŒ–ç§ç¾¤"""
        return np.random.uniform(
            self.lower_bounds, self.upper_bounds, 
            size=(n_particles, self.n_dims)
        )
    
    def _evaluate(self, x):
        """è¯„ä¼°ç›®æ ‡å‡½æ•°"""
        self.n_evaluations += 1
        return self.objective_func(x)
    
    def _print_header(self, algo_name):
        """æ‰“å°ç®—æ³•å¤´éƒ¨"""
        print("\n" + "="*60)
        print(f"ğŸ”§ {algo_name} ä¼˜åŒ–å¼€å§‹")
        print("="*60)
        print(f"  å‚æ•°ç»´åº¦: {self.n_dims}")
        print(f"  æœ€å¤§è¿­ä»£: {self.max_iter}")
        print("-"*60)
    
    def _print_progress(self, iteration, best_val, mean_val=None):
        """æ‰“å°è¿›åº¦"""
        if mean_val:
            print(f"  è¿­ä»£ {iteration:4d}: æœ€ä¼˜ = {best_val:.6f}, å¹³å‡ = {mean_val:.6f}")
        else:
            print(f"  è¿­ä»£ {iteration:4d}: æœ€ä¼˜ = {best_val:.6f}")
    
    def _print_results(self, algo_name):
        """æ‰“å°æœ€ç»ˆç»“æœ"""
        print("-"*60)
        print(f"âœ… {algo_name} ä¼˜åŒ–å®Œæˆ")
        print(f"\n  æœ€ä¼˜è§£:")
        for i, (val, (low, high)) in enumerate(zip(self.best_solution, self.bounds)):
            print(f"    x[{i}] = {val:.6f}  âˆˆ [{low}, {high}]")
        print(f"\n  æœ€ä¼˜ç›®æ ‡å€¼: {self.best_value:.6f}")
        print(f"  å‡½æ•°è¯„ä¼°æ¬¡æ•°: {self.n_evaluations}")
        print("="*60)
    
    def plot_convergence(self, save_path=None):
        """ç»˜åˆ¶æ”¶æ•›æ›²çº¿"""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        iterations = range(1, len(self.history['best_values']) + 1)
        
        ax.plot(iterations, self.history['best_values'], 
               color=PlotStyleConfig.COLORS['primary'], linewidth=2.5, 
               label='æœ€ä¼˜å€¼')
        
        if self.history['mean_values']:
            ax.plot(iterations, self.history['mean_values'],
                   color=PlotStyleConfig.COLORS['secondary'], linewidth=2,
                   linestyle='--', label='å¹³å‡å€¼', alpha=0.7)
        
        ax.axhline(self.best_value, color=PlotStyleConfig.COLORS['danger'],
                  linestyle=':', linewidth=1.5, alpha=0.7)
        
        ax.set_xlabel('è¿­ä»£æ¬¡æ•°', fontsize=12, fontweight='bold')
        ax.set_ylabel('ç›®æ ‡å‡½æ•°å€¼', fontsize=12, fontweight='bold')
        ax.set_title('ä¼˜åŒ–æ”¶æ•›æ›²çº¿', fontsize=14, fontweight='bold', pad=15)
        ax.legend(loc='upper right')
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, ax


class PSO(BaseOptimizer):
    """
    ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³• (Particle Swarm Optimization)
    
    æ ¸å¿ƒå…¬å¼ï¼š
    v(t+1) = w*v(t) + c1*r1*(pbest-x) + c2*r2*(gbest-x)
    x(t+1) = x(t) + v(t+1)
    
    ç‰¹ç‚¹ï¼š
    - æ”¶æ•›é€Ÿåº¦å¿«
    - å®ç°ç®€å•
    - é€‚åˆè¿ç»­ä¼˜åŒ–é—®é¢˜
    """
    
    def __init__(self, objective_func, bounds, n_dims=None, 
                 pop_size=30, max_iter=100,
                 w=0.7, c1=1.5, c2=1.5, w_decay=True,
                 random_seed=42, verbose=True):
        """
        åˆå§‹åŒ–PSO
        
        :param pop_size: ç²’å­æ•°é‡
        :param w: æƒ¯æ€§æƒé‡
        :param c1: ä¸ªä½“å­¦ä¹ å› å­
        :param c2: ç¤¾ä¼šå­¦ä¹ å› å­
        :param w_decay: æ˜¯å¦ä½¿ç”¨æƒ¯æ€§æƒé‡çº¿æ€§é€’å‡
        """
        super().__init__(objective_func, bounds, n_dims, max_iter, random_seed, verbose)
        
        self.pop_size = pop_size
        self.w = w
        self.w_init = w
        self.c1 = c1
        self.c2 = c2
        self.w_decay = w_decay
        
    def optimize(self):
        """æ‰§è¡ŒPSOä¼˜åŒ–"""
        if self.verbose:
            self._print_header("ç²’å­ç¾¤ä¼˜åŒ– (PSO)")
        
        # åˆå§‹åŒ–ç²’å­
        positions = self._random_init(self.pop_size)
        velocities = np.zeros_like(positions)
        
        # ä¸ªä½“æœ€ä¼˜
        personal_best_pos = positions.copy()
        personal_best_val = np.array([self._evaluate(p) for p in positions])
        
        # å…¨å±€æœ€ä¼˜
        global_best_idx = np.argmin(personal_best_val)
        global_best_pos = positions[global_best_idx].copy()
        global_best_val = personal_best_val[global_best_idx]
        
        # é€Ÿåº¦é™åˆ¶
        v_max = 0.2 * (self.upper_bounds - self.lower_bounds)
        
        # è¿­ä»£ä¼˜åŒ–
        for it in range(self.max_iter):
            # æƒ¯æ€§æƒé‡é€’å‡
            if self.w_decay:
                self.w = self.w_init - (self.w_init - 0.4) * (it / self.max_iter)
            
            # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
            r1 = np.random.rand(self.pop_size, self.n_dims)
            r2 = np.random.rand(self.pop_size, self.n_dims)
            
            velocities = (self.w * velocities + 
                         self.c1 * r1 * (personal_best_pos - positions) +
                         self.c2 * r2 * (global_best_pos - positions))
            
            # é™åˆ¶é€Ÿåº¦
            velocities = np.clip(velocities, -v_max, v_max)
            
            # æ›´æ–°ä½ç½®
            positions = self._clip_to_bounds(positions + velocities)
            
            # è¯„ä¼°
            fitness = np.array([self._evaluate(p) for p in positions])
            
            # æ›´æ–°ä¸ªä½“æœ€ä¼˜
            improved = fitness < personal_best_val
            personal_best_pos[improved] = positions[improved]
            personal_best_val[improved] = fitness[improved]
            
            # æ›´æ–°å…¨å±€æœ€ä¼˜
            best_idx = np.argmin(personal_best_val)
            if personal_best_val[best_idx] < global_best_val:
                global_best_val = personal_best_val[best_idx]
                global_best_pos = personal_best_pos[best_idx].copy()
            
            # è®°å½•å†å²
            self.history['best_values'].append(global_best_val)
            self.history['mean_values'].append(np.mean(fitness))
            
            if self.verbose and (it + 1) % max(1, self.max_iter // 10) == 0:
                self._print_progress(it + 1, global_best_val, np.mean(fitness))
        
        self.best_solution = global_best_pos
        self.best_value = global_best_val
        
        if self.verbose:
            self._print_results("PSO")
        
        return self.best_solution, self.best_value


class GeneticAlgorithm(BaseOptimizer):
    """
    é—ä¼ ç®—æ³• (Genetic Algorithm)
    
    æ ¸å¿ƒæ“ä½œï¼š
    1. é€‰æ‹©ï¼šé”¦æ ‡èµ›/è½®ç›˜èµŒ
    2. äº¤å‰ï¼šSBXäº¤å‰
    3. å˜å¼‚ï¼šå¤šé¡¹å¼å˜å¼‚
    
    ç‰¹ç‚¹ï¼š
    - å…¨å±€æœç´¢èƒ½åŠ›å¼º
    - é€‚åˆç¦»æ•£å’Œè¿ç»­é—®é¢˜
    - å¯å¹¶è¡ŒåŒ–
    """
    
    def __init__(self, objective_func, bounds, n_dims=None,
                 pop_size=50, max_iter=100,
                 crossover_rate=0.9, mutation_rate=0.1,
                 tournament_size=3, elitism=True,
                 random_seed=42, verbose=True):
        """
        åˆå§‹åŒ–GA
        
        :param pop_size: ç§ç¾¤å¤§å°
        :param crossover_rate: äº¤å‰æ¦‚ç‡
        :param mutation_rate: å˜å¼‚æ¦‚ç‡
        :param tournament_size: é”¦æ ‡èµ›å¤§å°
        :param elitism: æ˜¯å¦ä¿ç•™ç²¾è‹±
        """
        super().__init__(objective_func, bounds, n_dims, max_iter, random_seed, verbose)
        
        self.pop_size = pop_size
        self.crossover_rate = crossover_rate
        self.mutation_rate = mutation_rate
        self.tournament_size = tournament_size
        self.elitism = elitism
        
    def _tournament_selection(self, population, fitness):
        """é”¦æ ‡èµ›é€‰æ‹©"""
        selected = []
        for _ in range(self.pop_size):
            candidates = np.random.choice(self.pop_size, self.tournament_size, replace=False)
            winner = candidates[np.argmin(fitness[candidates])]
            selected.append(population[winner])
        return np.array(selected)
    
    def _sbx_crossover(self, parent1, parent2, eta=20):
        """æ¨¡æ‹ŸäºŒè¿›åˆ¶äº¤å‰ (SBX)"""
        if np.random.rand() > self.crossover_rate:
            return parent1.copy(), parent2.copy()
        
        child1, child2 = parent1.copy(), parent2.copy()
        
        for i in range(self.n_dims):
            if np.random.rand() < 0.5:
                if abs(parent1[i] - parent2[i]) > 1e-14:
                    u = np.random.rand()
                    if u <= 0.5:
                        beta = (2 * u) ** (1 / (eta + 1))
                    else:
                        beta = (1 / (2 * (1 - u))) ** (1 / (eta + 1))
                    
                    child1[i] = 0.5 * ((1 + beta) * parent1[i] + (1 - beta) * parent2[i])
                    child2[i] = 0.5 * ((1 - beta) * parent1[i] + (1 + beta) * parent2[i])
        
        return self._clip_to_bounds(child1), self._clip_to_bounds(child2)
    
    def _polynomial_mutation(self, individual, eta=20):
        """å¤šé¡¹å¼å˜å¼‚"""
        mutant = individual.copy()
        
        for i in range(self.n_dims):
            if np.random.rand() < self.mutation_rate:
                u = np.random.rand()
                if u < 0.5:
                    delta = (2 * u) ** (1 / (eta + 1)) - 1
                else:
                    delta = 1 - (2 * (1 - u)) ** (1 / (eta + 1))
                
                mutant[i] += delta * (self.upper_bounds[i] - self.lower_bounds[i])
        
        return self._clip_to_bounds(mutant)
    
    def optimize(self):
        """æ‰§è¡ŒGAä¼˜åŒ–"""
        if self.verbose:
            self._print_header("é—ä¼ ç®—æ³• (GA)")
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = self._random_init(self.pop_size)
        fitness = np.array([self._evaluate(ind) for ind in population])
        
        # è®°å½•æœ€ä¼˜
        best_idx = np.argmin(fitness)
        self.best_solution = population[best_idx].copy()
        self.best_value = fitness[best_idx]
        
        # è¿­ä»£è¿›åŒ–
        for it in range(self.max_iter):
            # é€‰æ‹©
            selected = self._tournament_selection(population, fitness)
            
            # äº¤å‰
            offspring = []
            for i in range(0, self.pop_size, 2):
                p1, p2 = selected[i], selected[min(i+1, self.pop_size-1)]
                c1, c2 = self._sbx_crossover(p1, p2)
                offspring.extend([c1, c2])
            offspring = np.array(offspring[:self.pop_size])
            
            # å˜å¼‚
            offspring = np.array([self._polynomial_mutation(ind) for ind in offspring])
            
            # è¯„ä¼°å­ä»£
            offspring_fitness = np.array([self._evaluate(ind) for ind in offspring])
            
            # ç²¾è‹±ä¿ç•™
            if self.elitism:
                worst_idx = np.argmax(offspring_fitness)
                if self.best_value < offspring_fitness[worst_idx]:
                    offspring[worst_idx] = self.best_solution.copy()
                    offspring_fitness[worst_idx] = self.best_value
            
            # æ›´æ–°ç§ç¾¤
            population = offspring
            fitness = offspring_fitness
            
            # æ›´æ–°æœ€ä¼˜
            best_idx = np.argmin(fitness)
            if fitness[best_idx] < self.best_value:
                self.best_value = fitness[best_idx]
                self.best_solution = population[best_idx].copy()
            
            # è®°å½•å†å²
            self.history['best_values'].append(self.best_value)
            self.history['mean_values'].append(np.mean(fitness))
            
            if self.verbose and (it + 1) % max(1, self.max_iter // 10) == 0:
                self._print_progress(it + 1, self.best_value, np.mean(fitness))
        
        if self.verbose:
            self._print_results("GA")
        
        return self.best_solution, self.best_value


class DifferentialEvolution(BaseOptimizer):
    """
    å·®åˆ†è¿›åŒ–ç®—æ³• (Differential Evolution)
    
    æ ¸å¿ƒæ“ä½œï¼š
    å˜å¼‚: v = x_r1 + F * (x_r2 - x_r3)
    äº¤å‰: äºŒé¡¹å¼äº¤å‰
    é€‰æ‹©: è´ªå©ªé€‰æ‹©
    
    ç‰¹ç‚¹ï¼š
    - è¿ç»­ä¼˜åŒ–æ•ˆæœå¥½
    - å‚æ•°å°‘ï¼Œæ˜“è°ƒèŠ‚
    - é€‚åˆé«˜ç»´é—®é¢˜
    """
    
    def __init__(self, objective_func, bounds, n_dims=None,
                 pop_size=50, max_iter=100,
                 F=0.8, CR=0.9, strategy='best/1/bin',
                 random_seed=42, verbose=True):
        """
        åˆå§‹åŒ–DE
        
        :param F: ç¼©æ”¾å› å­ (0.4-1.0)
        :param CR: äº¤å‰æ¦‚ç‡ (0.5-1.0)
        :param strategy: å˜å¼‚ç­–ç•¥ 'rand/1/bin', 'best/1/bin', 'rand/2/bin'
        """
        super().__init__(objective_func, bounds, n_dims, max_iter, random_seed, verbose)
        
        self.pop_size = pop_size
        self.F = F
        self.CR = CR
        self.strategy = strategy
        
    def optimize(self):
        """æ‰§è¡ŒDEä¼˜åŒ–"""
        if self.verbose:
            self._print_header("å·®åˆ†è¿›åŒ– (DE)")
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = self._random_init(self.pop_size)
        fitness = np.array([self._evaluate(ind) for ind in population])
        
        # è®°å½•æœ€ä¼˜
        best_idx = np.argmin(fitness)
        self.best_solution = population[best_idx].copy()
        self.best_value = fitness[best_idx]
        
        # è¿­ä»£è¿›åŒ–
        for it in range(self.max_iter):
            for i in range(self.pop_size):
                # é€‰æ‹©å˜å¼‚ä¸ªä½“
                idxs = [j for j in range(self.pop_size) if j != i]
                
                if 'best' in self.strategy:
                    base = self.best_solution
                    r = np.random.choice(idxs, 2, replace=False)
                    mutant = base + self.F * (population[r[0]] - population[r[1]])
                else:
                    r = np.random.choice(idxs, 3, replace=False)
                    mutant = population[r[0]] + self.F * (population[r[1]] - population[r[2]])
                
                mutant = self._clip_to_bounds(mutant)
                
                # äº¤å‰
                trial = population[i].copy()
                j_rand = np.random.randint(self.n_dims)
                for j in range(self.n_dims):
                    if np.random.rand() < self.CR or j == j_rand:
                        trial[j] = mutant[j]
                
                # é€‰æ‹©
                trial_fitness = self._evaluate(trial)
                if trial_fitness < fitness[i]:
                    population[i] = trial
                    fitness[i] = trial_fitness
                    
                    if trial_fitness < self.best_value:
                        self.best_value = trial_fitness
                        self.best_solution = trial.copy()
            
            # è®°å½•å†å²
            self.history['best_values'].append(self.best_value)
            self.history['mean_values'].append(np.mean(fitness))
            
            if self.verbose and (it + 1) % max(1, self.max_iter // 10) == 0:
                self._print_progress(it + 1, self.best_value, np.mean(fitness))
        
        if self.verbose:
            self._print_results("DE")
        
        return self.best_solution, self.best_value


class ParameterInversion:
    """
    å‚æ•°åæ¼”/æ ‡å®šå·¥å…·
    
    åœºæ™¯ï¼šç»™å®šæ­£å‘æ¨¡å‹å’Œè§‚æµ‹æ•°æ®ï¼Œåæ¨æ¨¡å‹å‚æ•°
    
    åº”ç”¨ï¼š
    - æ¨¡å‹å‚æ•°æ ‡å®š
    - ç³»ç»Ÿè¾¨è¯†
    - é€†é—®é¢˜æ±‚è§£
    """
    
    def __init__(self, forward_model, param_bounds, verbose=True):
        """
        åˆå§‹åŒ–å‚æ•°åæ¼”å™¨
        
        :param forward_model: æ­£å‘æ¨¡å‹ f(params) -> predictions
        :param param_bounds: å‚æ•°èŒƒå›´ [(low1, high1), ...]
        """
        self.forward_model = forward_model
        self.param_bounds = param_bounds
        self.verbose = verbose
        self.n_params = len(param_bounds)
        
        self.best_params = None
        self.best_rmse = None
        self.optimizer = None
        
    def objective(self, params, observations, weights=None):
        """ç›®æ ‡å‡½æ•°ï¼šåŠ æƒRMSE"""
        predictions = self.forward_model(params)
        residuals = observations - predictions
        
        if weights is None:
            return np.sqrt(np.mean(residuals**2))
        else:
            return np.sqrt(np.average(residuals**2, weights=weights))
    
    def fit(self, observations, method='pso', weights=None, **kwargs):
        """
        æ‰§è¡Œå‚æ•°åæ¼”
        
        :param observations: è§‚æµ‹æ•°æ®
        :param method: ä¼˜åŒ–æ–¹æ³• 'pso', 'ga', 'de'
        :param weights: è§‚æµ‹æƒé‡ï¼ˆå¯é€‰ï¼‰
        :param kwargs: ä¼˜åŒ–å™¨å‚æ•°
        """
        # å®šä¹‰ç›®æ ‡å‡½æ•°
        def obj_func(params):
            return self.objective(params, observations, weights)
        
        # é€‰æ‹©ä¼˜åŒ–å™¨
        if method.lower() == 'pso':
            self.optimizer = PSO(obj_func, self.param_bounds, verbose=self.verbose, **kwargs)
        elif method.lower() == 'ga':
            self.optimizer = GeneticAlgorithm(obj_func, self.param_bounds, verbose=self.verbose, **kwargs)
        elif method.lower() == 'de':
            self.optimizer = DifferentialEvolution(obj_func, self.param_bounds, verbose=self.verbose, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        # æ‰§è¡Œä¼˜åŒ–
        self.best_params, self.best_rmse = self.optimizer.optimize()
        
        return self.best_params, self.best_rmse
    
    def predict(self, params=None):
        """ä½¿ç”¨å‚æ•°è¿›è¡Œé¢„æµ‹"""
        if params is None:
            params = self.best_params
        return self.forward_model(params)
    
    def sensitivity_analysis(self, observations, n_samples=100):
        """
        å‚æ•°æ•æ„Ÿæ€§åˆ†æ
        
        :return: å„å‚æ•°çš„æ•æ„Ÿæ€§æŒ‡æ ‡
        """
        sensitivities = []
        
        for i in range(self.n_params):
            # åœ¨æœ€ä¼˜è§£é™„è¿‘æ‰°åŠ¨
            param_range = np.linspace(
                self.param_bounds[i][0], 
                self.param_bounds[i][1], 
                n_samples
            )
            
            rmses = []
            for val in param_range:
                params = self.best_params.copy()
                params[i] = val
                rmse = self.objective(params, observations)
                rmses.append(rmse)
            
            # æ•æ„Ÿæ€§ = RMSEå˜åŒ–èŒƒå›´
            sensitivity = max(rmses) - min(rmses)
            sensitivities.append({
                'param_idx': i,
                'sensitivity': sensitivity,
                'rmse_range': (min(rmses), max(rmses))
            })
        
        if self.verbose:
            print("\n" + "="*55)
            print("ğŸ“Š å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
            print("="*55)
            for s in sorted(sensitivities, key=lambda x: -x['sensitivity']):
                print(f"  å‚æ•° {s['param_idx']}: æ•æ„Ÿæ€§ = {s['sensitivity']:.4f}")
            print("="*55)
        
        return sensitivities
    
    def plot_fit(self, observations, x=None, save_path=None):
        """ç»˜åˆ¶æ‹Ÿåˆæ•ˆæœ"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        predictions = self.predict()
        
        if x is None:
            x = np.arange(len(observations))
        
        # æ‹Ÿåˆå¯¹æ¯”
        axes[0].scatter(x, observations, color=PlotStyleConfig.COLORS['primary'],
                       s=60, alpha=0.7, label='è§‚æµ‹å€¼', edgecolors='white')
        axes[0].plot(x, predictions, color=PlotStyleConfig.COLORS['danger'],
                    linewidth=2.5, label='æ¨¡å‹é¢„æµ‹')
        axes[0].set_xlabel('X', fontweight='bold')
        axes[0].set_ylabel('Y', fontweight='bold')
        axes[0].set_title('æ¨¡å‹æ‹Ÿåˆæ•ˆæœ', fontsize=12, fontweight='bold')
        axes[0].legend()
        
        # æ®‹å·®å›¾
        residuals = observations - predictions
        axes[1].scatter(predictions, residuals, color=PlotStyleConfig.COLORS['secondary'],
                       s=60, alpha=0.7, edgecolors='white')
        axes[1].axhline(0, color='gray', linestyle='--', linewidth=1.5)
        axes[1].set_xlabel('é¢„æµ‹å€¼', fontweight='bold')
        axes[1].set_ylabel('æ®‹å·®', fontweight='bold')
        axes[1].set_title('æ®‹å·®åˆ†æ', fontsize=12, fontweight='bold')
        
        for ax in axes:
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
        
        plt.suptitle(f'å‚æ•°åæ¼”ç»“æœ (RMSE = {self.best_rmse:.4f})', 
                    fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, axes


# ==================== æ ‡å‡†æµ‹è¯•å‡½æ•° ====================

class BenchmarkFunctions:
    """æ ‡å‡†æµ‹è¯•å‡½æ•°"""
    
    @staticmethod
    def sphere(x):
        """çƒå‡½æ•° - æœ€ç®€å•çš„å•å³°å‡½æ•°"""
        return np.sum(x**2)
    
    @staticmethod
    def rastrigin(x):
        """Rastriginå‡½æ•° - å¤šå³°å‡½æ•°"""
        A = 10
        return A * len(x) + np.sum(x**2 - A * np.cos(2 * np.pi * x))
    
    @staticmethod
    def rosenbrock(x):
        """Rosenbrockå‡½æ•° - é¦™è•‰å½¢å±±è°·"""
        return np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)
    
    @staticmethod
    def ackley(x):
        """Ackleyå‡½æ•°"""
        n = len(x)
        sum1 = np.sum(x**2)
        sum2 = np.sum(np.cos(2 * np.pi * x))
        return -20 * np.exp(-0.2 * np.sqrt(sum1/n)) - np.exp(sum2/n) + 20 + np.e


def compare_optimizers(objective_func, bounds, n_dims, max_iter=100, n_runs=5):
    """
    æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½
    """
    results = {}
    
    for name, Optimizer in [('PSO', PSO), ('GA', GeneticAlgorithm), ('DE', DifferentialEvolution)]:
        values = []
        for seed in range(n_runs):
            opt = Optimizer(objective_func, bounds, n_dims=n_dims, 
                           max_iter=max_iter, random_seed=seed, verbose=False)
            _, best_val = opt.optimize()
            values.append(best_val)
        
        results[name] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'best': np.min(values),
            'worst': np.max(values)
        }
    
    print("\n" + "="*60)
    print("ğŸ“Š ä¼˜åŒ–å™¨æ€§èƒ½æ¯”è¾ƒ")
    print("="*60)
    print(f"  {'ç®—æ³•':<8} {'å¹³å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'æœ€ä¼˜':<12} {'æœ€å·®':<12}")
    print("  " + "-"*52)
    for name, r in results.items():
        print(f"  {name:<8} {r['mean']:<12.6f} {r['std']:<12.6f} {r['best']:<12.6f} {r['worst']:<12.6f}")
    print("="*60)
    
    return results


if __name__ == "__main__":
    print("="*60)
    print("ğŸ”§ ä¼˜åŒ–ç®—æ³•ä¸å‚æ•°åæ¼”æ¼”ç¤º")
    print("="*60)
    
    # ================== ç¤ºä¾‹1: å‡½æ•°ä¼˜åŒ– ==================
    print("\n" + "="*60)
    print("ç¤ºä¾‹1: Rastriginå‡½æ•°ä¼˜åŒ–")
    print("="*60)
    
    # ä½¿ç”¨PSOä¼˜åŒ–Rastriginå‡½æ•°
    pso = PSO(
        objective_func=BenchmarkFunctions.rastrigin,
        bounds=(-5.12, 5.12),
        n_dims=5,
        pop_size=30,
        max_iter=100
    )
    best_x, best_val = pso.optimize()
    
    fig1, ax1 = pso.plot_convergence()
    plt.show()
    
    # ================== ç¤ºä¾‹2: å‚æ•°åæ¼” ==================
    print("\n" + "="*60)
    print("ç¤ºä¾‹2: å‚æ•°åæ¼”ï¼ˆé€†é—®é¢˜ï¼‰")
    print("="*60)
    
    # å®šä¹‰æ­£å‘æ¨¡å‹ï¼šy = a*sin(b*x + c) + d
    def forward_model(params):
        a, b, c, d = params
        x = np.linspace(0, 2*np.pi, 50)
        return a * np.sin(b * x + c) + d
    
    # ç”Ÿæˆè§‚æµ‹æ•°æ®ï¼ˆçœŸå®å‚æ•°ï¼ša=3, b=2, c=0.5, d=1ï¼‰
    true_params = [3, 2, 0.5, 1]
    x = np.linspace(0, 2*np.pi, 50)
    observations = forward_model(true_params) + np.random.normal(0, 0.2, 50)
    
    print(f"çœŸå®å‚æ•°: a={true_params[0]}, b={true_params[1]}, c={true_params[2]}, d={true_params[3]}")
    
    # å‚æ•°åæ¼”
    inverter = ParameterInversion(
        forward_model=forward_model,
        param_bounds=[(0, 5), (0, 5), (-np.pi, np.pi), (-5, 5)]
    )
    best_params, rmse = inverter.fit(observations, method='de', max_iter=100)
    
    print(f"\nåæ¼”å‚æ•°: a={best_params[0]:.3f}, b={best_params[1]:.3f}, "
          f"c={best_params[2]:.3f}, d={best_params[3]:.3f}")
    
    fig2, axes2 = inverter.plot_fit(observations, x)
    plt.show()
    
    # æ•æ„Ÿæ€§åˆ†æ
    inverter.sensitivity_analysis(observations)
    
    # ================== ç¤ºä¾‹3: ç®—æ³•æ¯”è¾ƒ ==================
    print("\n" + "="*60)
    print("ç¤ºä¾‹3: ä¼˜åŒ–ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("="*60)
    
    compare_optimizers(BenchmarkFunctions.rastrigin, (-5.12, 5.12), n_dims=10, max_iter=100, n_runs=5)
    
    print("\nâœ… ä¼˜åŒ–ç®—æ³•æ¼”ç¤ºå®Œæˆ!")
