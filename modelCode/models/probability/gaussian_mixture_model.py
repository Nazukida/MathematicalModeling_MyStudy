"""
============================================================
é«˜æ–¯æ··åˆæ¨¡å‹ (Gaussian Mixture Model, GMM)
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šèšç±»åˆ†æã€å¯†åº¦ä¼°è®¡ã€å¼‚å¸¸æ£€æµ‹ã€è½¯åˆ†ç±»
åŸç†ï¼šå‡è®¾æ•°æ®ç”±å¤šä¸ªé«˜æ–¯åˆ†å¸ƒæ··åˆç”Ÿæˆï¼Œä½¿ç”¨EMç®—æ³•ä¼°è®¡å‚æ•°
ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ
============================================================

åº”ç”¨åœºæ™¯ï¼š
- å®¢æˆ·åˆ†ç¾¤ï¼ˆè½¯èšç±»ï¼‰
- å›¾åƒåˆ†å‰²
- è¯­éŸ³è¯†åˆ«
- å¼‚å¸¸æ£€æµ‹ï¼ˆä½æ¦‚ç‡åŒºåŸŸï¼‰
- æ•°æ®å¯†åº¦å»ºæ¨¡

æ•°å­¦æ¨¡å‹ï¼š
p(x) = Î£ Ï€_k * N(x | Î¼_k, Î£_k)
å…¶ä¸­ Ï€_k æ˜¯æ··åˆæƒé‡ï¼Œæ»¡è¶³ Î£Ï€_k = 1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from scipy import stats
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import warnings

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from visualization.plot_config import PlotStyleConfig, FigureSaver

PlotStyleConfig.setup_style()
warnings.filterwarnings('ignore')


class GMMClustering:
    """
    é«˜æ–¯æ··åˆæ¨¡å‹èšç±»åˆ†æç±»
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è‡ªåŠ¨ç¡®å®šæœ€ä½³èšç±»æ•°ï¼ˆBIC/AICï¼‰
    2. EMç®—æ³•å‚æ•°ä¼°è®¡
    3. è½¯èšç±»ï¼ˆæ¦‚ç‡åˆ†é…ï¼‰
    4. å¼‚å¸¸æ£€æµ‹
    5. ä¸°å¯Œçš„å¯è§†åŒ–
    """
    
    def __init__(self, n_components='auto', covariance_type='full', 
                 max_components=10, random_state=42, verbose=True):
        """
        åˆå§‹åŒ–GMMæ¨¡å‹
        
        :param n_components: èšç±»æ•°ï¼ˆ'auto'è‡ªåŠ¨é€‰æ‹©ï¼‰
        :param covariance_type: åæ–¹å·®ç±»å‹ 
                               'full'(å®Œå…¨), 'tied'(å…±äº«), 
                               'diag'(å¯¹è§’), 'spherical'(çƒå½¢)
        :param max_components: è‡ªåŠ¨é€‰æ‹©æ—¶çš„æœ€å¤§èšç±»æ•°
        :param random_state: éšæœºç§å­
        :param verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.n_components = n_components
        self.covariance_type = covariance_type
        self.max_components = max_components
        self.random_state = random_state
        self.verbose = verbose
        
        self.model = None
        self.scaler = None
        self.X_scaled = None
        self.labels_ = None
        self.probabilities_ = None
        self.bic_scores = None
        self.aic_scores = None
        self.optimal_k = None
        self.feature_names = None
        
    def fit(self, X, scale=True):
        """
        æ‹ŸåˆGMMæ¨¡å‹
        
        :param X: ç‰¹å¾æ•°æ®ï¼ˆDataFrameæˆ–æ•°ç»„ï¼‰
        :param scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®
        :return: self
        """
        # å¤„ç†è¾“å…¥
        if isinstance(X, pd.DataFrame):
            self.feature_names = list(X.columns)
            X_array = X.values
        else:
            X_array = np.array(X)
            self.feature_names = [f'ç‰¹å¾{i+1}' for i in range(X_array.shape[1])]
        
        # æ ‡å‡†åŒ–
        if scale:
            self.scaler = StandardScaler()
            self.X_scaled = self.scaler.fit_transform(X_array)
        else:
            self.X_scaled = X_array
        
        # è‡ªåŠ¨é€‰æ‹©èšç±»æ•°
        if self.n_components == 'auto':
            self._find_optimal_k()
            self.n_components = self.optimal_k
        
        # æ‹Ÿåˆæ¨¡å‹
        self.model = GaussianMixture(
            n_components=self.n_components,
            covariance_type=self.covariance_type,
            random_state=self.random_state,
            n_init=5
        )
        self.model.fit(self.X_scaled)
        
        # é¢„æµ‹
        self.labels_ = self.model.predict(self.X_scaled)
        self.probabilities_ = self.model.predict_proba(self.X_scaled)
        
        if self.verbose:
            self._print_results()
        
        return self
    
    def _find_optimal_k(self):
        """ä½¿ç”¨BIC/AICå‡†åˆ™å¯»æ‰¾æœ€ä¼˜èšç±»æ•°"""
        self.bic_scores = []
        self.aic_scores = []
        k_range = range(1, self.max_components + 1)
        
        for k in k_range:
            gmm = GaussianMixture(
                n_components=k,
                covariance_type=self.covariance_type,
                random_state=self.random_state,
                n_init=3
            )
            gmm.fit(self.X_scaled)
            self.bic_scores.append(gmm.bic(self.X_scaled))
            self.aic_scores.append(gmm.aic(self.X_scaled))
        
        # ä½¿ç”¨BICé€‰æ‹©ï¼ˆBICæƒ©ç½šå¤æ‚æ¨¡å‹ï¼‰
        self.optimal_k = k_range[np.argmin(self.bic_scores)]
        
        if self.verbose:
            print(f"\n  ğŸ” è‡ªåŠ¨é€‰æ‹©èšç±»æ•°: k = {self.optimal_k} (åŸºäºBIC)")
    
    def _print_results(self):
        """æ‰“å°æ‹Ÿåˆç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š é«˜æ–¯æ··åˆæ¨¡å‹ (GMM) èšç±»ç»“æœ")
        print("="*60)
        print(f"\n  æ ·æœ¬é‡: {len(self.X_scaled)}")
        print(f"  ç‰¹å¾æ•°: {self.X_scaled.shape[1]}")
        print(f"  èšç±»æ•°: {self.n_components}")
        print(f"  åæ–¹å·®ç±»å‹: {self.covariance_type}")
        
        print(f"\n  å„ç°‡ç»Ÿè®¡:")
        print("  " + "-"*50)
        print(f"  {'ç°‡':^6} {'æ ·æœ¬æ•°':^10} {'å æ¯”':^10} {'æ··åˆæƒé‡':^12}")
        print("  " + "-"*50)
        
        for k in range(self.n_components):
            n_k = np.sum(self.labels_ == k)
            pct = n_k / len(self.labels_) * 100
            weight = self.model.weights_[k]
            print(f"  {k:^6} {n_k:^10} {pct:^9.1f}% {weight:^12.4f}")
        
        print("  " + "-"*50)
        print(f"\n  æ¨¡å‹è¯„ä¼°:")
        print(f"    å¯¹æ•°ä¼¼ç„¶: {self.model.score(self.X_scaled):.4f}")
        print(f"    BIC: {self.model.bic(self.X_scaled):.2f}")
        print(f"    AIC: {self.model.aic(self.X_scaled):.2f}")
        print(f"    æ”¶æ•›: {'æ˜¯' if self.model.converged_ else 'å¦'}")
        print("="*60)
    
    def predict(self, X_new):
        """é¢„æµ‹æ–°æ•°æ®çš„ç°‡æ ‡ç­¾"""
        if self.scaler:
            X_new_scaled = self.scaler.transform(X_new)
        else:
            X_new_scaled = X_new
        return self.model.predict(X_new_scaled)
    
    def predict_proba(self, X_new):
        """é¢„æµ‹æ–°æ•°æ®å±äºå„ç°‡çš„æ¦‚ç‡"""
        if self.scaler:
            X_new_scaled = self.scaler.transform(X_new)
        else:
            X_new_scaled = X_new
        return self.model.predict_proba(X_new_scaled)
    
    def get_cluster_summary(self):
        """
        è·å–å„ç°‡çš„ç»Ÿè®¡æ‘˜è¦
        
        :return: DataFrameï¼ŒåŒ…å«å„ç°‡çš„å‡å€¼ç­‰ç»Ÿè®¡ä¿¡æ¯
        """
        summary = []
        
        # è·å–åŸå§‹å°ºåº¦çš„å‡å€¼
        if self.scaler:
            means = self.scaler.inverse_transform(self.model.means_)
        else:
            means = self.model.means_
        
        for k in range(self.n_components):
            cluster_info = {'ç°‡': k, 'æ ·æœ¬æ•°': np.sum(self.labels_ == k)}
            cluster_info['æƒé‡'] = self.model.weights_[k]
            
            for i, name in enumerate(self.feature_names):
                cluster_info[f'{name}_å‡å€¼'] = means[k, i]
            
            summary.append(cluster_info)
        
        return pd.DataFrame(summary)
    
    def detect_anomalies(self, threshold=0.01):
        """
        åŸºäºå¯†åº¦çš„å¼‚å¸¸æ£€æµ‹
        
        :param threshold: æ¦‚ç‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºå¼‚å¸¸ï¼‰
        :return: å¼‚å¸¸æ ·æœ¬çš„ç´¢å¼•
        """
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¯¹æ•°ä¼¼ç„¶
        log_prob = self.model.score_samples(self.X_scaled)
        
        # è½¬æ¢ä¸ºæ¦‚ç‡å¯†åº¦
        prob = np.exp(log_prob)
        
        # ä½¿ç”¨åˆ†ä½æ•°ç¡®å®šé˜ˆå€¼
        cutoff = np.percentile(prob, threshold * 100)
        anomalies = np.where(prob < cutoff)[0]
        
        if self.verbose:
            print(f"\n  ğŸ” å¼‚å¸¸æ£€æµ‹: å‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸ç‚¹ (é˜ˆå€¼: {threshold*100:.1f}%)")
        
        return anomalies, prob
    
    def sample(self, n_samples=100):
        """ä»æ‹Ÿåˆçš„GMMç”Ÿæˆæ–°æ ·æœ¬"""
        samples, labels = self.model.sample(n_samples)
        if self.scaler:
            samples = self.scaler.inverse_transform(samples)
        return samples, labels
    
    # ==================== å¯è§†åŒ–æ–¹æ³• ====================
    
    def plot_bic_aic(self, save_path=None):
        """ç»˜åˆ¶BIC/AICæ›²çº¿ï¼ˆç”¨äºé€‰æ‹©èšç±»æ•°ï¼‰"""
        if self.bic_scores is None:
            print("éœ€è¦å…ˆä½¿ç”¨ n_components='auto' æ‹Ÿåˆæ¨¡å‹")
            return None, None
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        k_range = range(1, len(self.bic_scores) + 1)
        
        ax.plot(k_range, self.bic_scores, 'o-', color=PlotStyleConfig.COLORS['primary'],
               linewidth=2.5, markersize=8, label='BIC')
        ax.plot(k_range, self.aic_scores, 's--', color=PlotStyleConfig.COLORS['secondary'],
               linewidth=2.5, markersize=8, label='AIC')
        
        # æ ‡è®°æœ€ä¼˜ç‚¹
        ax.axvline(self.optimal_k, color=PlotStyleConfig.COLORS['accent'],
                  linestyle=':', linewidth=2, label=f'æœ€ä¼˜ k={self.optimal_k}')
        ax.scatter([self.optimal_k], [self.bic_scores[self.optimal_k-1]], 
                  s=200, color=PlotStyleConfig.COLORS['danger'], zorder=5, marker='*')
        
        ax.set_xlabel('èšç±»æ•° k', fontsize=12, fontweight='bold')
        ax.set_ylabel('ä¿¡æ¯å‡†åˆ™å€¼', fontsize=12, fontweight='bold')
        ax.set_title('GMM èšç±»æ•°é€‰æ‹© (BIC/AIC)', fontsize=14, fontweight='bold', pad=15)
        ax.legend(loc='upper right')
        ax.set_xticks(k_range)
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, ax
    
    def plot_clusters_2d(self, feature_indices=(0, 1), show_ellipse=True, save_path=None):
        """
        2Dèšç±»å¯è§†åŒ–
        
        :param feature_indices: æ˜¾ç¤ºçš„ä¸¤ä¸ªç‰¹å¾ç´¢å¼•
        :param show_ellipse: æ˜¯å¦æ˜¾ç¤ºåæ–¹å·®æ¤­åœ†
        """
        fig, ax = plt.subplots(figsize=(10, 8))
        
        i, j = feature_indices
        X_plot = self.X_scaled[:, [i, j]]
        
        colors = PlotStyleConfig.get_palette(self.n_components)
        
        # ç»˜åˆ¶æ•£ç‚¹
        for k in range(self.n_components):
            mask = self.labels_ == k
            ax.scatter(X_plot[mask, 0], X_plot[mask, 1], 
                      c=colors[k], label=f'ç°‡ {k}', s=50, alpha=0.6, edgecolors='white')
        
        # ç»˜åˆ¶åæ–¹å·®æ¤­åœ†
        if show_ellipse:
            for k in range(self.n_components):
                mean = self.model.means_[k, [i, j]]
                
                if self.covariance_type == 'full':
                    cov = self.model.covariances_[k][[i, j], :][:, [i, j]]
                elif self.covariance_type == 'tied':
                    cov = self.model.covariances_[[i, j], :][:, [i, j]]
                elif self.covariance_type == 'diag':
                    cov = np.diag(self.model.covariances_[k, [i, j]])
                else:  # spherical
                    cov = np.eye(2) * self.model.covariances_[k]
                
                self._draw_ellipse(ax, mean, cov, colors[k])
        
        # ç»˜åˆ¶ç°‡ä¸­å¿ƒ
        centers = self.model.means_[:, [i, j]]
        ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, 
                  marker='X', edgecolors='white', linewidth=2, label='ç°‡ä¸­å¿ƒ', zorder=5)
        
        ax.set_xlabel(f'{self.feature_names[i]} (æ ‡å‡†åŒ–)', fontsize=12, fontweight='bold')
        ax.set_ylabel(f'{self.feature_names[j]} (æ ‡å‡†åŒ–)', fontsize=12, fontweight='bold')
        ax.set_title('GMM èšç±»ç»“æœ (2D)', fontsize=14, fontweight='bold', pad=15)
        ax.legend(loc='upper right')
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, ax
    
    def _draw_ellipse(self, ax, mean, cov, color, n_std=2):
        """ç»˜åˆ¶åæ–¹å·®æ¤­åœ†"""
        eigenvalues, eigenvectors = np.linalg.eigh(cov)
        order = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[order]
        eigenvectors = eigenvectors[:, order]
        
        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))
        width, height = 2 * n_std * np.sqrt(eigenvalues)
        
        ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle,
                         facecolor=color, alpha=0.2, edgecolor=color, linewidth=2)
        ax.add_patch(ellipse)
    
    def plot_probability_heatmap(self, save_path=None):
        """ç»˜åˆ¶æ ·æœ¬å½’å±æ¦‚ç‡çƒ­åŠ›å›¾"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # é€‰å–éƒ¨åˆ†æ ·æœ¬ï¼ˆå¤ªå¤šåˆ™é‡‡æ ·ï¼‰
        n_show = min(100, len(self.probabilities_))
        indices = np.random.choice(len(self.probabilities_), n_show, replace=False)
        probs = self.probabilities_[sorted(indices)]
        
        im = ax.imshow(probs, aspect='auto', cmap='YlOrRd')
        
        ax.set_xlabel('ç°‡', fontsize=12, fontweight='bold')
        ax.set_ylabel('æ ·æœ¬', fontsize=12, fontweight='bold')
        ax.set_title('æ ·æœ¬å½’å±æ¦‚ç‡åˆ†å¸ƒ (è½¯èšç±»)', fontsize=14, fontweight='bold', pad=15)
        ax.set_xticks(range(self.n_components))
        ax.set_xticklabels([f'ç°‡{k}' for k in range(self.n_components)])
        
        cbar = fig.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label('å½’å±æ¦‚ç‡', fontweight='bold')
        
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, ax
    
    def plot_density_contour(self, feature_indices=(0, 1), n_points=100, save_path=None):
        """ç»˜åˆ¶GMMå¯†åº¦ç­‰é«˜çº¿"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        i, j = feature_indices
        X_plot = self.X_scaled[:, [i, j]]
        
        # åˆ›å»ºç½‘æ ¼
        x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1
        y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, n_points),
                            np.linspace(y_min, y_max, n_points))
        
        # æ„å»ºå®Œæ•´ç‰¹å¾ç©ºé—´çš„ç½‘æ ¼ç‚¹
        grid_points = np.zeros((n_points * n_points, self.X_scaled.shape[1]))
        grid_points[:, i] = xx.ravel()
        grid_points[:, j] = yy.ravel()
        # å…¶ä»–ç‰¹å¾ç”¨å‡å€¼å¡«å……
        for k in range(self.X_scaled.shape[1]):
            if k not in [i, j]:
                grid_points[:, k] = self.X_scaled[:, k].mean()
        
        # è®¡ç®—å¯†åº¦
        Z = np.exp(self.model.score_samples(grid_points))
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶ç­‰é«˜çº¿
        contour = ax.contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.7)
        ax.contour(xx, yy, Z, levels=10, colors='white', alpha=0.5, linewidths=0.5)
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        ax.scatter(X_plot[:, 0], X_plot[:, 1], c='white', s=20, alpha=0.5, edgecolors='black')
        
        # ç»˜åˆ¶ç°‡ä¸­å¿ƒ
        centers = self.model.means_[:, [i, j]]
        ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, 
                  marker='X', edgecolors='white', linewidth=2, zorder=5)
        
        ax.set_xlabel(f'{self.feature_names[i]}', fontsize=12, fontweight='bold')
        ax.set_ylabel(f'{self.feature_names[j]}', fontsize=12, fontweight='bold')
        ax.set_title('GMM å¯†åº¦ä¼°è®¡ç­‰é«˜çº¿', fontsize=14, fontweight='bold', pad=15)
        
        cbar = fig.colorbar(contour, ax=ax, shrink=0.8)
        cbar.set_label('æ¦‚ç‡å¯†åº¦', fontweight='bold')
        
        plt.tight_layout()
        
        if save_path:
            saver = FigureSaver(os.path.dirname(save_path))
            saver.save(fig, os.path.basename(save_path).split('.')[0])
        
        return fig, ax


def generate_gmm_sample_data(n_samples=500, n_clusters=3, n_features=2, random_state=42):
    """
    ç”ŸæˆGMMæµ‹è¯•æ•°æ®
    
    :return: Xæ•°æ®, çœŸå®æ ‡ç­¾
    """
    np.random.seed(random_state)
    
    samples_per_cluster = n_samples // n_clusters
    X = []
    y = []
    
    # éšæœºç”Ÿæˆèšç±»ä¸­å¿ƒ
    centers = np.random.randn(n_clusters, n_features) * 5
    
    for k in range(n_clusters):
        # éšæœºåæ–¹å·®
        A = np.random.randn(n_features, n_features)
        cov = A @ A.T / n_features + np.eye(n_features) * 0.5
        
        samples = np.random.multivariate_normal(centers[k], cov, samples_per_cluster)
        X.append(samples)
        y.extend([k] * samples_per_cluster)
    
    return np.vstack(X), np.array(y)


if __name__ == "__main__":
    print("="*60)
    print("ğŸ“Š é«˜æ–¯æ··åˆæ¨¡å‹ (GMM) æ¼”ç¤º")
    print("="*60)
    
    # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
    X, y_true = generate_gmm_sample_data(n_samples=500, n_clusters=3, n_features=4)
    feature_names = ['ç‰¹å¾A', 'ç‰¹å¾B', 'ç‰¹å¾C', 'ç‰¹å¾D']
    df = pd.DataFrame(X, columns=feature_names)
    
    print(f"\næ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"çœŸå®èšç±»æ•°: {len(np.unique(y_true))}")
    
    # 2. è‡ªåŠ¨é€‰æ‹©èšç±»æ•°å¹¶æ‹Ÿåˆ
    gmm = GMMClustering(n_components='auto', max_components=8)
    gmm.fit(df)
    
    # 3. è·å–èšç±»æ‘˜è¦
    summary = gmm.get_cluster_summary()
    print("\nç°‡ç»Ÿè®¡æ‘˜è¦:")
    print(summary)
    
    # 4. å¼‚å¸¸æ£€æµ‹
    anomalies, probs = gmm.detect_anomalies(threshold=0.02)
    
    # 5. å¯è§†åŒ–
    fig1, ax1 = gmm.plot_bic_aic()
    plt.show()
    
    fig2, ax2 = gmm.plot_clusters_2d(feature_indices=(0, 1))
    plt.show()
    
    fig3, ax3 = gmm.plot_probability_heatmap()
    plt.show()
    
    fig4, ax4 = gmm.plot_density_contour(feature_indices=(0, 1))
    plt.show()
    
    # 6. ç”Ÿæˆæ–°æ ·æœ¬
    new_samples, new_labels = gmm.sample(20)
    print(f"\nç”Ÿæˆ {len(new_samples)} ä¸ªæ–°æ ·æœ¬")
    
    print("\nâœ… GMM æ¼”ç¤ºå®Œæˆ!")
