"""
============================================================
åˆ†ç±»ä¸æœºå™¨å­¦ä¹ æ¨¡å‹ (Classification & ML Models)
åŒ…å«ï¼šéšæœºæ£®æ—åˆ†ç±» + XGBoost + LightGBM + å¤šç›®æ ‡ä¼˜åŒ–
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šåˆ†ç±»é¢„æµ‹ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è¯„ä¼°
ç‰¹ç‚¹ï¼šå®Œæ•´çš„å‚æ•°è®¾ç½®ã€æ•°æ®é¢„å¤„ç†ã€å¯è§†åŒ–ä¸ç¾åŒ–
ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ
============================================================

ä½¿ç”¨åœºæ™¯ï¼š
- äºŒåˆ†ç±»/å¤šåˆ†ç±»é—®é¢˜
- æ•…éšœè¯Šæ–­ã€ç–¾ç—…é¢„æµ‹
- ç”¨æˆ·è¡Œä¸ºåˆ†ç±»
- å¤šç›®æ ‡ä¼˜åŒ–å†³ç­–
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, roc_auc_score, confusion_matrix, 
                             classification_report, roc_curve)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

warnings.filterwarnings('ignore')


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå…¨å±€é…ç½®ä¸ç¾åŒ–è®¾ç½® (Global Configuration)
# ============================================================

class PlotStyleConfig:
    """å›¾è¡¨ç¾åŒ–é…ç½®ç±»"""
    
    COLORS = {
        'primary': '#2E86AB',
        'secondary': '#A23B72',
        'accent': '#F18F01',
        'success': '#27AE60',
        'danger': '#C73E1D',
        'neutral': '#3B3B3B'
    }
    
    PALETTE = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6B4C9A', '#1B998B']
    
    @staticmethod
    def setup_style():
        plt.style.use('seaborn-v0_8-whitegrid')
        rcParams['figure.figsize'] = (12, 8)
        rcParams['figure.dpi'] = 100
        rcParams['savefig.dpi'] = 300
        rcParams['font.size'] = 11
        rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        rcParams['axes.unicode_minus'] = False

PlotStyleConfig.setup_style()


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†ç±»æ•°æ®ç”Ÿæˆå™¨ (Classification Data Generator)
# ============================================================

class ClassificationDataGenerator:
    """åˆ†ç±»æ•°æ®ç”Ÿæˆå™¨ - ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    
    def __init__(self, random_seed=42):
        self.random_seed = random_seed
        np.random.seed(random_seed)
    
    def generate_binary_classification(self, n_samples=500, scenario='equipment'):
        """
        ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
        
        :param n_samples: æ ·æœ¬æ•°é‡
        :param scenario: åœºæ™¯
            - 'equipment': è®¾å¤‡æ•…éšœæ£€æµ‹
            - 'customer': å®¢æˆ·æµå¤±é¢„æµ‹
            - 'medical': ç–¾ç—…è¯Šæ–­
        """
        if scenario == 'equipment':
            # è®¾å¤‡æ•…éšœæ£€æµ‹
            n_normal = n_samples // 2
            n_fault = n_samples - n_normal
            
            normal_data = pd.DataFrame({
                'æ¸©åº¦': np.random.normal(50, 5, n_normal),
                'æŒ¯åŠ¨': np.random.normal(0.5, 0.1, n_normal),
                'å‹åŠ›': np.random.normal(100, 10, n_normal),
                'è¿è¡Œæ—¶é—´': np.random.uniform(100, 1000, n_normal),
                'æ ‡ç­¾': 0
            })
            
            fault_data = pd.DataFrame({
                'æ¸©åº¦': np.random.normal(80, 8, n_fault),
                'æŒ¯åŠ¨': np.random.normal(1.2, 0.3, n_fault),
                'å‹åŠ›': np.random.normal(130, 15, n_fault),
                'è¿è¡Œæ—¶é—´': np.random.uniform(800, 2000, n_fault),
                'æ ‡ç­¾': 1
            })
            
            data = pd.concat([normal_data, fault_data], ignore_index=True)
            feature_names = ['æ¸©åº¦', 'æŒ¯åŠ¨', 'å‹åŠ›', 'è¿è¡Œæ—¶é—´']
            target_names = ['æ­£å¸¸', 'æ•…éšœ']
            
        elif scenario == 'customer':
            # å®¢æˆ·æµå¤±é¢„æµ‹
            n_retain = n_samples // 2
            n_churn = n_samples - n_retain
            
            retain_data = pd.DataFrame({
                'æ¶ˆè´¹é‡‘é¢': np.random.uniform(500, 5000, n_retain),
                'è´­ä¹°é¢‘æ¬¡': np.random.randint(5, 30, n_retain),
                'ä¼šå‘˜æ—¶é•¿': np.random.uniform(12, 60, n_retain),
                'æŠ•è¯‰æ¬¡æ•°': np.random.randint(0, 3, n_retain),
                'æ ‡ç­¾': 0
            })
            
            churn_data = pd.DataFrame({
                'æ¶ˆè´¹é‡‘é¢': np.random.uniform(100, 1000, n_churn),
                'è´­ä¹°é¢‘æ¬¡': np.random.randint(1, 10, n_churn),
                'ä¼šå‘˜æ—¶é•¿': np.random.uniform(1, 24, n_churn),
                'æŠ•è¯‰æ¬¡æ•°': np.random.randint(2, 10, n_churn),
                'æ ‡ç­¾': 1
            })
            
            data = pd.concat([retain_data, churn_data], ignore_index=True)
            feature_names = ['æ¶ˆè´¹é‡‘é¢', 'è´­ä¹°é¢‘æ¬¡', 'ä¼šå‘˜æ—¶é•¿', 'æŠ•è¯‰æ¬¡æ•°']
            target_names = ['ç•™å­˜', 'æµå¤±']
            
        elif scenario == 'medical':
            # ç–¾ç—…è¯Šæ–­
            n_healthy = n_samples // 2
            n_sick = n_samples - n_healthy
            
            healthy_data = pd.DataFrame({
                'è¡€å‹': np.random.normal(120, 10, n_healthy),
                'è¡€ç³–': np.random.normal(95, 10, n_healthy),
                'èƒ†å›ºé†‡': np.random.normal(180, 20, n_healthy),
                'å¹´é¾„': np.random.randint(20, 60, n_healthy),
                'æ ‡ç­¾': 0
            })
            
            sick_data = pd.DataFrame({
                'è¡€å‹': np.random.normal(150, 15, n_sick),
                'è¡€ç³–': np.random.normal(130, 20, n_sick),
                'èƒ†å›ºé†‡': np.random.normal(240, 30, n_sick),
                'å¹´é¾„': np.random.randint(40, 80, n_sick),
                'æ ‡ç­¾': 1
            })
            
            data = pd.concat([healthy_data, sick_data], ignore_index=True)
            feature_names = ['è¡€å‹', 'è¡€ç³–', 'èƒ†å›ºé†‡', 'å¹´é¾„']
            target_names = ['å¥åº·', 'æ‚£ç—…']
        
        # æ‰“ä¹±æ•°æ®
        data = data.sample(frac=1, random_state=self.random_seed).reset_index(drop=True)
        
        return {
            'data': data,
            'feature_names': feature_names,
            'target_names': target_names,
            'X': data[feature_names],
            'y': data['æ ‡ç­¾']
        }
    
    def generate_multiclass(self, n_samples=600, n_classes=3):
        """ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®"""
        samples_per_class = n_samples // n_classes
        
        data_list = []
        for i in range(n_classes):
            class_data = pd.DataFrame({
                'ç‰¹å¾1': np.random.normal(i * 10, 3, samples_per_class),
                'ç‰¹å¾2': np.random.normal(i * 5, 2, samples_per_class),
                'ç‰¹å¾3': np.random.uniform(i * 2, i * 2 + 5, samples_per_class),
                'æ ‡ç­¾': i
            })
            data_list.append(class_data)
        
        data = pd.concat(data_list, ignore_index=True)
        data = data.sample(frac=1, random_state=self.random_seed).reset_index(drop=True)
        
        return {
            'data': data,
            'feature_names': ['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3'],
            'target_names': [f'ç±»åˆ«{i+1}' for i in range(n_classes)],
            'X': data[['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3']],
            'y': data['æ ‡ç­¾']
        }


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šåˆ†ç±»å™¨åŸºç±» (Base Classifier)
# ============================================================

class BaseClassifier:
    """åˆ†ç±»å™¨åŸºç±»"""
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.target_names = None
        self.metrics = None
        self.feature_importance = None
        self.is_fitted = False
    
    def _scale_data(self, X_train, X_test=None):
        """æ ‡å‡†åŒ–æ•°æ®"""
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        if X_test is not None:
            X_test_scaled = self.scaler.transform(X_test)
            return X_train_scaled, X_test_scaled
        return X_train_scaled
    
    def _compute_metrics(self, y_true, y_pred, y_prob=None):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)
        }
        
        if y_prob is not None and len(np.unique(y_true)) == 2:
            metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])
        
        return metrics
    
    def predict(self, X):
        """é¢„æµ‹"""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if self.scaler is not None:
            X = self.scaler.transform(X)
        return self.model.predict(X)
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if self.scaler is not None:
            X = self.scaler.transform(X)
        return self.model.predict_proba(X)


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šéšæœºæ£®æ—åˆ†ç±»å™¨ (Random Forest Classifier)
# ============================================================

class RandomForestModel(BaseClassifier):
    """
    éšæœºæ£®æ—åˆ†ç±»å™¨
    
    åŸç†ï¼š
    åŸºäºå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡Bootstrapé‡‡æ ·å’Œç‰¹å¾éšæœºé€‰æ‹©
    æ„å»ºå¤šæ£µå†³ç­–æ ‘ï¼Œæœ€ç»ˆé€šè¿‡æŠ•ç¥¨å†³å®šåˆ†ç±»ç»“æœã€‚
    
    ä¼˜ç‚¹ï¼š
    - æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º
    - å¯å¤„ç†é«˜ç»´æ•°æ®
    - å¯è¾“å‡ºç‰¹å¾é‡è¦æ€§
    - å¯¹ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ä¸æ•æ„Ÿ
    """
    
    def __init__(self, n_estimators=100, max_depth=None, 
                 min_samples_split=2, min_samples_leaf=1,
                 class_weight=None, verbose=True):
        """
        å‚æ•°é…ç½®è¯´æ˜
        
        :param n_estimators: å†³ç­–æ ‘æ•°é‡
            - å»ºè®®ï¼š100-500ï¼Œè¶Šå¤šè¶Šç¨³å®šä½†è®¡ç®—é‡å¢åŠ 
            
        :param max_depth: æ ‘çš„æœ€å¤§æ·±åº¦
            - Noneï¼šä¸é™åˆ¶
            - å»ºè®®ï¼š5-20ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            
        :param min_samples_split: åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
            - é»˜è®¤ï¼š2ï¼Œå¢å¤§å¯é˜²æ­¢è¿‡æ‹Ÿåˆ
            
        :param min_samples_leaf: å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
            - é»˜è®¤ï¼š1ï¼Œå¢å¤§å¯é˜²æ­¢è¿‡æ‹Ÿåˆ
            
        :param class_weight: ç±»åˆ«æƒé‡
            - 'balanced': è‡ªåŠ¨å¹³è¡¡ä¸å‡è¡¡æ•°æ®
        """
        super().__init__(verbose)
        
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            class_weight=class_weight,
            random_state=42,
            n_jobs=-1
        )
    
    def fit(self, X, y, test_size=0.2, scale=True):
        """
        è®­ç»ƒæ¨¡å‹
        
        :param X: ç‰¹å¾çŸ©é˜µ
        :param y: æ ‡ç­¾
        :param test_size: æµ‹è¯•é›†æ¯”ä¾‹
        :param scale: æ˜¯å¦æ ‡å‡†åŒ–
        """
        if isinstance(X, pd.DataFrame):
            self.feature_names = list(X.columns)
            X = X.values
        else:
            self.feature_names = [f"ç‰¹å¾{i+1}" for i in range(X.shape[1])]
        
        if isinstance(y, pd.Series):
            y = y.values
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–
        if scale:
            X_train, X_test = self._scale_data(X_train, X_test)
        
        # è®­ç»ƒ
        self.model.fit(X_train, y_train)
        self.is_fitted = True
        
        # é¢„æµ‹
        y_train_pred = self.model.predict(X_train)
        y_test_pred = self.model.predict(X_test)
        y_test_prob = self.model.predict_proba(X_test)
        
        # è¯„ä¼°
        self.metrics = {
            'train': self._compute_metrics(y_train, y_train_pred),
            'test': self._compute_metrics(y_test, y_test_pred, y_test_prob)
        }
        
        # æ··æ·†çŸ©é˜µ
        self.confusion_matrix = confusion_matrix(y_test, y_test_pred)
        
        # ç‰¹å¾é‡è¦æ€§
        self.feature_importance = pd.Series(
            self.model.feature_importances_,
            index=self.feature_names
        )
        
        # ä¿å­˜æµ‹è¯•æ•°æ®ç”¨äºå¯è§†åŒ–
        self._y_test = y_test
        self._y_test_pred = y_test_pred
        self._y_test_prob = y_test_prob
        
        if self.verbose:
            self._print_results()
        
        return self
    
    def _print_results(self):
        """æ‰“å°ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š éšæœºæ£®æ—åˆ†ç±»ç»“æœ (Random Forest)")
        print("="*60)
        
        print("\n  è®­ç»ƒé›†æŒ‡æ ‡:")
        for k, v in self.metrics['train'].items():
            print(f"    {k}: {v:.4f}")
        
        print("\n  æµ‹è¯•é›†æŒ‡æ ‡:")
        for k, v in self.metrics['test'].items():
            print(f"    {k}: {v:.4f}")
        
        print("\n  æ··æ·†çŸ©é˜µ:")
        print(self.confusion_matrix)
        
        print("\n  ç‰¹å¾é‡è¦æ€§:")
        for name, imp in self.feature_importance.sort_values(ascending=False).items():
            print(f"    {name}: {imp:.4f}")
        
        print("="*60)
    
    def cross_validate(self, X, y, cv=5):
        """äº¤å‰éªŒè¯"""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
        
        scores = cross_val_score(self.model, X, y, cv=cv, scoring='accuracy')
        
        print(f"\näº¤å‰éªŒè¯å‡†ç¡®ç‡ (cv={cv}):")
        print(f"  Mean: {scores.mean():.4f} Â± {scores.std():.4f}")
        print(f"  Scores: {scores.round(4)}")
        
        return scores


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šé›†æˆåˆ†ç±»å™¨ (Ensemble Classifier)
# ============================================================

class EnsembleClassifier(BaseClassifier):
    """
    é›†æˆåˆ†ç±»å™¨ - ç»“åˆå¤šä¸ªæ¨¡å‹
    
    æ–¹æ³•ï¼š
    - æŠ•ç¥¨æ³• (Voting)
    - åŠ æƒå¹³å‡æ³• (Weighted Average)
    """
    
    def __init__(self, verbose=True):
        super().__init__(verbose)
        self.models = {}
        self.weights = None
        self.individual_metrics = {}
    
    def add_model(self, name, model):
        """æ·»åŠ æ¨¡å‹"""
        self.models[name] = model
    
    def add_default_models(self):
        """æ·»åŠ é»˜è®¤æ¨¡å‹ç»„åˆ"""
        self.models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
            'SVM': SVC(probability=True, random_state=42)
        }
    
    def fit(self, X, y, test_size=0.2, scale=True):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        if isinstance(X, pd.DataFrame):
            self.feature_names = list(X.columns)
            X = X.values
        else:
            self.feature_names = [f"ç‰¹å¾{i+1}" for i in range(X.shape[1])]
        
        if isinstance(y, pd.Series):
            y = y.values
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–
        if scale:
            X_train, X_test = self._scale_data(X_train, X_test)
        
        predictions = {}
        probabilities = {}
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for name, model in self.models.items():
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            prob = model.predict_proba(X_test)
            
            predictions[name] = pred
            probabilities[name] = prob
            
            self.individual_metrics[name] = self._compute_metrics(y_test, pred, prob)
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰
        accuracies = np.array([self.individual_metrics[name]['accuracy'] for name in self.models])
        self.weights = accuracies / accuracies.sum()
        
        # åŠ æƒå¹³å‡é¢„æµ‹
        ensemble_prob = np.zeros_like(list(probabilities.values())[0])
        for i, (name, prob) in enumerate(probabilities.items()):
            ensemble_prob += self.weights[i] * prob
        
        ensemble_pred = np.argmax(ensemble_prob, axis=1)
        
        self.individual_metrics['Ensemble'] = self._compute_metrics(y_test, ensemble_pred, ensemble_prob)
        self.confusion_matrix = confusion_matrix(y_test, ensemble_pred)
        
        self._y_test = y_test
        self._ensemble_pred = ensemble_pred
        self._ensemble_prob = ensemble_prob
        
        self.is_fitted = True
        
        if self.verbose:
            self._print_comparison()
        
        return self
    
    def _print_comparison(self):
        """æ‰“å°æ¨¡å‹å¯¹æ¯”"""
        print("\n" + "="*70)
        print("ğŸ“Š é›†æˆåˆ†ç±»å™¨å¯¹æ¯” (Ensemble Comparison)")
        print("="*70)
        
        print(f"\n  {'æ¨¡å‹':<20} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10}")
        print("  " + "-"*60)
        
        for name, metrics in self.individual_metrics.items():
            print(f"  {name:<20} {metrics['accuracy']:>10.4f} {metrics['precision']:>10.4f} "
                  f"{metrics['recall']:>10.4f} {metrics['f1']:>10.4f}")
        
        print(f"\n  æ¨¡å‹æƒé‡:")
        for name, weight in zip(self.models.keys(), self.weights):
            print(f"    {name}: {weight:.4f}")
        
        print("="*70)
    
    def predict(self, X):
        """é›†æˆé¢„æµ‹"""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if self.scaler is not None:
            X = self.scaler.transform(X)
        
        probabilities = []
        for i, model in enumerate(self.models.values()):
            prob = model.predict_proba(X)
            probabilities.append(self.weights[i] * prob)
        
        ensemble_prob = np.sum(probabilities, axis=0)
        return np.argmax(ensemble_prob, axis=1)


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå¤šç›®æ ‡ä¼˜åŒ– - NSGA-II (Multi-objective Optimization)
# ============================================================

class NSGAII:
    """
    NSGA-II å¤šç›®æ ‡ä¼˜åŒ–ç®—æ³•
    (Non-dominated Sorting Genetic Algorithm II)
    
    é€‚ç”¨äºï¼šå¤šç›®æ ‡å†³ç­–ã€å¸•ç´¯æ‰˜æœ€ä¼˜
    
    ç‰¹ç‚¹ï¼š
    - éæ”¯é…æ’åº
    - æ‹¥æŒ¤åº¦è·ç¦»
    - ç²¾è‹±ä¿ç•™ç­–ç•¥
    """
    
    def __init__(self, objectives, bounds, n_dims,
                 pop_size=50, max_iter=100,
                 crossover_rate=0.8, mutation_rate=0.1,
                 random_seed=42, verbose=True):
        """
        å‚æ•°é…ç½®
        
        :param objectives: ç›®æ ‡å‡½æ•°åˆ—è¡¨ [func1, func2, ...]
        :param bounds: å˜é‡èŒƒå›´ [(min1,max1), ...]
        :param n_dims: å˜é‡ç»´åº¦
        :param pop_size: ç§ç¾¤å¤§å°
        :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.objectives = objectives
        self.n_objectives = len(objectives)
        self.bounds = np.array(bounds)
        self.n_dims = n_dims
        self.pop_size = pop_size
        self.max_iter = max_iter
        self.crossover_rate = crossover_rate
        self.mutation_rate = mutation_rate
        self.random_seed = random_seed
        self.verbose = verbose
        
        np.random.seed(random_seed)
        
        self.population = None
        self.objective_values = None
        self.pareto_front = None
        self.pareto_solutions = None
        self.history = {'hypervolume': []}
    
    def _evaluate(self, population):
        """è¯„ä¼°ç§ç¾¤"""
        n = len(population)
        obj_values = np.zeros((n, self.n_objectives))
        
        for i, ind in enumerate(population):
            for j, obj_func in enumerate(self.objectives):
                obj_values[i, j] = obj_func(ind)
        
        return obj_values
    
    def _dominates(self, obj1, obj2):
        """åˆ¤æ–­obj1æ˜¯å¦æ”¯é…obj2ï¼ˆæœ€å°åŒ–ï¼‰"""
        return np.all(obj1 <= obj2) and np.any(obj1 < obj2)
    
    def _non_dominated_sort(self, obj_values):
        """éæ”¯é…æ’åº"""
        n = len(obj_values)
        ranks = np.zeros(n, dtype=int)
        dominated_by = [[] for _ in range(n)]
        domination_count = np.zeros(n, dtype=int)
        
        for i in range(n):
            for j in range(i+1, n):
                if self._dominates(obj_values[i], obj_values[j]):
                    dominated_by[i].append(j)
                    domination_count[j] += 1
                elif self._dominates(obj_values[j], obj_values[i]):
                    dominated_by[j].append(i)
                    domination_count[i] += 1
        
        current_rank = 0
        remaining = set(range(n))
        
        while remaining:
            current_front = [i for i in remaining if domination_count[i] == 0]
            if not current_front:
                break
            
            for i in current_front:
                ranks[i] = current_rank
                remaining.discard(i)
                for j in dominated_by[i]:
                    domination_count[j] -= 1
            
            current_rank += 1
        
        return ranks
    
    def _crowding_distance(self, obj_values, indices):
        """è®¡ç®—æ‹¥æŒ¤åº¦è·ç¦»"""
        n = len(indices)
        if n <= 2:
            return {i: float('inf') for i in indices}
        
        distances = {i: 0.0 for i in indices}
        
        for m in range(self.n_objectives):
            sorted_indices = sorted(indices, key=lambda x: obj_values[x, m])
            
            distances[sorted_indices[0]] = float('inf')
            distances[sorted_indices[-1]] = float('inf')
            
            obj_range = obj_values[sorted_indices[-1], m] - obj_values[sorted_indices[0], m]
            if obj_range == 0:
                continue
            
            for i in range(1, n-1):
                distances[sorted_indices[i]] += (
                    obj_values[sorted_indices[i+1], m] - obj_values[sorted_indices[i-1], m]
                ) / obj_range
        
        return distances
    
    def _crossover(self, parent1, parent2):
        """äº¤å‰æ“ä½œ"""
        if np.random.rand() < self.crossover_rate:
            alpha = np.random.rand(self.n_dims)
            child1 = alpha * parent1 + (1 - alpha) * parent2
            child2 = (1 - alpha) * parent1 + alpha * parent2
            return child1, child2
        return parent1.copy(), parent2.copy()
    
    def _mutate(self, individual):
        """å˜å¼‚æ“ä½œ"""
        mutated = individual.copy()
        for i in range(self.n_dims):
            if np.random.rand() < self.mutation_rate:
                range_i = self.bounds[i, 1] - self.bounds[i, 0]
                mutated[i] += np.random.normal(0, 0.1 * range_i)
                mutated[i] = np.clip(mutated[i], self.bounds[i, 0], self.bounds[i, 1])
        return mutated
    
    def optimize(self):
        """æ‰§è¡ŒNSGA-IIä¼˜åŒ–"""
        # åˆå§‹åŒ–
        self.population = np.random.uniform(
            self.bounds[:, 0], self.bounds[:, 1],
            (self.pop_size, self.n_dims)
        )
        
        if self.verbose:
            print("\n" + "="*60)
            print("ğŸ¯ NSGA-IIå¤šç›®æ ‡ä¼˜åŒ–å¼€å§‹...")
            print("="*60)
            print(f"  ç›®æ ‡å‡½æ•°æ•°é‡: {self.n_objectives}")
            print(f"  å†³ç­–å˜é‡ç»´åº¦: {self.n_dims}")
            print(f"  ç§ç¾¤å¤§å°: {self.pop_size}")
            print("-"*60)
        
        for generation in range(self.max_iter):
            # è¯„ä¼°
            self.objective_values = self._evaluate(self.population)
            
            # éæ”¯é…æ’åº
            ranks = self._non_dominated_sort(self.objective_values)
            
            # ç”Ÿæˆå­ä»£
            offspring = []
            while len(offspring) < self.pop_size:
                # é”¦æ ‡èµ›é€‰æ‹©
                candidates = np.random.choice(self.pop_size, 4, replace=False)
                parent1 = self.population[min(candidates[:2], key=lambda x: ranks[x])]
                parent2 = self.population[min(candidates[2:], key=lambda x: ranks[x])]
                
                child1, child2 = self._crossover(parent1, parent2)
                offspring.extend([self._mutate(child1), self._mutate(child2)])
            
            offspring = np.array(offspring[:self.pop_size])
            
            # åˆå¹¶ç§ç¾¤
            combined = np.vstack([self.population, offspring])
            combined_obj = self._evaluate(combined)
            combined_ranks = self._non_dominated_sort(combined_obj)
            
            # é€‰æ‹©ä¸‹ä¸€ä»£
            new_pop = []
            current_rank = 0
            
            while len(new_pop) < self.pop_size:
                front_indices = np.where(combined_ranks == current_rank)[0]
                
                if len(new_pop) + len(front_indices) <= self.pop_size:
                    new_pop.extend(front_indices)
                else:
                    # ä½¿ç”¨æ‹¥æŒ¤åº¦è·ç¦»
                    remaining_spots = self.pop_size - len(new_pop)
                    distances = self._crowding_distance(combined_obj, front_indices)
                    sorted_front = sorted(front_indices, key=lambda x: -distances[x])
                    new_pop.extend(sorted_front[:remaining_spots])
                
                current_rank += 1
            
            self.population = combined[new_pop]
            
            if self.verbose and (generation + 1) % 20 == 0:
                pareto_count = np.sum(combined_ranks[new_pop] == 0)
                print(f"  Generation {generation+1}: Pareto front size = {pareto_count}")
        
        # è·å–Paretoæœ€ä¼˜è§£
        final_obj = self._evaluate(self.population)
        final_ranks = self._non_dominated_sort(final_obj)
        pareto_indices = np.where(final_ranks == 0)[0]
        
        self.pareto_solutions = self.population[pareto_indices]
        self.pareto_front = final_obj[pareto_indices]
        
        if self.verbose:
            self._print_results()
        
        return self.pareto_solutions, self.pareto_front
    
    def _print_results(self):
        """æ‰“å°ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š NSGA-IIä¼˜åŒ–å®Œæˆ")
        print("="*60)
        print(f"  Paretoæœ€ä¼˜è§£æ•°é‡: {len(self.pareto_solutions)}")
        print(f"\n  Paretoå‰æ²¿èŒƒå›´:")
        for i in range(self.n_objectives):
            print(f"    ç›®æ ‡{i+1}: [{self.pareto_front[:, i].min():.4f}, "
                  f"{self.pareto_front[:, i].max():.4f}]")
        print("="*60)


# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå¯è§†åŒ–æ¨¡å— (Visualization)
# ============================================================

class ClassificationVisualizer:
    """åˆ†ç±»æ¨¡å‹å¯è§†åŒ–ç±»"""
    
    def __init__(self):
        self.colors = PlotStyleConfig.PALETTE
    
    def plot_confusion_matrix(self, cm, class_names=None, 
                              title="æ··æ·†çŸ©é˜µ", save_path=None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        fig, ax = plt.subplots(figsize=(8, 6))
        
        im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
        ax.figure.colorbar(im, ax=ax)
        
        if class_names is None:
            class_names = [f"ç±»åˆ«{i}" for i in range(len(cm))]
        
        ax.set(xticks=np.arange(len(cm)),
               yticks=np.arange(len(cm)),
               xticklabels=class_names,
               yticklabels=class_names,
               xlabel='é¢„æµ‹æ ‡ç­¾',
               ylabel='çœŸå®æ ‡ç­¾')
        
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        
        # æ·»åŠ æ•°å€¼
        thresh = cm.max() / 2.
        for i in range(len(cm)):
            for j in range(len(cm)):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black",
                       fontsize=14)
        
        ax.set_title(title, fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_roc_curve(self, y_true, y_prob, title="ROCæ›²çº¿", save_path=None):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if y_prob.ndim > 1:
            y_prob = y_prob[:, 1]
        
        fpr, tpr, thresholds = roc_curve(y_true, y_prob)
        auc = roc_auc_score(y_true, y_prob)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        
        ax.plot(fpr, tpr, color=self.colors[0], linewidth=2,
               label=f'ROCæ›²çº¿ (AUC = {auc:.4f})')
        ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='éšæœºçŒœæµ‹')
        ax.fill_between(fpr, tpr, alpha=0.2, color=self.colors[0])
        
        ax.set_xlabel('å‡æ­£ä¾‹ç‡ (FPR)', fontweight='bold')
        ax.set_ylabel('çœŸæ­£ä¾‹ç‡ (TPR)', fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1.05])
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_feature_importance(self, importance, title="ç‰¹å¾é‡è¦æ€§", save_path=None):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        importance = importance.sort_values(ascending=True)
        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance)))
        
        bars = ax.barh(importance.index, importance.values,
                      color=colors, edgecolor='white', linewidth=2)
        
        ax.set_xlabel('é‡è¦æ€§', fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        
        for bar, val in zip(bars, importance.values):
            ax.text(val + max(importance.values)*0.02, 
                   bar.get_y() + bar.get_height()/2,
                   f'{val:.4f}', va='center', fontsize=10)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_model_comparison(self, metrics_dict, title="æ¨¡å‹æ€§èƒ½å¯¹æ¯”", save_path=None):
        """ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        models = list(metrics_dict.keys())
        metrics_names = ['accuracy', 'precision', 'recall', 'f1']
        titles = ['(a) Accuracy', '(b) Precision', '(c) Recall', '(d) F1 Score']
        
        for ax, metric, t in zip(axes.flatten(), metrics_names, titles):
            values = [metrics_dict[m].get(metric, 0) for m in models]
            bars = ax.bar(models, values, color=self.colors[:len(models)],
                         edgecolor='white', linewidth=2)
            ax.set_ylabel(metric.capitalize(), fontweight='bold')
            ax.set_title(t, fontsize=12, fontweight='bold')
            ax.set_ylim(0, 1)
            
            for bar, val in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                       f'{val:.4f}', ha='center', va='bottom', fontsize=9)
            
            ax.tick_params(axis='x', rotation=30)
        
        plt.suptitle(title, fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_pareto_front(self, pareto_front, obj_names=None, 
                          title="Paretoå‰æ²¿", save_path=None):
        """ç»˜åˆ¶Paretoå‰æ²¿ï¼ˆ2ç›®æ ‡ï¼‰"""
        if pareto_front.shape[1] != 2:
            print("Paretoå‰æ²¿å¯è§†åŒ–ä»…æ”¯æŒ2ä¸ªç›®æ ‡")
            return
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # æ’åºä»¥ä¾¿è¿çº¿
        sorted_idx = np.argsort(pareto_front[:, 0])
        sorted_front = pareto_front[sorted_idx]
        
        ax.scatter(pareto_front[:, 0], pareto_front[:, 1],
                  s=100, c=self.colors[0], edgecolors='white',
                  linewidths=2, zorder=5, label='Paretoæœ€ä¼˜è§£')
        ax.plot(sorted_front[:, 0], sorted_front[:, 1],
               '--', color=self.colors[1], alpha=0.7, linewidth=2)
        
        if obj_names is None:
            obj_names = ['ç›®æ ‡1', 'ç›®æ ‡2']
        
        ax.set_xlabel(obj_names[0], fontweight='bold')
        ax.set_ylabel(obj_names[1], fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ============================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šä¸»ç¨‹åºä¸å®Œæ•´ç¤ºä¾‹ (Main Program)
# ============================================================

if __name__ == "__main__":
    
    print("\n" + "="*70)
    print("   CLASSIFICATION & ML MODELS FOR MCM/ICM")
    print("   åˆ†ç±»ä¸æœºå™¨å­¦ä¹ æ¨¡å‹")
    print("   Extended Version with Visualization")
    print("="*70)
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    ğŸ“Š åˆ†ç±»æ¨¡å‹åˆ†ææµç¨‹                            â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                  â•‘
    â•‘   [åˆ†ç±»ç®—æ³•]                                                      â•‘
    â•‘      â”œâ”€ éšæœºæ£®æ—: é›†æˆå­¦ä¹ ï¼Œå¯è§£é‡Šæ€§å¼º                            â•‘
    â•‘      â”œâ”€ æ¢¯åº¦æå‡: é«˜ç²¾åº¦ï¼Œé€‚åˆå¤æ‚é—®é¢˜                            â•‘
    â•‘      â”œâ”€ é€»è¾‘å›å½’: ç®€å•é«˜æ•ˆï¼Œå¯è§£é‡Šæ€§å¥½                            â•‘
    â•‘      â””â”€ æ”¯æŒå‘é‡æœº: é«˜ç»´æ•°æ®æ•ˆæœå¥½                               â•‘
    â•‘                                                                  â•‘
    â•‘   [è¯„ä¼°æŒ‡æ ‡]                                                      â•‘
    â•‘      â”œâ”€ Accuracy: æ•´ä½“å‡†ç¡®ç‡                                     â•‘
    â•‘      â”œâ”€ Precision: ç²¾ç¡®ç‡ï¼ˆæŸ¥å‡†ç‡ï¼‰                              â•‘
    â•‘      â”œâ”€ Recall: å¬å›ç‡ï¼ˆæŸ¥å…¨ç‡ï¼‰                                 â•‘
    â•‘      â”œâ”€ F1 Score: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡                        â•‘
    â•‘      â””â”€ AUC: ROCæ›²çº¿ä¸‹é¢ç§¯                                       â•‘
    â•‘                                                                  â•‘
    â•‘   [å¤šç›®æ ‡ä¼˜åŒ–]                                                    â•‘
    â•‘      â””â”€ NSGA-II: Paretoæœ€ä¼˜è§£é›†                                  â•‘
    â•‘                                                                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    generator = ClassificationDataGenerator(random_seed=2026)
    visualizer = ClassificationVisualizer()
    
    # ================================================================
    # ç¤ºä¾‹1ï¼šè®¾å¤‡æ•…éšœæ£€æµ‹
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 1: è®¾å¤‡æ•…éšœæ£€æµ‹ (Equipment Fault Detection)")
    print("="*70)
    
    data = generator.generate_binary_classification(n_samples=500, scenario='equipment')
    print(f"\næ•°æ®æ¦‚è§ˆ:")
    print(f"  æ ·æœ¬æ•°é‡: {len(data['data'])}")
    print(f"  ç‰¹å¾: {data['feature_names']}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {data['y'].value_counts().to_dict()}")
    
    # éšæœºæ£®æ—åˆ†ç±»
    rf_model = RandomForestModel(n_estimators=100, max_depth=10, verbose=True)
    rf_model.fit(data['X'], data['y'], test_size=0.2)
    
    # å¯è§†åŒ–
    visualizer.plot_confusion_matrix(
        rf_model.confusion_matrix, 
        class_names=data['target_names'],
        title="è®¾å¤‡æ•…éšœæ£€æµ‹æ··æ·†çŸ©é˜µ"
    )
    
    visualizer.plot_roc_curve(
        rf_model._y_test, rf_model._y_test_prob,
        title="è®¾å¤‡æ•…éšœæ£€æµ‹ROCæ›²çº¿"
    )
    
    visualizer.plot_feature_importance(
        rf_model.feature_importance,
        title="æ•…éšœæ£€æµ‹ç‰¹å¾é‡è¦æ€§"
    )
    
    # äº¤å‰éªŒè¯
    rf_model.cross_validate(data['X'], data['y'], cv=5)
    
    # ================================================================
    # ç¤ºä¾‹2ï¼šé›†æˆåˆ†ç±»å™¨å¯¹æ¯”
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 2: é›†æˆåˆ†ç±»å™¨å¯¹æ¯”")
    print("="*70)
    
    ensemble = EnsembleClassifier(verbose=True)
    ensemble.add_default_models()
    ensemble.fit(data['X'], data['y'], test_size=0.2)
    
    visualizer.plot_model_comparison(
        ensemble.individual_metrics,
        title="å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”"
    )
    
    # ================================================================
    # ç¤ºä¾‹3ï¼šå®¢æˆ·æµå¤±é¢„æµ‹
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 3: å®¢æˆ·æµå¤±é¢„æµ‹ (Customer Churn Prediction)")
    print("="*70)
    
    churn_data = generator.generate_binary_classification(n_samples=600, scenario='customer')
    
    rf_churn = RandomForestModel(n_estimators=150, class_weight='balanced', verbose=True)
    rf_churn.fit(churn_data['X'], churn_data['y'])
    
    visualizer.plot_feature_importance(
        rf_churn.feature_importance,
        title="å®¢æˆ·æµå¤±é¢„æµ‹ç‰¹å¾é‡è¦æ€§"
    )
    
    # ================================================================
    # ç¤ºä¾‹4ï¼šNSGA-IIå¤šç›®æ ‡ä¼˜åŒ–
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 4: NSGA-IIå¤šç›®æ ‡ä¼˜åŒ–")
    print("="*70)
    
    # å®šä¹‰ä¸¤ä¸ªç›®æ ‡å‡½æ•°
    def objective1(x):
        """æˆæœ¬æœ€å°åŒ–"""
        return x[0]**2 + x[1]**2
    
    def objective2(x):
        """æ•ˆç‡æœ€å¤§åŒ–ï¼ˆè½¬ä¸ºæœ€å°åŒ–ï¼‰"""
        return (x[0] - 2)**2 + (x[1] - 2)**2
    
    nsga = NSGAII(
        objectives=[objective1, objective2],
        bounds=[(0, 5), (0, 5)],
        n_dims=2,
        pop_size=50,
        max_iter=100,
        verbose=True
    )
    
    pareto_solutions, pareto_front = nsga.optimize()
    
    visualizer.plot_pareto_front(
        pareto_front,
        obj_names=['æˆæœ¬ (æœ€å°åŒ–)', 'æ•ˆç‡æŸå¤± (æœ€å°åŒ–)'],
        title="Paretoæœ€ä¼˜å‰æ²¿"
    )
    
    # ================================================================
    # ä½¿ç”¨è¯´æ˜
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“– ä½¿ç”¨è¯´æ˜ (Usage Guide)")
    print("="*70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     åˆ†ç±»æ¨¡å‹ä½¿ç”¨æŒ‡å—                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ã€åŸºæœ¬ä½¿ç”¨ã€‘
    
    1ï¸âƒ£ éšæœºæ£®æ—
       model = RandomForestModel(n_estimators=100)
       model.fit(X, y, test_size=0.2)
       predictions = model.predict(X_new)
    
    2ï¸âƒ£ é›†æˆåˆ†ç±»å™¨
       ensemble = EnsembleClassifier()
       ensemble.add_default_models()
       ensemble.fit(X, y)
    
    3ï¸âƒ£ å¤šç›®æ ‡ä¼˜åŒ–
       nsga = NSGAII(objectives=[obj1, obj2], bounds=[(0,5),(0,5)], n_dims=2)
       pareto_solutions, pareto_front = nsga.optimize()
    
    ã€ä¸å‡è¡¡æ•°æ®å¤„ç†ã€‘
    
    model = RandomForestModel(class_weight='balanced')
    
    ã€æ¨¡å‹é€‰æ‹©å»ºè®®ã€‘
    
    - å°æ ·æœ¬: é€»è¾‘å›å½’ã€SVM
    - ä¸­ç­‰æ ·æœ¬: éšæœºæ£®æ—ã€æ¢¯åº¦æå‡
    - å¤§æ ·æœ¬: æ·±åº¦å­¦ä¹ 
    - é«˜ç»´ç¨€ç–: SVMã€Lasso
    
    ã€è®ºæ–‡å›¾è¡¨å»ºè®®ã€‘
    
    Figure 1: æ··æ·†çŸ©é˜µ
    Figure 2: ROCæ›²çº¿
    Figure 3: ç‰¹å¾é‡è¦æ€§
    Figure 4: æ¨¡å‹å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    Figure 5: Paretoå‰æ²¿ï¼ˆå¤šç›®æ ‡ï¼‰
    
    Table 1: æ•°æ®é›†æè¿°
    Table 2: æ¨¡å‹å‚æ•°è®¾ç½®
    Table 3: è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”
    """)
    
    print("\n" + "="*70)
    print("   âœ… All examples completed successfully!")
    print("   ğŸ’¡ Use the above code templates for your MCM/ICM paper")
    print("="*70)
