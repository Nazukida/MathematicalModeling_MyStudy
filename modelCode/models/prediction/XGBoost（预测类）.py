"""
============================================================
XGBoost å›å½’é¢„æµ‹
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šé«˜ç²¾åº¦å›å½’é¢„æµ‹ã€ç‰¹å¾é‡è¦æ€§åˆ†æã€éçº¿æ€§å»ºæ¨¡
åŸç†ï¼šæ¢¯åº¦æå‡å†³ç­–æ ‘ (GBDT)
ä½œè€…ï¼šMCM/ICM Team
============================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# å›¾è¡¨ç¾åŒ–è®¾ç½®
plt.style.use('seaborn-v0_8-whitegrid')
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False


class XGBPredictor:
    """
    XGBoostå›å½’é¢„æµ‹å™¨å°è£…ç±»
    
    æ ¸å¿ƒåŸç†ï¼š
    - æ¢¯åº¦æå‡ï¼šæ¯æ£µæ ‘æ‹Ÿåˆå‰ä¸€è½®çš„æ®‹å·®
    - æ­£åˆ™åŒ–ï¼šL1/L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
    - å¹¶è¡Œè®¡ç®—ï¼šåˆ—é‡‡æ ·åŠ é€Ÿè®­ç»ƒ
    
    å…³é”®å‚æ•°ï¼š
    - learning_rate: å­¦ä¹ ç‡ï¼ˆ0.01-0.3ï¼‰
    - max_depth: æ ‘æ·±åº¦ï¼ˆ3-10ï¼‰
    - n_estimators: è¿­ä»£æ¬¡æ•°ï¼ˆ100-1000ï¼‰
    - subsample: æ ·æœ¬é‡‡æ ·æ¯”ä¾‹ï¼ˆ0.5-1.0ï¼‰
    - colsample_bytree: ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    """
    
    def __init__(self, n_estimators=100, max_depth=5, learning_rate=0.1,
                 subsample=0.8, colsample_bytree=0.8, random_state=42, verbose=True):
        """
        å‚æ•°é…ç½®
        
        :param n_estimators: è¿­ä»£æ¬¡æ•°
        :param max_depth: æœ€å¤§æ·±åº¦
        :param learning_rate: å­¦ä¹ ç‡
        :param subsample: æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
        :param colsample_bytree: ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
        """
        self.model = XGBRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            random_state=random_state,
            n_jobs=-1
        )
        self.verbose = verbose
        self.feature_names = None
        self.feature_importance = None
        self.r2 = None
        self.rmse = None
        self.mae = None
        self.y_test = None
        self.y_pred = None
    
    def fit(self, X, y, test_size=0.2, early_stopping=False):
        """
        è®­ç»ƒæ¨¡å‹
        
        :param X: ç‰¹å¾DataFrameæˆ–æ•°ç»„
        :param y: æ ‡ç­¾
        :param test_size: æµ‹è¯•é›†æ¯”ä¾‹
        :param early_stopping: æ˜¯å¦ä½¿ç”¨æ—©åœ
        """
        if isinstance(X, pd.DataFrame):
            self.feature_names = list(X.columns)
        else:
            self.feature_names = [f"ç‰¹å¾{i+1}" for i in range(X.shape[1])]
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        # è®­ç»ƒ
        if early_stopping:
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=False
            )
        else:
            self.model.fit(X_train, y_train)
        
        # é¢„æµ‹ä¸è¯„ä¼°
        self.y_test = y_test
        self.y_pred = self.model.predict(X_test)
        self.r2 = r2_score(y_test, self.y_pred)
        self.rmse = np.sqrt(mean_squared_error(y_test, self.y_pred))
        self.mae = mean_absolute_error(y_test, self.y_pred)
        
        # ç‰¹å¾é‡è¦æ€§
        self.feature_importance = pd.Series(
            self.model.feature_importances_,
            index=self.feature_names
        ).sort_values(ascending=False)
        
        if self.verbose:
            self._print_results()
        
        return self
    
    def _print_results(self):
        """æ‰“å°ç»“æœ"""
        print("\n" + "="*50)
        print("ğŸš€ XGBoost å›å½’ç»“æœ")
        print("="*50)
        print(f"\n  RÂ² å¾—åˆ†: {self.r2:.4f}")
        print(f"  RMSE: {self.rmse:.4f}")
        print(f"  MAE: {self.mae:.4f}")
        print(f"\n  ç‰¹å¾é‡è¦æ€§:")
        for name, imp in self.feature_importance.items():
            bar = "â–ˆ" * int(imp * 30)
            print(f"    {name}: {imp:.4f} {bar}")
        print("="*50)
    
    def cross_validate(self, X, y, cv=5):
        """äº¤å‰éªŒè¯"""
        scores = cross_val_score(self.model, X, y, cv=cv, scoring='r2')
        print(f"\näº¤å‰éªŒè¯ (cv={cv}):")
        print(f"  RÂ² å¾—åˆ†: {scores.mean():.4f} Â± {scores.std():.4f}")
        print(f"  å„æŠ˜å¾—åˆ†: {scores.round(4)}")
        return scores
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.model.predict(X)
    
    def plot_feature_importance(self, save_path=None):
        """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
        if self.feature_importance is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨fit()è®­ç»ƒæ¨¡å‹")
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        importance = self.feature_importance.sort_values(ascending=True)
        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance)))
        
        bars = ax.barh(importance.index, importance.values, color=colors,
                      edgecolor='white', linewidth=2)
        
        ax.set_xlabel('é‡è¦æ€§', fontsize=12, fontweight='bold')
        ax.set_title('XGBoost ç‰¹å¾é‡è¦æ€§', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        
        for bar, val in zip(bars, importance.values):
            ax.text(val + 0.01, bar.get_y() + bar.get_height()/2,
                   f'{val:.4f}', va='center', fontsize=10)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_prediction(self, save_path=None):
        """å¯è§†åŒ–é¢„æµ‹ vs çœŸå®"""
        if self.y_test is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨fit()è®­ç»ƒæ¨¡å‹")
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # å·¦å›¾ï¼šæ•£ç‚¹å›¾
        ax1 = axes[0]
        ax1.scatter(self.y_test, self.y_pred, alpha=0.6, 
                   color='#2E86AB', edgecolor='white', s=60)
        
        lims = [min(self.y_test.min(), self.y_pred.min()),
                max(self.y_test.max(), self.y_pred.max())]
        ax1.plot(lims, lims, 'r--', linewidth=2, label='ç†æƒ³é¢„æµ‹çº¿')
        
        ax1.set_xlabel('çœŸå®å€¼', fontsize=12, fontweight='bold')
        ax1.set_ylabel('é¢„æµ‹å€¼', fontsize=12, fontweight='bold')
        ax1.set_title(f'é¢„æµ‹ vs çœŸå® (RÂ²={self.r2:.4f})', fontsize=14, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šæ®‹å·®åˆ†å¸ƒ
        ax2 = axes[1]
        residuals = self.y_test - self.y_pred
        ax2.hist(residuals, bins=30, color='#E94F37', alpha=0.7,
                edgecolor='white', linewidth=1)
        ax2.axvline(x=0, color='black', linestyle='--', linewidth=2)
        ax2.set_xlabel('æ®‹å·®', fontsize=12, fontweight='bold')
        ax2.set_ylabel('é¢‘æ•°', fontsize=12, fontweight='bold')
        ax2.set_title('æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ============================================================
# ç¤ºä¾‹è¿è¡Œ
# ============================================================
if __name__ == "__main__":
    
    print("\n" + "="*60)
    print("   XGBoost å›å½’æ¼”ç¤º - é”€é‡é¢„æµ‹")
    print("="*60)
    
    # 1. æ¨¡æ‹Ÿæ•°æ®ï¼ˆé”€é‡ä¸ç‰¹å¾çš„å…³ç³»ï¼‰
    np.random.seed(42)
    n = 200
    ad_spend = np.random.uniform(10, 100, n)
    promotion = np.random.randint(1, 6, n)
    traffic = np.random.uniform(500, 2000, n)
    # éçº¿æ€§å…³ç³»
    sales = (5 + 0.3*ad_spend + 2*promotion + 0.01*traffic + 
             0.001*ad_spend*promotion + np.random.normal(0, 2, n))
    
    data = pd.DataFrame({
        "å¹¿å‘ŠæŠ•å…¥": ad_spend,
        "ä¿ƒé”€åŠ›åº¦": promotion,
        "å®¢æµé‡": traffic,
        "é”€é‡": sales
    })
    
    print("\næ•°æ®æ¦‚è§ˆï¼š")
    print(data.describe().round(2))
    
    # 2. è®­ç»ƒæ¨¡å‹
    X = data[["å¹¿å‘ŠæŠ•å…¥", "ä¿ƒé”€åŠ›åº¦", "å®¢æµé‡"]]
    y = data["é”€é‡"]
    
    xgb = XGBPredictor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        verbose=True
    )
    xgb.fit(X, y, test_size=0.2)
    
    # 3. äº¤å‰éªŒè¯
    xgb.cross_validate(X, y, cv=5)
    
    # 4. å¯è§†åŒ–
    xgb.plot_feature_importance()
    xgb.plot_prediction()
    
    # 5. æ–°æ ·æœ¬é¢„æµ‹
    new_data = pd.DataFrame({
        "å¹¿å‘ŠæŠ•å…¥": [50, 80],
        "ä¿ƒé”€åŠ›åº¦": [3, 5],
        "å®¢æµé‡": [1000, 1500]
    })
    predictions = xgb.predict(new_data)
    print(f"\næ–°æ ·æœ¬é¢„æµ‹é”€é‡: {predictions.round(2)}")
