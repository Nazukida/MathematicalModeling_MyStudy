"""
============================================================
æ¨¡å‹éªŒè¯å·¥ä½œæµ (Model Validation Pipeline)
============================================================

ã€æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ - ä¸€ä¸ªå‚æ•°åˆ‡æ¢æ¨¡å‹ã€‘

    from workflow.model_validation_pipeline import *
    
    pipeline = ModelValidationPipeline("æˆ‘çš„ä»»åŠ¡")
    pipeline.load_data(my_data, "æ•°æ®")
    pipeline.set_model(get_model("kmeans"))  # â† æ”¹è¿™ä¸ªå­—ç¬¦ä¸²åˆ‡æ¢æ¨¡å‹ï¼
    pipeline.configure_model(n_clusters=4)    # â† æ”¹è¿™é‡Œè°ƒå‚æ•°
    pipeline.run()
    result = pipeline.get_model_result()

ã€å¯ç”¨çš„æ¨¡å‹åç§°ã€‘ï¼ˆä¼ ç»™ get_model() çš„å­—ç¬¦ä¸²å‚æ•°ï¼‰

    èšç±»: "kmeans", "hierarchical"
    åˆ†ç±»: "decision_tree", "knn", "naive_bayes", "random_forest"â˜…, "svm", "xgboost_cls"
    å›å½’: "linear", "ridge", "lasso", "polynomial", "xgboost_reg"
    é¢„æµ‹: "grey", "arima", "exp_smoothing"
    è¯„ä»·: "topsis"â˜…, "entropy"â˜…, "ahp"
    ä¼˜åŒ–: "dp", "pso"â˜…, "ga"â˜…, "sa", "linear_prog", "integer_prog"
    é™ç»´: "pca"
    æ¨¡æ‹Ÿ: "monte_carlo"â˜…
    
    â˜… = è°ƒç”¨ models/ ç›®å½•ä¸‹çš„å®Œæ•´å®ç°ï¼Œå…¶ä»–ä¸ºé€‚é…å™¨å†…åµŒå®ç°

ã€å„æ¨¡å‹å¸¸ç”¨å‚æ•°ã€‘ï¼ˆconfigure_model å¯ä»¥è®¾ç½®çš„å‚æ•°ï¼‰

    kmeans:        n_clusters=3
    hierarchical:  n_clusters=3, linkage='ward'
    decision_tree: max_depth=None, test_size=0.2
    knn:           n_neighbors=5, test_size=0.2
    random_forest: n_estimators=100, max_depth=None
    svm:           C=1.0, test_size=0.2
    xgboost_*:     n_estimators=100, max_depth=6, learning_rate=0.1
    
    linear/ridge:  alpha=1.0 (ridge/lassoä¸“ç”¨)
    polynomial:    degree=2
    
    grey:          n_predict=5
    arima:         order=(1,1,1), n_predict=5
    exp_smoothing: alpha=0.3, n_predict=5
    
    topsis:        weights=[...], is_benefit=[True, False, ...]
    entropy:       is_benefit=[True, False, ...]
    ahp:           comparison_matrix=[[1,2,3],[1/2,1,2],[1/3,1/2,1]]
    
    dp:            capacity=10
    pso/ga/sa:     bounds=(-5,5), n_dims=2, max_iter=100 (éœ€è¦å…ˆ set_objective)
    linear_prog:   c=[...], A_ub=[[...]], b_ub=[...]
    integer_prog:  åŒä¸Šï¼ŒåŠ  integrality=[1,1,0,...]
    
    pca:           n_components=2
    monte_carlo:   n_simulations=10000, confidence=0.95 (éœ€è¦å…ˆ set_simulation)

ã€æ¨¡å‹åº“è°ƒç”¨è¯´æ˜ã€‘

    æœ¬ pipeline ä¸­çš„é€‚é…å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶è°ƒç”¨ models/ ç›®å½•ä¸‹çš„æ¨¡å‹ç±»ï¼š
    
    - TOPSIS/ç†µæƒæ³•  â†’ models.evaluation.evaluation_toolkit.TOPSIS/EntropyWeightMethod
    - PSO/é—ä¼ ç®—æ³•   â†’ models.optimization.optimization_toolkit.PSO/GeneticAlgorithm
    - éšæœºæ£®æ—åˆ†ç±»   â†’ models.classification.classification_toolkit.RandomForestModel
    - è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ   â†’ models.probability.monte_carlo_simulation.MonteCarloSimulator
    
    å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°é€‚é…å™¨å†…çš„ç®€åŒ–å®ç°ã€‚

============================================================
ã€å®Œæ•´ä½¿ç”¨æµç¨‹ã€‘

    # 1. å¯¼å…¥
    from workflow.model_validation_pipeline import *
    
    # 2. åˆ›å»ºå·¥ä½œæµ
    pipeline = ModelValidationPipeline("ä»»åŠ¡å")
    
    # 3. åŠ è½½æ•°æ® (DataFrame/array/list/dict)
    pipeline.load_data(my_data, "æ•°æ®æè¿°")
    
    # 4. é¢„å¤„ç†ï¼ˆå¯é€‰ï¼Œå¯å¤šä¸ªï¼‰
    pipeline.add_preprocessing(MissingValueStep('mean'))
    pipeline.add_preprocessing(OutlierRemovalStep('iqr'))
    pipeline.add_preprocessing(NormalizationStep('minmax'))
    
    # 5. è®¾ç½®æ¨¡å‹ â† æ ¸å¿ƒï¼šæ”¹ get_model("xxx") çš„å‚æ•°
    pipeline.set_model(get_model("topsis"))
    pipeline.configure_model(weights=[0.3, 0.3, 0.2, 0.2])
    
    # 6. å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
    pipeline.add_visualization(DPTableVisualization())
    
    # 7. è¿è¡Œ
    pipeline.run()
    
    # 8. è·å–ç»“æœ
    result = pipeline.get_model_result()
    data = pipeline.get_processed_data()

============================================================
ã€é¢„å¤„ç†é€‰é¡¹ã€‘

    MissingValueStep('mean')      - å‡å€¼å¡«å……
    MissingValueStep('median')    - ä¸­ä½æ•°å¡«å……
    MissingValueStep('knn')       - KNNæ’è¡¥
    MissingValueStep('drop')      - åˆ é™¤ç¼ºå¤±è¡Œ
    OutlierRemovalStep('iqr')     - IQRå¼‚å¸¸å€¼å¤„ç†
    OutlierRemovalStep('zscore')  - Z-scoreå¼‚å¸¸å€¼å¤„ç†
    NormalizationStep('zscore')   - Z-scoreæ ‡å‡†åŒ–
    NormalizationStep('minmax')   - Min-Maxå½’ä¸€åŒ–
    NormalizationStep('robust')   - ç¨³å¥æ ‡å‡†åŒ–

============================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import os
import sys

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# å¯¼å…¥å„æ¨¡å—
try:
    from data_preprocessing.preprocessing_toolkit import (
        MissingValueHandler, OutlierDetector, SampleDataGenerator
    )
except ImportError:
    MissingValueHandler = None
    OutlierDetector = None
    SampleDataGenerator = None

try:
    from visualization.plot_config import PlotStyleConfig, PlotTemplates, FigureSaver
except ImportError:
    PlotStyleConfig = None
    PlotTemplates = None
    FigureSaver = None

# ===== å¯¼å…¥æ¨¡å‹åº“ =====
# è¯„ä»·æ¨¡å‹
try:
    from models.evaluation.evaluation_toolkit import (
        EntropyWeightMethod as _EntropyWeightMethod,
        TOPSIS as _TOPSIS
    )
except ImportError:
    _EntropyWeightMethod = None
    _TOPSIS = None

# ä¼˜åŒ–æ¨¡å‹
try:
    from models.optimization.optimization_toolkit import (
        ParticleSwarmOptimization as _PSO,
        GeneticAlgorithm as _GA
    )
except ImportError:
    _PSO = None
    _GA = None

# åŠ¨åŠ›å­¦æ¨¡å‹
try:
    from models.dynamics.dynamics_toolkit import (
        SIRModel as _SIRModel,
        SEIRModel as _SEIRModel,
        LotkaVolterra as _LotkaVolterra,
        PopulationDynamics as _PopulationDynamics
    )
except ImportError:
    _SIRModel = None
    _SEIRModel = None
    _LotkaVolterra = None
    _PopulationDynamics = None

# åˆ†ç±»æ¨¡å‹
try:
    from models.classification.classification_toolkit import (
        RandomForestModel as _RandomForestModel,
        EnsembleClassifier as _EnsembleClassifier,
        BaseClassifier as _BaseClassifier
    )
except ImportError:
    _RandomForestModel = None
    _EnsembleClassifier = None
    _BaseClassifier = None

# é¢„æµ‹æ¨¡å‹
try:
    from models.prediction.prediction_toolkit import (
        TimeSeriesAnalyzer as _TimeSeriesAnalyzer
    )
except ImportError:
    _TimeSeriesAnalyzer = None

# æ¦‚ç‡/æ¨¡æ‹Ÿæ¨¡å‹
try:
    from models.probability.monte_carlo_simulation import (
        MonteCarloSimulator as _MonteCarloSimulator
    )
except ImportError:
    _MonteCarloSimulator = None


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šç»Ÿä¸€æ•°æ®æ ¼å¼ (Unified Data Format)
# ============================================================
"""
ã€PipelineData æ˜¯ä»€ä¹ˆï¼Ÿã€‘
- æ•°æ®çš„"åŒ…è£…ç›’"ï¼Œè®©æ•°æ®èƒ½åœ¨å„æ¨¡å—é—´ä¼ é€’
- ä½ ä¸éœ€è¦ç›´æ¥åˆ›å»ºå®ƒï¼Œpipeline.load_data() ä¼šè‡ªåŠ¨åˆ›å»º

ã€ä½ å¯èƒ½ç”¨åˆ°çš„æ–¹æ³•ã€‘
- pipeline_data.get_dataframe()  â†’ è·å– pandas DataFrame
- pipeline_data.get_array()      â†’ è·å– numpy array  
- pipeline_data.get_list()       â†’ è·å– Python list
- pipeline_data.summary()        â†’ æ‰“å°æ•°æ®æ‘˜è¦
"""

class PipelineData:
    """
    å·¥ä½œæµæ•°æ®å®¹å™¨ - ç»Ÿä¸€å„æ¨¡å—é—´çš„æ•°æ®ä¼ é€’æ ¼å¼
    
    ä½œç”¨ï¼šç¡®ä¿æ•°æ®åœ¨é¢„å¤„ç†ã€æ¨¡å‹ã€å¯è§†åŒ–ä¹‹é—´é¡ºåˆ©æµè½¬
    """
    
    def __init__(self, data=None, name="æœªå‘½åæ•°æ®"):
        """
        åˆå§‹åŒ–æ•°æ®å®¹å™¨
        
        :param data: åŸå§‹æ•°æ® (DataFrame, ndarray, dict, list)
        :param name: æ•°æ®åç§°
        """
        self.name = name
        self.raw_data = None           # åŸå§‹æ•°æ®
        self.processed_data = None     # é¢„å¤„ç†åçš„æ•°æ®
        self.model_input = None        # æ¨¡å‹è¾“å…¥æ ¼å¼
        self.model_output = None       # æ¨¡å‹è¾“å‡ºç»“æœ
        self.metadata = {}             # å…ƒæ•°æ®ï¼ˆåˆ—åã€ç±»å‹ç­‰ï¼‰
        self.history = []              # å¤„ç†å†å²è®°å½•
        
        if data is not None:
            self.load(data)
    
    def load(self, data):
        """åŠ è½½æ•°æ®"""
        if isinstance(data, pd.DataFrame):
            self.raw_data = data.copy()
            self.metadata['columns'] = list(data.columns)
            self.metadata['dtypes'] = data.dtypes.to_dict()
        elif isinstance(data, np.ndarray):
            self.raw_data = pd.DataFrame(data)
            self.metadata['columns'] = list(self.raw_data.columns)
        elif isinstance(data, dict):
            self.raw_data = pd.DataFrame(data)
            self.metadata['columns'] = list(self.raw_data.columns)
        elif isinstance(data, list):
            self.raw_data = pd.DataFrame(data)
            self.metadata['columns'] = list(self.raw_data.columns)
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")
        
        self.processed_data = self.raw_data.copy()
        self._log("æ•°æ®åŠ è½½å®Œæˆ", f"å½¢çŠ¶: {self.raw_data.shape}")
        return self
    
    def _log(self, operation, details=""):
        """è®°å½•æ“ä½œå†å²"""
        self.history.append({
            'time': datetime.now().strftime("%H:%M:%S"),
            'operation': operation,
            'details': details
        })
    
    def get_array(self):
        """è·å–numpyæ•°ç»„æ ¼å¼"""
        return self.processed_data.values if self.processed_data is not None else None
    
    def get_dataframe(self):
        """è·å–DataFrameæ ¼å¼"""
        return self.processed_data
    
    def get_list(self):
        """è·å–åˆ—è¡¨æ ¼å¼ï¼ˆé€‚åˆåŠ¨æ€è§„åˆ’ç­‰ï¼‰"""
        return self.processed_data.values.tolist() if self.processed_data is not None else None
    
    def get_dict(self):
        """è·å–å­—å…¸æ ¼å¼"""
        return self.processed_data.to_dict('list') if self.processed_data is not None else None
    
    def set_model_output(self, output, output_type="general"):
        """
        è®¾ç½®æ¨¡å‹è¾“å‡º
        
        :param output: æ¨¡å‹è¾“å‡ºç»“æœ
        :param output_type: è¾“å‡ºç±»å‹æ ‡ç­¾
        """
        self.model_output = {
            'result': output,
            'type': output_type,
            'timestamp': datetime.now()
        }
        self._log("æ¨¡å‹è¾“å‡ºå·²è®¾ç½®", f"ç±»å‹: {output_type}")
        return self
    
    def summary(self):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"ğŸ“¦ æ•°æ®å®¹å™¨: {self.name}")
        print("="*60)
        print(f"  åŸå§‹æ•°æ®å½¢çŠ¶: {self.raw_data.shape if self.raw_data is not None else 'None'}")
        print(f"  å¤„ç†åæ•°æ®å½¢çŠ¶: {self.processed_data.shape if self.processed_data is not None else 'None'}")
        print(f"  åˆ—å: {self.metadata.get('columns', [])}")
        print(f"  æ¨¡å‹è¾“å‡º: {'å·²è®¾ç½®' if self.model_output else 'æœªè®¾ç½®'}")
        print(f"\n  ğŸ“‹ å¤„ç†å†å²:")
        for h in self.history[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
            print(f"    [{h['time']}] {h['operation']}: {h['details']}")
        print("="*60)


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé¢„å¤„ç†æ­¥éª¤ (Preprocessing Steps)
# ============================================================
"""
ã€é¢„å¤„ç†æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿã€‘
- å¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ã€è½¬æ¢çš„æ“ä½œ
- å¯ä»¥æ·»åŠ 0ä¸ªã€1ä¸ªæˆ–å¤šä¸ªæ­¥éª¤
- æŒ‰æ·»åŠ é¡ºåºä¾æ¬¡æ‰§è¡Œ

ã€å¯ç”¨çš„é¢„å¤„ç†æ­¥éª¤ã€‘ï¼ˆåœ¨ç¬¬4æ­¥ add_preprocessing æ—¶é€‰æ‹©ï¼‰

1. ç¼ºå¤±å€¼å¤„ç† - MissingValueStep(method)
   method å¯é€‰å€¼ï¼š
   - 'mean'   : ç”¨è¯¥åˆ—å‡å€¼å¡«å……ï¼ˆæ¨èç”¨äºæ­£æ€åˆ†å¸ƒæ•°æ®ï¼‰
   - 'median' : ç”¨è¯¥åˆ—ä¸­ä½æ•°å¡«å……ï¼ˆæ¨èç”¨äºæœ‰åæ–œçš„æ•°æ®ï¼‰
   - 'mode'   : ç”¨è¯¥åˆ—ä¼—æ•°å¡«å……ï¼ˆæ¨èç”¨äºåˆ†ç±»æ•°æ®ï¼‰
   - 'knn'    : ç”¨KNNç®—æ³•æ’è¡¥ï¼ˆæ¨èç”¨äºæœ‰ç›¸å…³æ€§çš„å¤šåˆ—æ•°æ®ï¼‰
   - 'drop'   : ç›´æ¥åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ
   
   ç¤ºä¾‹ï¼špipeline.add_preprocessing(MissingValueStep('mean'))

2. å¼‚å¸¸å€¼å¤„ç† - OutlierRemovalStep(method, threshold)
   method å¯é€‰å€¼ï¼š
   - 'iqr'    : å››åˆ†ä½è·æ–¹æ³•ï¼Œthresholdå»ºè®®1.5ï¼ˆé»˜è®¤ï¼‰
   - 'zscore' : æ ‡å‡†å·®æ–¹æ³•ï¼Œthresholdå»ºè®®2æˆ–3
   
   ç¤ºä¾‹ï¼špipeline.add_preprocessing(OutlierRemovalStep('iqr', 1.5))

3. æ ‡å‡†åŒ– - NormalizationStep(method)
   method å¯é€‰å€¼ï¼š
   - 'zscore' : Z-scoreæ ‡å‡†åŒ–ï¼Œç»“æœå‡å€¼0æ ‡å‡†å·®1
   - 'minmax' : Min-Maxå½’ä¸€åŒ–ï¼Œç»“æœåœ¨[0,1]ä¹‹é—´
   - 'robust' : ç¨³å¥æ ‡å‡†åŒ–ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
   
   ç¤ºä¾‹ï¼špipeline.add_preprocessing(NormalizationStep('minmax'))

ã€ç»„åˆç¤ºä¾‹ã€‘
   # å…ˆå¡«å……ç¼ºå¤±å€¼ï¼Œå†å¤„ç†å¼‚å¸¸å€¼ï¼Œæœ€åæ ‡å‡†åŒ–
   pipeline.add_preprocessing(MissingValueStep('mean'))
   pipeline.add_preprocessing(OutlierRemovalStep('iqr', 1.5))
   pipeline.add_preprocessing(NormalizationStep('minmax'))
"""

class PreprocessingStep:
    """é¢„å¤„ç†æ­¥éª¤åŸºç±»"""
    
    def __init__(self, name="é¢„å¤„ç†æ­¥éª¤"):
        self.name = name
        self.params = {}
    
    def apply(self, pipeline_data: PipelineData) -> PipelineData:
        """åº”ç”¨é¢„å¤„ç†æ­¥éª¤"""
        raise NotImplementedError


class MissingValueStep(PreprocessingStep):
    """ç¼ºå¤±å€¼å¤„ç†æ­¥éª¤"""
    
    def __init__(self, method='mean', **kwargs):
        """
        :param method: 'mean', 'median', 'mode', 'knn', 'drop'
        """
        super().__init__(f"ç¼ºå¤±å€¼å¤„ç†({method})")
        self.method = method
        self.params = kwargs
    
    def apply(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe()
        
        if data.isnull().sum().sum() == 0:
            pipeline_data._log(self.name, "æ— ç¼ºå¤±å€¼ï¼Œè·³è¿‡å¤„ç†")
            return pipeline_data
        
        if MissingValueHandler is not None:
            handler = MissingValueHandler(verbose=False)
            if self.method == 'drop':
                filled = handler.drop_missing(data, **self.params)
            else:
                filled = handler.fill_missing(data, method=self.method, **self.params)
        else:
            # å¤‡ç”¨å®ç°
            if self.method == 'mean':
                filled = data.fillna(data.mean())
            elif self.method == 'median':
                filled = data.fillna(data.median())
            elif self.method == 'drop':
                filled = data.dropna()
            else:
                filled = data.fillna(0)
        
        pipeline_data.processed_data = filled
        pipeline_data._log(self.name, f"å¤„ç†äº† {data.isnull().sum().sum()} ä¸ªç¼ºå¤±å€¼")
        return pipeline_data


class OutlierRemovalStep(PreprocessingStep):
    """å¼‚å¸¸å€¼å¤„ç†æ­¥éª¤"""
    
    def __init__(self, method='iqr', threshold=1.5):
        """
        :param method: 'iqr' æˆ– 'zscore'
        :param threshold: IQRå€æ•° æˆ– Z-scoreé˜ˆå€¼
        """
        super().__init__(f"å¼‚å¸¸å€¼å¤„ç†({method})")
        self.method = method
        self.threshold = threshold
    
    def apply(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe().copy()
        outlier_count = 0
        
        for col in data.select_dtypes(include=[np.number]).columns:
            if self.method == 'iqr':
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower = Q1 - self.threshold * IQR
                upper = Q3 + self.threshold * IQR
                mask = (data[col] < lower) | (data[col] > upper)
            else:  # zscore
                z = np.abs((data[col] - data[col].mean()) / data[col].std())
                mask = z > self.threshold
            
            outlier_count += mask.sum()
            # ç”¨è¾¹ç•Œå€¼æ›¿æ¢
            if self.method == 'iqr':
                data.loc[data[col] < lower, col] = lower
                data.loc[data[col] > upper, col] = upper
        
        pipeline_data.processed_data = data
        pipeline_data._log(self.name, f"å¤„ç†äº† {outlier_count} ä¸ªå¼‚å¸¸å€¼")
        return pipeline_data


class NormalizationStep(PreprocessingStep):
    """æ•°æ®æ ‡å‡†åŒ–æ­¥éª¤"""
    
    def __init__(self, method='zscore'):
        """
        :param method: 'zscore', 'minmax', 'robust'
        """
        super().__init__(f"æ ‡å‡†åŒ–({method})")
        self.method = method
    
    def apply(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe().copy()
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if self.method == 'zscore':
                data[col] = (data[col] - data[col].mean()) / data[col].std()
            elif self.method == 'minmax':
                min_val = data[col].min()
                max_val = data[col].max()
                data[col] = (data[col] - min_val) / (max_val - min_val + 1e-10)
            elif self.method == 'robust':
                median = data[col].median()
                iqr = data[col].quantile(0.75) - data[col].quantile(0.25)
                data[col] = (data[col] - median) / (iqr + 1e-10)
        
        pipeline_data.processed_data = data
        pipeline_data._log(self.name, f"æ ‡å‡†åŒ–äº† {len(numeric_cols)} åˆ—")
        return pipeline_data


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹é€‚é…å™¨ (Model Adapters)
# ============================================================
"""
ã€æ¨¡å‹é€‚é…å™¨æ˜¯ä»€ä¹ˆï¼Ÿã€‘
- æŠŠä½ çš„æ¨¡å‹"åŒ…è£…"æˆå·¥ä½œæµèƒ½è¯†åˆ«çš„æ ¼å¼
- å¿…é¡»è®¾ç½®ä¸€ä¸ªæ¨¡å‹ï¼ˆç”¨ set_modelï¼‰
- å¯ä»¥è®¾ç½®å‚æ•°ï¼ˆç”¨ configure_modelï¼‰

============================================================
ã€å·²å®ç°çš„æ¨¡å‹é€‚é…å™¨ã€‘ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
============================================================

1. DynamicProgrammingAdapter() - åŠ¨æ€è§„åˆ’ï¼ˆèƒŒåŒ…é—®é¢˜ï¼‰
2. OptimizationAdapter('pso')  - ç²’å­ç¾¤ä¼˜åŒ–
3. LinearProgrammingAdapter()  - çº¿æ€§è§„åˆ’
4. GreyPredictionAdapter()     - ç°è‰²é¢„æµ‹
5. TOPSISAdapter()             - TOPSISè¯„ä»·

============================================================
ã€models/ ç›®å½•ä¸‹å¯æ¥å…¥çš„æ¨¡å‹ã€‘ï¼ˆéœ€è¦è‡ªå·±å†™é€‚é…å™¨ï¼‰
============================================================

optimization/ ä¼˜åŒ–ç±»ï¼š
    - linear_programming.py      çº¿æ€§è§„åˆ’
    - integer_programming.py     æ•´æ•°è§„åˆ’
    - zero_one_programming.py    0-1è§„åˆ’
    - nonlinear_programming.py   éçº¿æ€§è§„åˆ’
    - simulated_annealing.py     æ¨¡æ‹Ÿé€€ç«
    - nsga2_multi_objective.py   NSGA2å¤šç›®æ ‡ä¼˜åŒ–

prediction/ é¢„æµ‹ç±»ï¼š
    - grey_prediction.py         ç°è‰²é¢„æµ‹
    - arma_prediction.py         ARMAæ—¶é—´åºåˆ—
    - logistic_prediction.py     Logisticå¢é•¿é¢„æµ‹
    - markov_prediction.py       é©¬å°”å¯å¤«é¢„æµ‹
    - prophet_forecast.py        Propheté¢„æµ‹
    - xgboost_regression.py      XGBoostå›å½’

clustering/ èšç±»ç±»ï¼š
    - kmeans_clustering.py       K-meansèšç±»
    - hierarchical_clustering.py å±‚æ¬¡èšç±»
    - som_clustering.py          SOMè‡ªç»„ç»‡æ˜ å°„

classification/ åˆ†ç±»ç±»ï¼š
    - decision_tree_classification.py  å†³ç­–æ ‘
    - knn_classification.py            KNN
    - naive_bayes_classification.py    æœ´ç´ è´å¶æ–¯

evaluation/ è¯„ä»·ç±»ï¼š
    - evaluation_toolkit.py      ç†µæƒæ³• + TOPSIS

dynamics/ åŠ¨åŠ›å­¦ï¼š
    - glv_ecosystem_model.py     GLVç”Ÿæ€ç³»ç»Ÿæ¨¡å‹
    - war_model.py               æˆ˜äº‰æ¨¡å‹

============================================================
ã€å¦‚ä½•æ¥å…¥ä¸Šè¿°ä»»æ„æ¨¡å‹ï¼Ÿå¤åˆ¶è¿™ä¸ªæ¨¡æ¿ã€‘
============================================================

å‡è®¾ä½ è¦ç”¨ models/prediction/grey_prediction.py é‡Œçš„ç°è‰²é¢„æµ‹ï¼š

```python
# åœ¨ä½ çš„ main.py ä¸­ï¼š

from workflow.model_validation_pipeline import ModelAdapter, PipelineData
# å¯¼å…¥ä½ è¦ç”¨çš„æ¨¡å‹ï¼ˆæ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
# from models.prediction.grey_prediction import GreyPredictor

class GreyPredictionAdapter(ModelAdapter):
    '''ç°è‰²é¢„æµ‹æ¨¡å‹é€‚é…å™¨'''
    
    def __init__(self):
        super().__init__("ç°è‰²é¢„æµ‹GM(1,1)")
        self.params = {
            'n_predict': 5,  # é¢„æµ‹æœªæ¥5ä¸ªæ—¶é—´ç‚¹
        }
    
    def run(self, pipeline_data):
        # 1. è·å–æ•°æ®
        data = pipeline_data.get_array().flatten()  # ä¸€ç»´æ—¶é—´åºåˆ—
        
        # 2. è·å–å‚æ•°
        n_predict = self.params['n_predict']
        
        # 3. è°ƒç”¨ä½ çš„æ¨¡å‹
        # ============ æŠŠæ¨¡å‹ä»£ç æ”¾è¿™é‡Œ ============
        # predictor = GreyPredictor()
        # predictor.fit(data)
        # predictions = predictor.predict(n_predict)
        
        # ç¤ºä¾‹ï¼šç®€å•å®ç°
        predictions = [data[-1] * 1.1 ** i for i in range(1, n_predict+1)]
        # ==========================================
        
        # 4. ä¿å­˜ç»“æœ
        self.result = {
            'predictions': predictions,
            'original_data': data,
        }
        
        # 5. è®¾ç½®è¾“å‡º
        pipeline_data.set_model_output(self.result, "grey_prediction")
        return pipeline_data

# ä½¿ç”¨ï¼š
# pipeline.set_model(GreyPredictionAdapter())
# pipeline.configure_model(n_predict=10)
```

============================================================
ã€æ›´å¤šé€‚é…å™¨æ¨¡æ¿ç¤ºä¾‹ã€‘
============================================================

--- èšç±»æ¨¡å‹é€‚é…å™¨æ¨¡æ¿ ---
```python
class KMeansAdapter(ModelAdapter):
    def __init__(self):
        super().__init__("K-Meansèšç±»")
        self.params = {'n_clusters': 3}
    
    def run(self, pipeline_data):
        from sklearn.cluster import KMeans
        data = pipeline_data.get_array()
        
        kmeans = KMeans(n_clusters=self.params['n_clusters'])
        labels = kmeans.fit_predict(data)
        
        self.result = {
            'labels': labels,
            'centers': kmeans.cluster_centers_,
            'inertia': kmeans.inertia_,
        }
        pipeline_data.set_model_output(self.result, "clustering")
        return pipeline_data
```

--- å›å½’æ¨¡å‹é€‚é…å™¨æ¨¡æ¿ ---
```python
class RegressionAdapter(ModelAdapter):
    def __init__(self, method='linear'):
        super().__init__(f"{method}å›å½’")
        self.method = method
        self.params = {}
    
    def run(self, pipeline_data):
        from sklearn.linear_model import LinearRegression, Ridge
        data = pipeline_data.get_dataframe()
        
        X = data.iloc[:, :-1].values  # å‰n-1åˆ—ä¸ºç‰¹å¾
        y = data.iloc[:, -1].values   # æœ€åä¸€åˆ—ä¸ºç›®æ ‡
        
        if self.method == 'linear':
            model = LinearRegression()
        elif self.method == 'ridge':
            model = Ridge(alpha=self.params.get('alpha', 1.0))
        
        model.fit(X, y)
        
        self.result = {
            'coefficients': model.coef_,
            'intercept': model.intercept_,
            'r2_score': model.score(X, y),
        }
        pipeline_data.set_model_output(self.result, "regression")
        return pipeline_data
```

--- TOPSISè¯„ä»·é€‚é…å™¨æ¨¡æ¿ ---
```python
class TOPSISAdapter(ModelAdapter):
    def __init__(self):
        super().__init__("TOPSISè¯„ä»·")
        self.params = {
            'weights': None,      # æƒé‡ï¼ŒNoneåˆ™ç­‰æƒ
            'is_benefit': None,   # å„æŒ‡æ ‡æ˜¯å¦ä¸ºæ•ˆç›Šå‹
        }
    
    def run(self, pipeline_data):
        data = pipeline_data.get_array()
        n_samples, n_features = data.shape
        
        # æƒé‡
        weights = self.params['weights']
        if weights is None:
            weights = np.ones(n_features) / n_features
        
        # å½’ä¸€åŒ–
        norm_data = data / np.sqrt(np.sum(data**2, axis=0))
        weighted = norm_data * weights
        
        # ç†æƒ³è§£
        is_benefit = self.params['is_benefit']
        if is_benefit is None:
            is_benefit = [True] * n_features
        
        ideal_best = np.array([weighted[:, j].max() if is_benefit[j] else weighted[:, j].min() 
                               for j in range(n_features)])
        ideal_worst = np.array([weighted[:, j].min() if is_benefit[j] else weighted[:, j].max() 
                                for j in range(n_features)])
        
        # è·ç¦»å’Œå¾—åˆ†
        d_best = np.sqrt(np.sum((weighted - ideal_best)**2, axis=1))
        d_worst = np.sqrt(np.sum((weighted - ideal_worst)**2, axis=1))
        scores = d_worst / (d_best + d_worst + 1e-10)
        
        self.result = {
            'scores': scores,
            'ranking': np.argsort(-scores) + 1,  # æ’å
            'weights': weights,
        }
        pipeline_data.set_model_output(self.result, "evaluation")
        return pipeline_data
```

--- åˆ†ç±»æ¨¡å‹é€‚é…å™¨æ¨¡æ¿ ---
```python
class ClassificationAdapter(ModelAdapter):
    def __init__(self, method='decision_tree'):
        super().__init__(f"{method}åˆ†ç±»")
        self.method = method
        self.params = {'test_size': 0.2}
    
    def run(self, pipeline_data):
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.metrics import accuracy_score
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'])
        
        if self.method == 'decision_tree':
            model = DecisionTreeClassifier()
        elif self.method == 'knn':
            model = KNeighborsClassifier(n_neighbors=self.params.get('k', 5))
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        self.result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'predictions': y_pred,
            'model': model,
        }
        pipeline_data.set_model_output(self.result, "classification")
        return pipeline_data
```

============================================================
ã€æ€»ç»“ï¼šæ¥å…¥ä»»æ„æ¨¡å‹çš„æ­¥éª¤ã€‘
============================================================

1. åˆ›å»ºç±»ï¼Œç»§æ‰¿ ModelAdapter
2. __init__ ä¸­å®šä¹‰ self.params = {...} å‚æ•°
3. run() ä¸­ï¼š
   - ç”¨ pipeline_data.get_array() æˆ– get_dataframe() è·å–æ•°æ®
   - ç”¨ self.params['xxx'] è·å–å‚æ•°
   - è¿è¡Œä½ çš„æ¨¡å‹é€»è¾‘
   - æŠŠç»“æœå­˜å…¥ self.result = {...}
   - è°ƒç”¨ pipeline_data.set_model_output(self.result, "ç±»å‹å")
   - return pipeline_data

å°±è¿™ä¹ˆç®€å•ï¼
"""

class ModelAdapter:
    """æ¨¡å‹é€‚é…å™¨åŸºç±» - å°†ä¸åŒæ¨¡å‹ç»Ÿä¸€ä¸ºç›¸åŒæ¥å£"""
    
    def __init__(self, name="æ¨¡å‹"):
        self.name = name
        self.params = {}
        self.result = None
    
    def set_params(self, **kwargs):
        """è®¾ç½®æ¨¡å‹å‚æ•°"""
        self.params.update(kwargs)
        return self
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        """è¿è¡Œæ¨¡å‹"""
        raise NotImplementedError
    
    def get_result(self):
        """è·å–ç»“æœ"""
        return self.result


class DynamicProgrammingAdapter(ModelAdapter):
    """åŠ¨æ€è§„åˆ’æ¨¡å‹é€‚é…å™¨ - èƒŒåŒ…é—®é¢˜ç¤ºä¾‹"""
    
    def __init__(self):
        super().__init__("åŠ¨æ€è§„åˆ’-èƒŒåŒ…é—®é¢˜")
        self.params = {
            'capacity': 10,  # èƒŒåŒ…å®¹é‡
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        """
        è¿è¡ŒåŠ¨æ€è§„åˆ’
        
        æœŸæœ›è¾“å…¥æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªç‰©å“ï¼Œåˆ—ä¸º [é‡é‡, ä»·å€¼] æˆ– DataFrame
        """
        data = pipeline_data.get_dataframe()
        
        # æå–é‡é‡å’Œä»·å€¼
        if data.shape[1] >= 2:
            weights = data.iloc[:, 0].values.astype(int)
            values = data.iloc[:, 1].values.astype(int)
        else:
            raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦è‡³å°‘ä¸¤åˆ—ï¼ˆé‡é‡å’Œä»·å€¼ï¼‰")
        
        capacity = self.params.get('capacity', 10)
        n = len(weights)
        
        # DPæ±‚è§£
        dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]
        
        for i in range(1, n + 1):
            for j in range(capacity + 1):
                if weights[i-1] <= j:
                    dp[i][j] = max(
                        values[i-1] + dp[i-1][j-weights[i-1]],
                        dp[i-1][j]
                    )
                else:
                    dp[i][j] = dp[i-1][j]
        
        # å›æº¯
        selected = []
        j = capacity
        for i in range(n, 0, -1):
            if dp[i][j] != dp[i-1][j]:
                selected.append(i-1)
                j -= weights[i-1]
        
        self.result = {
            'max_value': dp[n][capacity],
            'selected_items': selected,
            'total_weight': sum(weights[i] for i in selected),
            'dp_table': np.array(dp),
            'weights': weights,
            'values': values,
            'capacity': capacity
        }
        
        pipeline_data.set_model_output(self.result, "dynamic_programming")
        pipeline_data._log(self.name, f"æœ€å¤§ä»·å€¼: {self.result['max_value']}")
        return pipeline_data


class OptimizationAdapter(ModelAdapter):
    """
    ä¼˜åŒ–ç®—æ³•é€‚é…å™¨ - è°ƒç”¨ models.optimization.optimization_toolkit
    
    æ”¯æŒ PSO (ç²’å­ç¾¤ä¼˜åŒ–)
    
    ç”¨æ³•ï¼š
        model = get_model("pso")
        model.set_objective(lambda x: x[0]**2 + x[1]**2)  # è®¾ç½®ç›®æ ‡å‡½æ•°
        model.set_params(bounds=(-5, 5), n_dims=2, max_iter=100)
    """
    
    def __init__(self, algorithm='pso'):
        super().__init__(f"ä¼˜åŒ–ç®—æ³•-{algorithm.upper()}")
        self.algorithm = algorithm
        self.params = {
            'n_particles': 30,  # PSO: ç²’å­æ•°
            'pop_size': 30,     # GA: ç§ç¾¤å¤§å°
            'max_iter': 100,
            'bounds': (-5, 5),  # æœç´¢èŒƒå›´
            'n_dims': 2,        # å˜é‡ç»´åº¦
        }
        self.objective_func = None
        self._optimizer = None
    
    def set_objective(self, func):
        """è®¾ç½®ç›®æ ‡å‡½æ•° f(x) -> float"""
        self.objective_func = func
        return self
    
    def run(self, pipeline_data: PipelineData = None) -> PipelineData:
        """è¿è¡Œä¼˜åŒ–"""
        if self.objective_func is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_objective(func) è®¾ç½®ç›®æ ‡å‡½æ•°")
        
        bounds = self.params['bounds']
        n_dims = self.params['n_dims']
        max_iter = self.params['max_iter']
        
        # è½¬æ¢ bounds æ ¼å¼: (min, max) -> [(min, max), (min, max), ...]
        if isinstance(bounds, tuple) and len(bounds) == 2:
            bounds_list = [bounds] * n_dims
        else:
            bounds_list = list(bounds)
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ä¼˜åŒ–å™¨
        if self.algorithm == 'pso' and _PSO is not None:
            optimizer = _PSO(
                objective_func=self.objective_func,
                bounds=bounds_list,
                n_dims=n_dims,
                pop_size=self.params.get('n_particles', 30),
                max_iter=max_iter,
                verbose=False
            )
            optimizer.optimize()
            self._optimizer = optimizer
            
            self.result = {
                'best_position': optimizer.best_position,
                'best_value': optimizer.best_value,
                'convergence_history': optimizer.history,
            }
        elif self.algorithm == 'ga' and _GA is not None:
            optimizer = _GA(
                objective_func=self.objective_func,
                bounds=bounds_list,
                n_dims=n_dims,
                pop_size=self.params.get('pop_size', 50),
                max_iter=max_iter,
                verbose=False
            )
            optimizer.optimize()
            self._optimizer = optimizer
            
            self.result = {
                'best_position': optimizer.best_position,
                'best_value': optimizer.best_value,
                'convergence_history': optimizer.history,
            }
        else:
            # å›é€€åˆ°å†…ç½® PSO å®ç°
            n_particles = self.params['n_particles']
            
            lb = np.array([b[0] for b in bounds_list])
            ub = np.array([b[1] for b in bounds_list])
            
            positions = np.random.uniform(lb, ub, (n_particles, n_dims))
            velocities = np.random.uniform(-1, 1, (n_particles, n_dims))
            pbest_pos = positions.copy()
            pbest_val = np.array([self.objective_func(p) for p in positions])
            gbest_idx = np.argmin(pbest_val)
            gbest_pos = pbest_pos[gbest_idx].copy()
            gbest_val = pbest_val[gbest_idx]
            
            history = [gbest_val]
            w, c1, c2 = 0.7, 1.5, 1.5
            
            for _ in range(max_iter):
                r1, r2 = np.random.rand(n_particles, n_dims), np.random.rand(n_particles, n_dims)
                velocities = w * velocities + c1*r1*(pbest_pos - positions) + c2*r2*(gbest_pos - positions)
                positions = positions + velocities
                positions = np.clip(positions, lb, ub)
                
                fitness = np.array([self.objective_func(p) for p in positions])
                improved = fitness < pbest_val
                pbest_pos[improved] = positions[improved]
                pbest_val[improved] = fitness[improved]
                
                if np.min(pbest_val) < gbest_val:
                    gbest_idx = np.argmin(pbest_val)
                    gbest_pos = pbest_pos[gbest_idx].copy()
                    gbest_val = pbest_val[gbest_idx]
                
                history.append(gbest_val)
            
            self.result = {
                'best_position': gbest_pos,
                'best_value': gbest_val,
                'convergence_history': history
            }
        
        if pipeline_data:
            pipeline_data.set_model_output(self.result, "optimization")
            pipeline_data._log(self.name, f"æœ€ä¼˜å€¼: {self.result['best_value']:.6f}")
        
        return pipeline_data


# ============================================================
# æ›´å¤šå†…ç½®æ¨¡å‹é€‚é…å™¨
# ============================================================

class LinearProgrammingAdapter(ModelAdapter):
    """
    çº¿æ€§è§„åˆ’é€‚é…å™¨
    
    ç”¨æ³•ï¼š
        pipeline.set_model(LinearProgrammingAdapter())
        pipeline.configure_model(
            c=[-2, -3],           # ç›®æ ‡å‡½æ•°ç³»æ•°ï¼ˆæœ€å°åŒ– c^T xï¼‰
            A_ub=[[1, 1], [2, 1]],# ä¸ç­‰å¼çº¦æŸçŸ©é˜µ
            b_ub=[4, 5],          # ä¸ç­‰å¼çº¦æŸå³ç«¯
            bounds=[(0, None), (0, None)]  # å˜é‡èŒƒå›´
        )
    """
    
    def __init__(self):
        super().__init__("çº¿æ€§è§„åˆ’")
        self.params = {
            'c': None,        # ç›®æ ‡å‡½æ•°ç³»æ•°
            'A_ub': None,     # ä¸ç­‰å¼çº¦æŸ Ax <= b
            'b_ub': None,
            'A_eq': None,     # ç­‰å¼çº¦æŸ Ax = b
            'b_eq': None,
            'bounds': None,   # å˜é‡èŒƒå›´
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from scipy.optimize import linprog
        
        result = linprog(
            c=self.params['c'],
            A_ub=self.params.get('A_ub'),
            b_ub=self.params.get('b_ub'),
            A_eq=self.params.get('A_eq'),
            b_eq=self.params.get('b_eq'),
            bounds=self.params.get('bounds'),
            method='highs'
        )
        
        self.result = {
            'optimal_value': -result.fun if result.success else None,  # è½¬ä¸ºæœ€å¤§åŒ–
            'optimal_solution': result.x,
            'success': result.success,
            'message': result.message,
        }
        
        pipeline_data.set_model_output(self.result, "linear_programming")
        pipeline_data._log(self.name, f"æœ€ä¼˜å€¼: {self.result['optimal_value']}")
        return pipeline_data


class GreyPredictionAdapter(ModelAdapter):
    """
    ç°è‰²é¢„æµ‹GM(1,1)é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šä¸€åˆ—æ—¶é—´åºåˆ—æ•°æ®
    
    ç”¨æ³•ï¼š
        pipeline.set_model(GreyPredictionAdapter())
        pipeline.configure_model(n_predict=5)  # é¢„æµ‹æœªæ¥5ä¸ªç‚¹
    """
    
    def __init__(self):
        super().__init__("ç°è‰²é¢„æµ‹GM(1,1)")
        self.params = {
            'n_predict': 5,  # é¢„æµ‹æ­¥æ•°
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_array().flatten()
        n = len(data)
        n_predict = self.params['n_predict']
        
        # ç´¯åŠ ç”Ÿæˆ
        x1 = np.cumsum(data)
        
        # æ„å»ºçŸ©é˜µ
        B = np.zeros((n-1, 2))
        Y = np.zeros((n-1, 1))
        for i in range(n-1):
            B[i, 0] = -0.5 * (x1[i] + x1[i+1])
            B[i, 1] = 1
            Y[i, 0] = data[i+1]
        
        # æœ€å°äºŒä¹˜æ±‚å‚æ•°
        params = np.linalg.lstsq(B, Y, rcond=None)[0]
        a, b = params[0, 0], params[1, 0]
        
        # é¢„æµ‹
        predictions = []
        for k in range(1, n + n_predict + 1):
            x1_pred = (data[0] - b/a) * np.exp(-a * (k-1)) + b/a
            predictions.append(x1_pred)
        
        # ç´¯å‡è¿˜åŸ
        predictions = np.diff(np.array([0] + predictions))
        
        self.result = {
            'fitted': predictions[:n],
            'predictions': predictions[n:],
            'a': a,
            'b': b,
            'original': data,
        }
        
        pipeline_data.set_model_output(self.result, "grey_prediction")
        pipeline_data._log(self.name, f"é¢„æµ‹äº† {n_predict} ä¸ªç‚¹")
        return pipeline_data


class KMeansAdapter(ModelAdapter):
    """
    K-Meansèšç±»é€‚é…å™¨
    
    ç”¨æ³•ï¼š
        pipeline.set_model(KMeansAdapter())
        pipeline.configure_model(n_clusters=3)
    """
    
    def __init__(self):
        super().__init__("K-Meansèšç±»")
        self.params = {
            'n_clusters': 3,
            'random_state': 42,
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.cluster import KMeans
        
        data = pipeline_data.get_array()
        
        kmeans = KMeans(
            n_clusters=self.params['n_clusters'],
            random_state=self.params.get('random_state', 42),
            n_init=10
        )
        labels = kmeans.fit_predict(data)
        
        self.result = {
            'labels': labels,
            'centers': kmeans.cluster_centers_,
            'inertia': kmeans.inertia_,
            'n_clusters': self.params['n_clusters'],
        }
        
        pipeline_data.set_model_output(self.result, "clustering")
        pipeline_data._log(self.name, f"èšæˆ {self.params['n_clusters']} ç±»")
        return pipeline_data


class TOPSISAdapter(ModelAdapter):
    """
    TOPSISç»¼åˆè¯„ä»·é€‚é…å™¨ - è°ƒç”¨ models.evaluation.evaluation_toolkit.TOPSIS
    
    æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªè¯„ä»·å¯¹è±¡ï¼Œæ¯åˆ—ä¸€ä¸ªæŒ‡æ ‡
    
    ç”¨æ³•ï¼š
        pipeline.set_model(TOPSISAdapter())
        pipeline.configure_model(
            weights=[0.3, 0.3, 0.2, 0.2],  # æƒé‡ï¼ŒNoneåˆ™ç­‰æƒ
            is_benefit=[True, True, False, False]  # æ˜¯å¦æ•ˆç›Šå‹æŒ‡æ ‡
        )
    """
    
    def __init__(self):
        super().__init__("TOPSISè¯„ä»·")
        self.params = {
            'weights': None,
            'is_benefit': None,  # True=æ•ˆç›Šå‹(è¶Šå¤§è¶Šå¥½), False=æˆæœ¬å‹(è¶Šå°è¶Šå¥½)
        }
        self._topsis_model = None  # ä¿å­˜åŸå§‹æ¨¡å‹å®ä¾‹
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe()
        n_samples, n_features = data.shape
        
        weights = self.params.get('weights')
        is_benefit = self.params.get('is_benefit')
        
        # è½¬æ¢ is_benefit ä¸º indicator_types æ ¼å¼
        indicator_types = None
        if is_benefit is not None:
            indicator_types = ['positive' if b else 'negative' for b in is_benefit]
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ TOPSIS ç±»
        if _TOPSIS is not None:
            topsis = _TOPSIS(verbose=False)
            topsis.fit(data, weights=weights, indicator_types=indicator_types)
            self._topsis_model = topsis
            
            self.result = {
                'scores': topsis.closeness,
                'ranking': np.argsort(-topsis.closeness) + 1,
                'weights': topsis.weights,
                'd_best': topsis.distances_positive,
                'd_worst': topsis.distances_negative,
                'results_df': topsis.get_results(),
            }
        else:
            # å›é€€åˆ°å†…ç½®å®ç°
            data_arr = data.values if isinstance(data, pd.DataFrame) else data
            if weights is None:
                weights = np.ones(n_features) / n_features
            weights = np.array(weights)
            
            norm_data = data_arr / np.sqrt(np.sum(data_arr**2, axis=0) + 1e-10)
            weighted = norm_data * weights
            
            if is_benefit is None:
                is_benefit = [True] * n_features
            
            ideal_best = np.array([
                weighted[:, j].max() if is_benefit[j] else weighted[:, j].min()
                for j in range(n_features)
            ])
            ideal_worst = np.array([
                weighted[:, j].min() if is_benefit[j] else weighted[:, j].max()
                for j in range(n_features)
            ])
            
            d_best = np.sqrt(np.sum((weighted - ideal_best)**2, axis=1))
            d_worst = np.sqrt(np.sum((weighted - ideal_worst)**2, axis=1))
            scores = d_worst / (d_best + d_worst + 1e-10)
            
            self.result = {
                'scores': scores,
                'ranking': np.argsort(-scores) + 1,
                'weights': weights,
                'd_best': d_best,
                'd_worst': d_worst,
            }
        
        pipeline_data.set_model_output(self.result, "evaluation")
        pipeline_data._log(self.name, f"è¯„ä»·äº† {n_samples} ä¸ªå¯¹è±¡")
        return pipeline_data


class RegressionAdapter(ModelAdapter):
    """
    å›å½’æ¨¡å‹é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºç›®æ ‡y
    
    ç”¨æ³•ï¼š
        pipeline.set_model(RegressionAdapter('linear'))  # æˆ– 'ridge', 'lasso'
        pipeline.configure_model(alpha=1.0)  # Ridge/Lassoçš„æ­£åˆ™åŒ–å‚æ•°
    """
    
    def __init__(self, method='linear'):
        """
        :param method: 'linear', 'ridge', 'lasso', 'polynomial'
        """
        super().__init__(f"{method.capitalize()}å›å½’")
        self.method = method
        self.params = {
            'alpha': 1.0,
            'degree': 2,  # å¤šé¡¹å¼å›å½’çš„é˜¶æ•°
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.linear_model import LinearRegression, Ridge, Lasso
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.metrics import r2_score, mean_squared_error
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        if self.method == 'polynomial':
            poly = PolynomialFeatures(degree=self.params['degree'])
            X = poly.fit_transform(X)
            model = LinearRegression()
        elif self.method == 'ridge':
            model = Ridge(alpha=self.params['alpha'])
        elif self.method == 'lasso':
            model = Lasso(alpha=self.params['alpha'])
        else:
            model = LinearRegression()
        
        model.fit(X, y)
        y_pred = model.predict(X)
        
        self.result = {
            'coefficients': model.coef_,
            'intercept': model.intercept_,
            'r2_score': r2_score(y, y_pred),
            'rmse': np.sqrt(mean_squared_error(y, y_pred)),
            'predictions': y_pred,
        }
        
        pipeline_data.set_model_output(self.result, "regression")
        pipeline_data._log(self.name, f"RÂ² = {self.result['r2_score']:.4f}")
        return pipeline_data


# ============================================================
# æ›´å¤šæ¨¡å‹é€‚é…å™¨ (More Model Adapters)
# ============================================================

class HierarchicalClusteringAdapter(ModelAdapter):
    """
    å±‚æ¬¡èšç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—ä¸€ä¸ªç‰¹å¾
    
    å‚æ•°ï¼š
        n_clusters: èšç±»æ•°é‡ï¼Œé»˜è®¤3
        linkage: è¿æ¥æ–¹å¼ 'ward'/'complete'/'average'/'single'ï¼Œé»˜è®¤'ward'
    """
    def __init__(self):
        super().__init__("å±‚æ¬¡èšç±»")
        self.params = {'n_clusters': 3, 'linkage': 'ward'}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.cluster import AgglomerativeClustering
        from scipy.cluster.hierarchy import dendrogram, linkage
        
        data = pipeline_data.get_array()
        
        model = AgglomerativeClustering(
            n_clusters=self.params['n_clusters'],
            linkage=self.params['linkage']
        )
        labels = model.fit_predict(data)
        linkage_matrix = linkage(data, method=self.params['linkage'])
        
        self.result = {
            'labels': labels,
            'linkage_matrix': linkage_matrix,
            'n_clusters': self.params['n_clusters'],
        }
        pipeline_data.set_model_output(self.result, "clustering")
        pipeline_data._log(self.name, f"å±‚æ¬¡èšç±»å®Œæˆï¼Œ{self.params['n_clusters']}ç±»")
        return pipeline_data


class DecisionTreeAdapter(ModelAdapter):
    """
    å†³ç­–æ ‘åˆ†ç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾y
    
    å‚æ•°ï¼š
        max_depth: æœ€å¤§æ·±åº¦ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼Œé»˜è®¤0.2
    """
    def __init__(self):
        super().__init__("å†³ç­–æ ‘åˆ†ç±»")
        self.params = {'max_depth': None, 'test_size': 0.2, 'random_state': 42}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score, classification_report
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'], 
            random_state=self.params['random_state']
        )
        
        model = DecisionTreeClassifier(
            max_depth=self.params['max_depth'],
            random_state=self.params['random_state']
        )
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        self.result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'predictions': y_pred,
            'feature_importance': model.feature_importances_,
            'model': model,
        }
        pipeline_data.set_model_output(self.result, "classification")
        pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        return pipeline_data


class KNNAdapter(ModelAdapter):
    """
    KNNåˆ†ç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾y
    
    å‚æ•°ï¼š
        n_neighbors: é‚»å±…æ•°é‡ï¼Œé»˜è®¤5
        test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼Œé»˜è®¤0.2
    """
    def __init__(self):
        super().__init__("KNNåˆ†ç±»")
        self.params = {'n_neighbors': 5, 'test_size': 0.2, 'random_state': 42}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'],
            random_state=self.params['random_state']
        )
        
        model = KNeighborsClassifier(n_neighbors=self.params['n_neighbors'])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        self.result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'predictions': y_pred,
            'model': model,
        }
        pipeline_data.set_model_output(self.result, "classification")
        pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        return pipeline_data


class NaiveBayesAdapter(ModelAdapter):
    """
    æœ´ç´ è´å¶æ–¯åˆ†ç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾y
    
    å‚æ•°ï¼š
        method: 'gaussian'/'multinomial'/'bernoulli'ï¼Œé»˜è®¤'gaussian'
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
    """
    def __init__(self, method='gaussian'):
        super().__init__(f"æœ´ç´ è´å¶æ–¯({method})")
        self.method = method
        self.params = {'test_size': 0.2, 'random_state': 42}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'],
            random_state=self.params['random_state']
        )
        
        if self.method == 'multinomial':
            model = MultinomialNB()
        elif self.method == 'bernoulli':
            model = BernoulliNB()
        else:
            model = GaussianNB()
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        self.result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'predictions': y_pred,
            'model': model,
        }
        pipeline_data.set_model_output(self.result, "classification")
        pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        return pipeline_data


class RandomForestAdapter(ModelAdapter):
    """
    éšæœºæ£®æ—åˆ†ç±»é€‚é…å™¨ - è°ƒç”¨ models.classification.classification_toolkit.RandomForestModel
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾y
    
    å‚æ•°ï¼š
        n_estimators: æ ‘çš„æ•°é‡ï¼Œé»˜è®¤100
        max_depth: æœ€å¤§æ·±åº¦
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
    """
    def __init__(self):
        super().__init__("éšæœºæ£®æ—åˆ†ç±»")
        self.params = {'n_estimators': 100, 'max_depth': None, 'test_size': 0.2, 'random_state': 42}
        self._rf_model = None
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1]
        y = data.iloc[:, -1]
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ RandomForestModel
        if _RandomForestModel is not None:
            rf = _RandomForestModel(
                n_estimators=self.params['n_estimators'],
                max_depth=self.params['max_depth'],
                verbose=False
            )
            rf.fit(X, y, test_size=self.params['test_size'])
            self._rf_model = rf
            
            self.result = {
                'accuracy': rf.metrics['test']['accuracy'],
                'metrics': rf.metrics,
                'feature_importance': rf.feature_importance,
                'confusion_matrix': rf.confusion_matrix,
                'model': rf.model,
            }
        else:
            # å›é€€åˆ° sklearn ç›´æ¥è°ƒç”¨
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score
            
            X_arr = X.values
            y_arr = y.values
            
            X_train, X_test, y_train, y_test = train_test_split(
                X_arr, y_arr, test_size=self.params['test_size'],
                random_state=self.params['random_state']
            )
            
            model = RandomForestClassifier(
                n_estimators=self.params['n_estimators'],
                max_depth=self.params['max_depth'],
                random_state=self.params['random_state']
            )
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            self.result = {
                'accuracy': accuracy_score(y_test, y_pred),
                'predictions': y_pred,
                'feature_importance': model.feature_importances_,
                'model': model,
            }
        
        pipeline_data.set_model_output(self.result, "classification")
        pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        return pipeline_data


class SVMAdapter(ModelAdapter):
    """
    æ”¯æŒå‘é‡æœºåˆ†ç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºæ ‡ç­¾y
    
    å‚æ•°ï¼š
        kernel: æ ¸å‡½æ•° 'linear'/'rbf'/'poly'/'sigmoid'ï¼Œé»˜è®¤'rbf'
        C: æ­£åˆ™åŒ–å‚æ•°ï¼Œé»˜è®¤1.0
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
    """
    def __init__(self, kernel='rbf'):
        super().__init__(f"SVM({kernel})")
        self.kernel = kernel
        self.params = {'C': 1.0, 'test_size': 0.2, 'random_state': 42}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.svm import SVC
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        from sklearn.preprocessing import StandardScaler
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        # SVMéœ€è¦æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'],
            random_state=self.params['random_state']
        )
        
        model = SVC(kernel=self.kernel, C=self.params['C'], random_state=self.params['random_state'])
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        self.result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'predictions': y_pred,
            'model': model,
            'scaler': scaler,
        }
        pipeline_data.set_model_output(self.result, "classification")
        pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        return pipeline_data


class XGBoostAdapter(ModelAdapter):
    """
    XGBoostå›å½’/åˆ†ç±»é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šå‰n-1åˆ—ä¸ºç‰¹å¾Xï¼Œæœ€åä¸€åˆ—ä¸ºç›®æ ‡y
    
    å‚æ•°ï¼š
        task: 'regression'/'classification'ï¼Œé»˜è®¤'regression'
        n_estimators: æ ‘çš„æ•°é‡
        max_depth: æœ€å¤§æ·±åº¦
        learning_rate: å­¦ä¹ ç‡
    """
    def __init__(self, task='regression'):
        super().__init__(f"XGBoost({task})")
        self.task = task
        self.params = {
            'n_estimators': 100, 'max_depth': 6, 
            'learning_rate': 0.1, 'test_size': 0.2, 'random_state': 42
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        try:
            from xgboost import XGBRegressor, XGBClassifier
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£…xgboost: pip install xgboost")
        
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import r2_score, mean_squared_error, accuracy_score
        
        data = pipeline_data.get_dataframe()
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.params['test_size'],
            random_state=self.params['random_state']
        )
        
        if self.task == 'classification':
            model = XGBClassifier(
                n_estimators=self.params['n_estimators'],
                max_depth=self.params['max_depth'],
                learning_rate=self.params['learning_rate'],
                random_state=self.params['random_state'],
                use_label_encoder=False, eval_metric='logloss'
            )
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            self.result = {
                'accuracy': accuracy_score(y_test, y_pred),
                'predictions': y_pred,
                'feature_importance': model.feature_importances_,
                'model': model,
            }
            pipeline_data._log(self.name, f"å‡†ç¡®ç‡: {self.result['accuracy']:.4f}")
        else:
            model = XGBRegressor(
                n_estimators=self.params['n_estimators'],
                max_depth=self.params['max_depth'],
                learning_rate=self.params['learning_rate'],
                random_state=self.params['random_state']
            )
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            self.result = {
                'r2_score': r2_score(y_test, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'predictions': y_pred,
                'feature_importance': model.feature_importances_,
                'model': model,
            }
            pipeline_data._log(self.name, f"RÂ² = {self.result['r2_score']:.4f}")
        
        pipeline_data.set_model_output(self.result, self.task)
        return pipeline_data


class ARIMAAdapter(ModelAdapter):
    """
    ARIMAæ—¶é—´åºåˆ—é¢„æµ‹é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šä¸€åˆ—æ—¶é—´åºåˆ—æ•°æ®
    
    å‚æ•°ï¼š
        order: (p,d,q) ARIMAé˜¶æ•°ï¼Œé»˜è®¤(1,1,1)
        n_predict: é¢„æµ‹æ­¥æ•°
    """
    def __init__(self):
        super().__init__("ARIMAé¢„æµ‹")
        self.params = {'order': (1, 1, 1), 'n_predict': 5}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        try:
            from statsmodels.tsa.arima.model import ARIMA
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£…statsmodels: pip install statsmodels")
        
        data = pipeline_data.get_array().flatten()
        n_predict = self.params['n_predict']
        
        model = ARIMA(data, order=self.params['order'])
        fitted = model.fit()
        forecast = fitted.forecast(steps=n_predict)
        
        self.result = {
            'fitted_values': fitted.fittedvalues,
            'predictions': forecast,
            'aic': fitted.aic,
            'bic': fitted.bic,
            'original': data,
        }
        pipeline_data.set_model_output(self.result, "prediction")
        pipeline_data._log(self.name, f"é¢„æµ‹äº† {n_predict} ä¸ªç‚¹, AIC={fitted.aic:.2f}")
        return pipeline_data


class ExponentialSmoothingAdapter(ModelAdapter):
    """
    æŒ‡æ•°å¹³æ»‘é¢„æµ‹é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šä¸€åˆ—æ—¶é—´åºåˆ—æ•°æ®
    
    å‚æ•°ï¼š
        alpha: å¹³æ»‘ç³»æ•°ï¼Œ0-1ä¹‹é—´
        n_predict: é¢„æµ‹æ­¥æ•°
    """
    def __init__(self):
        super().__init__("æŒ‡æ•°å¹³æ»‘é¢„æµ‹")
        self.params = {'alpha': 0.3, 'n_predict': 5}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_array().flatten()
        alpha = self.params['alpha']
        n_predict = self.params['n_predict']
        
        # ç®€å•æŒ‡æ•°å¹³æ»‘
        smoothed = np.zeros(len(data))
        smoothed[0] = data[0]
        for i in range(1, len(data)):
            smoothed[i] = alpha * data[i] + (1 - alpha) * smoothed[i-1]
        
        # é¢„æµ‹
        predictions = [smoothed[-1]] * n_predict
        
        self.result = {
            'smoothed': smoothed,
            'predictions': np.array(predictions),
            'alpha': alpha,
            'original': data,
        }
        pipeline_data.set_model_output(self.result, "prediction")
        pipeline_data._log(self.name, f"å¹³æ»‘ç³»æ•°Î±={alpha}, é¢„æµ‹{n_predict}æ­¥")
        return pipeline_data


class MonteCarloAdapter(ModelAdapter):
    """
    è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿé€‚é…å™¨ - è°ƒç”¨ models.probability.monte_carlo_simulation.MonteCarloSimulator
    
    éœ€è¦è®¾ç½®æ¨¡æ‹Ÿå‡½æ•°
    
    å‚æ•°ï¼š
        n_simulations: æ¨¡æ‹Ÿæ¬¡æ•°ï¼Œé»˜è®¤10000
        confidence: ç½®ä¿¡æ°´å¹³ï¼Œé»˜è®¤0.95
    """
    def __init__(self):
        super().__init__("è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ")
        self.params = {'n_simulations': 10000, 'confidence': 0.95}
        self.simulation_func = None
        self._mc_simulator = None
    
    def set_simulation(self, func):
        """è®¾ç½®æ¨¡æ‹Ÿå‡½æ•° f() -> float"""
        self.simulation_func = func
        return self
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        if self.simulation_func is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ¨¡æ‹Ÿå‡½æ•°: model.set_simulation(func)")
        
        n = self.params['n_simulations']
        conf = self.params['confidence']
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ MonteCarloSimulator
        if _MonteCarloSimulator is not None:
            simulator = _MonteCarloSimulator(n_simulations=n, verbose=False)
            results = simulator.simulate(self.simulation_func)
            self._mc_simulator = simulator
            
            z = 1.96 if conf == 0.95 else 2.576
            
            self.result = {
                'mean': simulator.mean,
                'std': simulator.std,
                'ci_lower': simulator.ci_lower if hasattr(simulator, 'ci_lower') else simulator.mean - z * simulator.std / np.sqrt(n),
                'ci_upper': simulator.ci_upper if hasattr(simulator, 'ci_upper') else simulator.mean + z * simulator.std / np.sqrt(n),
                'percentile_5': np.percentile(results, 5),
                'percentile_95': np.percentile(results, 95),
                'var_95': np.percentile(results, 5),
                'simulations': results,
            }
        else:
            # å›é€€åˆ°å†…ç½®å®ç°
            results = np.array([self.simulation_func() for _ in range(n)])
            
            mean = np.mean(results)
            std = np.std(results)
            se = std / np.sqrt(n)
            z = 1.96 if conf == 0.95 else 2.576
            
            self.result = {
                'mean': mean,
                'std': std,
                'ci_lower': mean - z * se,
                'ci_upper': mean + z * se,
                'percentile_5': np.percentile(results, 5),
                'percentile_95': np.percentile(results, 95),
                'var_95': np.percentile(results, 5),
                'simulations': results,
            }
        
        pipeline_data.set_model_output(self.result, "simulation")
        pipeline_data._log(self.name, f"å‡å€¼={self.result['mean']:.4f}, æ ‡å‡†å·®={self.result['std']:.4f}")
        return pipeline_data


class PCAAdapter(ModelAdapter):
    """
    PCAé™ç»´é€‚é…å™¨
    
    æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—ä¸€ä¸ªç‰¹å¾
    
    å‚æ•°ï¼š
        n_components: ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ï¼Œé»˜è®¤2
    """
    def __init__(self):
        super().__init__("PCAé™ç»´")
        self.params = {'n_components': 2}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler
        
        data = pipeline_data.get_array()
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data)
        
        pca = PCA(n_components=self.params['n_components'])
        transformed = pca.fit_transform(data_scaled)
        
        self.result = {
            'transformed_data': transformed,
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'cumulative_variance': np.cumsum(pca.explained_variance_ratio_),
            'components': pca.components_,
            'n_components': self.params['n_components'],
        }
        pipeline_data.set_model_output(self.result, "dimensionality_reduction")
        pipeline_data._log(self.name, f"ä¿ç•™{self.params['n_components']}ä¸ªä¸»æˆåˆ†, è§£é‡Šæ–¹å·®{sum(pca.explained_variance_ratio_)*100:.1f}%")
        return pipeline_data


class EntropyWeightAdapter(ModelAdapter):
    """
    ç†µæƒæ³•é€‚é…å™¨ - è°ƒç”¨ models.evaluation.evaluation_toolkit.EntropyWeightMethod
    
    æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªè¯„ä»·å¯¹è±¡ï¼Œæ¯åˆ—ä¸€ä¸ªæŒ‡æ ‡
    
    å‚æ•°ï¼š
        is_benefit: å„æŒ‡æ ‡æ˜¯å¦ä¸ºæ•ˆç›Šå‹ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ï¼Œé»˜è®¤å…¨ä¸ºTrue
    """
    def __init__(self):
        super().__init__("ç†µæƒæ³•")
        self.params = {'is_benefit': None}
        self._entropy_model = None
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        data = pipeline_data.get_dataframe()
        n_samples, n_features = data.shape
        
        is_benefit = self.params.get('is_benefit')
        
        # è½¬æ¢ is_benefit ä¸º indicator_types æ ¼å¼
        indicator_types = None
        if is_benefit is not None:
            indicator_types = ['positive' if b else 'negative' for b in is_benefit]
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ EntropyWeightMethod ç±»
        if _EntropyWeightMethod is not None:
            entropy_model = _EntropyWeightMethod(verbose=False)
            entropy_model.fit(data, indicator_types=indicator_types)
            self._entropy_model = entropy_model
            
            self.result = {
                'weights': entropy_model.weights,
                'entropy': entropy_model.entropy,
                'difference_coefficient': 1 - entropy_model.entropy,
                'weights_series': entropy_model.get_weights(),
            }
        else:
            # å›é€€åˆ°å†…ç½®å®ç°
            data_arr = data.values if isinstance(data, pd.DataFrame) else data
            
            if is_benefit is None:
                is_benefit = [True] * n_features
            
            data_pos = data_arr.copy()
            for j in range(n_features):
                if not is_benefit[j]:
                    data_pos[:, j] = data_pos[:, j].max() - data_pos[:, j]
            
            data_norm = data_pos / (data_pos.sum(axis=0) + 1e-10)
            
            entropy = np.zeros(n_features)
            for j in range(n_features):
                p = data_norm[:, j]
                p = p[p > 0]
                entropy[j] = -np.sum(p * np.log(p + 1e-10)) / np.log(n_samples)
            
            d = 1 - entropy
            weights = d / (d.sum() + 1e-10)
            
            self.result = {
                'weights': weights,
                'entropy': entropy,
                'difference_coefficient': d,
            }
        
        pipeline_data.set_model_output(self.result, "evaluation")
        pipeline_data._log(self.name, f"è®¡ç®—äº† {n_features} ä¸ªæŒ‡æ ‡çš„æƒé‡")
        return pipeline_data


class AHPAdapter(ModelAdapter):
    """
    å±‚æ¬¡åˆ†ææ³•(AHP)é€‚é…å™¨
    
    éœ€è¦è¾“å…¥åˆ¤æ–­çŸ©é˜µ
    
    å‚æ•°ï¼š
        comparison_matrix: åˆ¤æ–­çŸ©é˜µï¼ˆéœ€è¦ç”¨æˆ·è®¾ç½®ï¼‰
    """
    def __init__(self):
        super().__init__("å±‚æ¬¡åˆ†ææ³•AHP")
        self.params = {'comparison_matrix': None}
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        # ä½¿ç”¨ä¼ å…¥çš„åˆ¤æ–­çŸ©é˜µæˆ–ä»æ•°æ®ä¸­è·å–
        matrix = self.params.get('comparison_matrix')
        if matrix is None:
            matrix = pipeline_data.get_array()
        matrix = np.array(matrix)
        
        n = matrix.shape[0]
        
        # ç‰¹å¾å€¼æ³•æ±‚æƒé‡
        eigenvalues, eigenvectors = np.linalg.eig(matrix)
        max_idx = np.argmax(eigenvalues.real)
        lambda_max = eigenvalues[max_idx].real
        weights = eigenvectors[:, max_idx].real
        weights = weights / weights.sum()  # å½’ä¸€åŒ–
        
        # ä¸€è‡´æ€§æ£€éªŒ
        CI = (lambda_max - n) / (n - 1) if n > 1 else 0
        RI_table = {1: 0, 2: 0, 3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45}
        RI = RI_table.get(n, 1.45)
        CR = CI / RI if RI > 0 else 0
        
        self.result = {
            'weights': np.abs(weights),
            'lambda_max': lambda_max,
            'CI': CI,
            'CR': CR,
            'is_consistent': CR < 0.1,
        }
        pipeline_data.set_model_output(self.result, "evaluation")
        status = "é€šè¿‡" if CR < 0.1 else "æœªé€šè¿‡"
        pipeline_data._log(self.name, f"ä¸€è‡´æ€§æ£€éªŒ{status}, CR={CR:.4f}")
        return pipeline_data


class SimulatedAnnealingAdapter(ModelAdapter):
    """
    æ¨¡æ‹Ÿé€€ç«ä¼˜åŒ–é€‚é…å™¨
    
    éœ€è¦è®¾ç½®ç›®æ ‡å‡½æ•°
    
    å‚æ•°ï¼š
        T0: åˆå§‹æ¸©åº¦ï¼Œé»˜è®¤1000
        T_min: æœ€ä½æ¸©åº¦ï¼Œé»˜è®¤1e-8
        alpha: é™æ¸©ç³»æ•°ï¼Œé»˜è®¤0.95
        max_iter: æ¯ä¸ªæ¸©åº¦çš„è¿­ä»£æ¬¡æ•°
        bounds: æœç´¢èŒƒå›´
        n_dims: å˜é‡ç»´åº¦
    """
    def __init__(self):
        super().__init__("æ¨¡æ‹Ÿé€€ç«ä¼˜åŒ–")
        self.params = {
            'T0': 1000, 'T_min': 1e-8, 'alpha': 0.95,
            'max_iter': 100, 'bounds': (-5, 5), 'n_dims': 2
        }
        self.objective_func = None
    
    def set_objective(self, func):
        """è®¾ç½®ç›®æ ‡å‡½æ•°ï¼ˆæœ€å°åŒ–ï¼‰"""
        self.objective_func = func
        return self
    
    def run(self, pipeline_data: PipelineData = None) -> PipelineData:
        if self.objective_func is None:
            raise ValueError("è¯·å…ˆè®¾ç½®ç›®æ ‡å‡½æ•°: model.set_objective(func)")
        
        T = self.params['T0']
        T_min = self.params['T_min']
        alpha = self.params['alpha']
        bounds = self.params['bounds']
        n_dims = self.params['n_dims']
        max_iter = self.params['max_iter']
        
        # åˆå§‹è§£
        x = np.random.uniform(bounds[0], bounds[1], n_dims)
        f = self.objective_func(x)
        best_x, best_f = x.copy(), f
        history = [best_f]
        
        while T > T_min:
            for _ in range(max_iter):
                # ç”Ÿæˆæ–°è§£
                x_new = x + np.random.normal(0, T * 0.01, n_dims)
                x_new = np.clip(x_new, bounds[0], bounds[1])
                f_new = self.objective_func(x_new)
                
                # Metropoliså‡†åˆ™
                delta = f_new - f
                if delta < 0 or np.random.rand() < np.exp(-delta / T):
                    x, f = x_new, f_new
                    if f < best_f:
                        best_x, best_f = x.copy(), f
            
            history.append(best_f)
            T *= alpha
        
        self.result = {
            'best_position': best_x,
            'best_value': best_f,
            'convergence_history': history,
        }
        
        if pipeline_data:
            pipeline_data.set_model_output(self.result, "optimization")
            pipeline_data._log(self.name, f"æœ€ä¼˜å€¼: {best_f:.6f}")
        
        return pipeline_data


class GeneticAlgorithmAdapter(ModelAdapter):
    """
    é—ä¼ ç®—æ³•ä¼˜åŒ–é€‚é…å™¨ - è°ƒç”¨ models.optimization.optimization_toolkit.GeneticAlgorithm
    
    éœ€è¦è®¾ç½®ç›®æ ‡å‡½æ•°
    
    å‚æ•°ï¼š
        pop_size: ç§ç¾¤å¤§å°ï¼Œé»˜è®¤50
        max_gen: æœ€å¤§ä»£æ•°ï¼Œé»˜è®¤100
        crossover_rate: äº¤å‰æ¦‚ç‡
        mutation_rate: å˜å¼‚æ¦‚ç‡
        bounds: æœç´¢èŒƒå›´
        n_dims: å˜é‡ç»´åº¦
    """
    def __init__(self):
        super().__init__("é—ä¼ ç®—æ³•ä¼˜åŒ–")
        self.params = {
            'pop_size': 50, 'max_gen': 100,
            'crossover_rate': 0.8, 'mutation_rate': 0.1,
            'bounds': (-5, 5), 'n_dims': 2
        }
        self.objective_func = None
        self._ga_optimizer = None
    
    def set_objective(self, func):
        """è®¾ç½®ç›®æ ‡å‡½æ•°ï¼ˆæœ€å°åŒ–ï¼‰"""
        self.objective_func = func
        return self
    
    def run(self, pipeline_data: PipelineData = None) -> PipelineData:
        if self.objective_func is None:
            raise ValueError("è¯·å…ˆè®¾ç½®ç›®æ ‡å‡½æ•°: model.set_objective(func)")
        
        pop_size = self.params['pop_size']
        max_gen = self.params['max_gen']
        cr = self.params['crossover_rate']
        mr = self.params['mutation_rate']
        bounds = self.params['bounds']
        n_dims = self.params['n_dims']
        
        # è½¬æ¢ bounds æ ¼å¼
        if isinstance(bounds, tuple) and len(bounds) == 2:
            bounds_list = [bounds] * n_dims
        else:
            bounds_list = list(bounds)
        
        # ä¼˜å…ˆä½¿ç”¨åº“ä¸­çš„ GeneticAlgorithm
        if _GA is not None:
            ga = _GA(
                objective_func=self.objective_func,
                bounds=bounds_list,
                n_dims=n_dims,
                pop_size=pop_size,
                max_iter=max_gen,
                crossover_rate=cr,
                mutation_rate=mr,
                verbose=False
            )
            ga.optimize()
            self._ga_optimizer = ga
            
            self.result = {
                'best_position': ga.best_position,
                'best_value': ga.best_value,
                'convergence_history': ga.history,
            }
        else:
            # å›é€€åˆ°å†…ç½®å®ç°
            lb = bounds[0] if isinstance(bounds, tuple) else min(b[0] for b in bounds_list)
            ub = bounds[1] if isinstance(bounds, tuple) else max(b[1] for b in bounds_list)
            
            pop = np.random.uniform(lb, ub, (pop_size, n_dims))
            fitness = np.array([self.objective_func(ind) for ind in pop])
            best_idx = np.argmin(fitness)
            best_x, best_f = pop[best_idx].copy(), fitness[best_idx]
            history = [best_f]
            
            for gen in range(max_gen):
                fit_inv = 1 / (fitness + 1e-10)
                probs = fit_inv / fit_inv.sum()
                indices = np.random.choice(pop_size, pop_size, p=probs)
                new_pop = pop[indices].copy()
                
                for i in range(0, pop_size-1, 2):
                    if np.random.rand() < cr:
                        point = np.random.randint(1, n_dims)
                        new_pop[i, point:], new_pop[i+1, point:] = \
                            new_pop[i+1, point:].copy(), new_pop[i, point:].copy()
                
                for i in range(pop_size):
                    if np.random.rand() < mr:
                        j = np.random.randint(n_dims)
                        new_pop[i, j] += np.random.normal(0, (ub-lb)*0.1)
                
                new_pop = np.clip(new_pop, lb, ub)
                pop = new_pop
                fitness = np.array([self.objective_func(ind) for ind in pop])
                
                if fitness.min() < best_f:
                    best_idx = np.argmin(fitness)
                    best_x, best_f = pop[best_idx].copy(), fitness[best_idx]
                history.append(best_f)
            
            self.result = {
                'best_position': best_x,
                'best_value': best_f,
                'convergence_history': history,
            }
        
        if pipeline_data:
            pipeline_data.set_model_output(self.result, "optimization")
            pipeline_data._log(self.name, f"æœ€ä¼˜å€¼: {self.result['best_value']:.6f}")
        
        return pipeline_data


class IntegerProgrammingAdapter(ModelAdapter):
    """
    æ•´æ•°è§„åˆ’é€‚é…å™¨
    
    å‚æ•°ï¼š
        c: ç›®æ ‡å‡½æ•°ç³»æ•°ï¼ˆæœ€å¤§åŒ–æ—¶å–è´Ÿï¼‰
        A_ub, b_ub: ä¸ç­‰å¼çº¦æŸ Ax <= b
        A_eq, b_eq: ç­‰å¼çº¦æŸ
        bounds: å˜é‡èŒƒå›´
        integrality: æ•´æ•°çº¦æŸ (1=æ•´æ•°, 0=è¿ç»­)
    """
    def __init__(self):
        super().__init__("æ•´æ•°è§„åˆ’")
        self.params = {
            'c': None, 'A_ub': None, 'b_ub': None,
            'A_eq': None, 'b_eq': None, 'bounds': None,
            'integrality': None  # 1è¡¨ç¤ºæ•´æ•°å˜é‡
        }
    
    def run(self, pipeline_data: PipelineData) -> PipelineData:
        from scipy.optimize import milp, LinearConstraint, Bounds
        
        c = np.array(self.params['c'])
        
        constraints = []
        if self.params['A_ub'] is not None:
            A_ub = np.array(self.params['A_ub'])
            b_ub = np.array(self.params['b_ub'])
            constraints.append(LinearConstraint(A_ub, -np.inf, b_ub))
        
        if self.params['A_eq'] is not None:
            A_eq = np.array(self.params['A_eq'])
            b_eq = np.array(self.params['b_eq'])
            constraints.append(LinearConstraint(A_eq, b_eq, b_eq))
        
        bounds_param = self.params.get('bounds')
        if bounds_param:
            lb = [b[0] if b[0] is not None else -np.inf for b in bounds_param]
            ub = [b[1] if b[1] is not None else np.inf for b in bounds_param]
            bounds = Bounds(lb, ub)
        else:
            bounds = None
        
        integrality = self.params.get('integrality')
        
        result = milp(c, constraints=constraints, bounds=bounds, integrality=integrality)
        
        self.result = {
            'optimal_value': -result.fun if result.success else None,
            'optimal_solution': result.x,
            'success': result.success,
            'message': result.message,
        }
        pipeline_data.set_model_output(self.result, "optimization")
        pipeline_data._log(self.name, f"æœ€ä¼˜å€¼: {self.result['optimal_value']}")
        return pipeline_data


# ============================================================
# ã€æ ¸å¿ƒã€‘æ¨¡å‹å·¥å‚ - ä¸€ä¸ªå‚æ•°åˆ‡æ¢æ‰€æœ‰æ¨¡å‹
# ============================================================
"""
ã€æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ã€‘

åªéœ€è¦ä¸€è¡Œä»£ç åˆ‡æ¢æ¨¡å‹ï¼š
    pipeline.set_model(get_model("kmeans"))
    pipeline.set_model(get_model("topsis"))
    pipeline.set_model(get_model("grey"))

æ”¯æŒçš„æ¨¡å‹åç§°ï¼ˆå­—ç¬¦ä¸²å‚æ•°ï¼‰ï¼š

ã€èšç±»ç±»ã€‘
    "kmeans"           - K-Meansèšç±»
    "hierarchical"     - å±‚æ¬¡èšç±»
    
ã€åˆ†ç±»ç±»ã€‘
    "decision_tree"    - å†³ç­–æ ‘åˆ†ç±»
    "knn"              - KNNåˆ†ç±»
    "naive_bayes"      - æœ´ç´ è´å¶æ–¯
    "random_forest"    - éšæœºæ£®æ—
    "svm"              - æ”¯æŒå‘é‡æœº
    "xgboost_cls"      - XGBooståˆ†ç±»

ã€å›å½’ç±»ã€‘
    "linear"           - çº¿æ€§å›å½’
    "ridge"            - å²­å›å½’
    "lasso"            - Lassoå›å½’
    "polynomial"       - å¤šé¡¹å¼å›å½’
    "xgboost_reg"      - XGBoostå›å½’

ã€é¢„æµ‹ç±»ã€‘
    "grey"             - ç°è‰²é¢„æµ‹GM(1,1)
    "arima"            - ARIMAæ—¶é—´åºåˆ—
    "exp_smoothing"    - æŒ‡æ•°å¹³æ»‘

ã€è¯„ä»·ç±»ã€‘
    "topsis"           - TOPSISç»¼åˆè¯„ä»·
    "entropy"          - ç†µæƒæ³•
    "ahp"              - å±‚æ¬¡åˆ†ææ³•AHP

ã€ä¼˜åŒ–ç±»ã€‘
    "dp"               - åŠ¨æ€è§„åˆ’(èƒŒåŒ…)
    "pso"              - ç²’å­ç¾¤ä¼˜åŒ–
    "ga"               - é—ä¼ ç®—æ³•
    "sa"               - æ¨¡æ‹Ÿé€€ç«
    "linear_prog"      - çº¿æ€§è§„åˆ’
    "integer_prog"     - æ•´æ•°è§„åˆ’

ã€é™ç»´ç±»ã€‘
    "pca"              - PCAé™ç»´

ã€æ¨¡æ‹Ÿç±»ã€‘
    "monte_carlo"      - è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
"""

# æ¨¡å‹æ³¨å†Œè¡¨
MODEL_REGISTRY = {
    # ===== èšç±» =====
    "kmeans": KMeansAdapter,
    "hierarchical": HierarchicalClusteringAdapter,
    
    # ===== åˆ†ç±» =====
    "decision_tree": DecisionTreeAdapter,
    "knn": KNNAdapter,
    "naive_bayes": lambda: NaiveBayesAdapter('gaussian'),
    "random_forest": RandomForestAdapter,
    "svm": lambda: SVMAdapter('rbf'),
    "xgboost_cls": lambda: XGBoostAdapter('classification'),
    
    # ===== å›å½’ =====
    "linear": lambda: RegressionAdapter('linear'),
    "ridge": lambda: RegressionAdapter('ridge'),
    "lasso": lambda: RegressionAdapter('lasso'),
    "polynomial": lambda: RegressionAdapter('polynomial'),
    "xgboost_reg": lambda: XGBoostAdapter('regression'),
    
    # ===== é¢„æµ‹ =====
    "grey": GreyPredictionAdapter,
    "arima": ARIMAAdapter,
    "exp_smoothing": ExponentialSmoothingAdapter,
    
    # ===== è¯„ä»· =====
    "topsis": TOPSISAdapter,
    "entropy": EntropyWeightAdapter,
    "ahp": AHPAdapter,
    
    # ===== ä¼˜åŒ– =====
    "dp": DynamicProgrammingAdapter,
    "pso": lambda: OptimizationAdapter('pso'),
    "ga": GeneticAlgorithmAdapter,
    "sa": SimulatedAnnealingAdapter,
    "linear_prog": LinearProgrammingAdapter,
    "integer_prog": IntegerProgrammingAdapter,
    
    # ===== é™ç»´ =====
    "pca": PCAAdapter,
    
    # ===== æ¨¡æ‹Ÿ =====
    "monte_carlo": MonteCarloAdapter,
}


def get_model(name: str) -> ModelAdapter:
    """
    ã€ä¸€é”®è·å–æ¨¡å‹ã€‘
    
    ç”¨æ³•ï¼š
        model = get_model("kmeans")      # è·å–K-Meansèšç±»
        model = get_model("topsis")      # è·å–TOPSISè¯„ä»·
        model = get_model("grey")        # è·å–ç°è‰²é¢„æµ‹
    
    :param name: æ¨¡å‹åç§°ï¼Œè§ä¸Šæ–¹æ”¯æŒåˆ—è¡¨
    :return: å¯¹åº”çš„æ¨¡å‹é€‚é…å™¨å®ä¾‹
    
    å®Œæ•´ç¤ºä¾‹ï¼š
        pipeline = ModelValidationPipeline("æˆ‘çš„ä»»åŠ¡")
        pipeline.load_data(my_data, "æ•°æ®")
        pipeline.set_model(get_model("kmeans"))  # â† æ”¹è¿™é‡Œåˆ‡æ¢æ¨¡å‹
        pipeline.configure_model(n_clusters=4)    # â† æ”¹è¿™é‡Œè°ƒå‚æ•°
        pipeline.run()
        result = pipeline.get_model_result()
    """
    name = name.lower().strip()
    
    if name not in MODEL_REGISTRY:
        available = ", ".join(sorted(MODEL_REGISTRY.keys()))
        raise ValueError(
            f"æœªçŸ¥æ¨¡å‹: '{name}'\n"
            f"å¯ç”¨æ¨¡å‹: {available}\n"
            f"ä½¿ç”¨æ–¹æ³•: get_model('kmeans')"
        )
    
    model_cls = MODEL_REGISTRY[name]
    
    # å¦‚æœæ˜¯lambdaå‡½æ•°åˆ™è°ƒç”¨ï¼Œå¦åˆ™å®ä¾‹åŒ–ç±»
    if callable(model_cls) and not isinstance(model_cls, type):
        return model_cls()
    else:
        return model_cls()


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    print("\n" + "="*60)
    print("ğŸ“š å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    print("="*60)
    
    categories = {
        "èšç±»": ["kmeans", "hierarchical"],
        "åˆ†ç±»": ["decision_tree", "knn", "naive_bayes", "random_forest", "svm", "xgboost_cls"],
        "å›å½’": ["linear", "ridge", "lasso", "polynomial", "xgboost_reg"],
        "é¢„æµ‹": ["grey", "arima", "exp_smoothing"],
        "è¯„ä»·": ["topsis", "entropy", "ahp"],
        "ä¼˜åŒ–": ["dp", "pso", "ga", "sa", "linear_prog", "integer_prog"],
        "é™ç»´": ["pca"],
        "æ¨¡æ‹Ÿ": ["monte_carlo"],
    }
    
    for cat, models in categories.items():
        print(f"\nã€{cat}ç±»ã€‘")
        for m in models:
            adapter = get_model(m)
            print(f"    '{m}' â†’ {adapter.name}")
    
    print("\n" + "="*60)
    print("ç”¨æ³•: pipeline.set_model(get_model('æ¨¡å‹å'))")
    print("="*60)


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–æ­¥éª¤ (Visualization Steps)
# ============================================================
"""
ã€å¯è§†åŒ–æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿã€‘
- æ ¹æ®æ¨¡å‹ç»“æœè‡ªåŠ¨ç”Ÿæˆå›¾è¡¨
- å¯ä»¥æ·»åŠ 0ä¸ªã€1ä¸ªæˆ–å¤šä¸ª
- è¿è¡Œåå¯ä»¥ç”¨ show_figures() æ˜¾ç¤ºï¼Œsave_figures() ä¿å­˜

ã€å¯ç”¨çš„å¯è§†åŒ–ã€‘ï¼ˆåœ¨ç¬¬6æ­¥ add_visualization æ—¶é€‰æ‹©ï¼‰

1. DPTableVisualization()
   ç”¨é€”ï¼šåŠ¨æ€è§„åˆ’ç»“æœå¯è§†åŒ–
   ç”Ÿæˆï¼šDPè¡¨æ ¼çƒ­åŠ›å›¾ + ç‰©å“é€‰æ‹©æŸ±çŠ¶å›¾
   
   ç¤ºä¾‹ï¼špipeline.add_visualization(DPTableVisualization())

2. ConvergenceVisualization()
   ç”¨é€”ï¼šä¼˜åŒ–ç®—æ³•ç»“æœå¯è§†åŒ–
   ç”Ÿæˆï¼šæ”¶æ•›æ›²çº¿å›¾
   
   ç¤ºä¾‹ï¼špipeline.add_visualization(ConvergenceVisualization())

3. DataComparisonVisualization()
   ç”¨é€”ï¼šå¯¹æ¯”é¢„å¤„ç†å‰åçš„æ•°æ®åˆ†å¸ƒ
   ç”Ÿæˆï¼šå„åˆ—æ•°æ®çš„ç›´æ–¹å›¾å¯¹æ¯”
   
   ç¤ºä¾‹ï¼špipeline.add_visualization(DataComparisonVisualization())

4. è‡ªå®šä¹‰å¯è§†åŒ– - ç»§æ‰¿ VisualizationStep

ã€å¦‚ä½•æ·»åŠ ä½ è‡ªå·±çš„å¯è§†åŒ–ï¼Ÿã€‘
å¤åˆ¶ä¸‹é¢çš„æ¨¡æ¿ï¼š

```python
class MyVisualization(VisualizationStep):
    def __init__(self):
        super().__init__("å›¾è¡¨åç§°")
    
    def plot(self, pipeline_data):
        # è·å–æ•°æ®
        data = pipeline_data.get_dataframe()
        
        # è·å–æ¨¡å‹ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        if pipeline_data.model_output:
            result = pipeline_data.model_output['result']
        
        # åˆ›å»ºå›¾è¡¨
        self.fig, self.ax = plt.subplots(figsize=(10, 6))
        
        # ========== ä½ çš„ç»‘å›¾ä»£ç  ==========
        self.ax.plot(data.iloc[:, 0], data.iloc[:, 1])
        self.ax.set_title('æˆ‘çš„å›¾è¡¨')
        # ==================================
        
        return self.fig
```

ä½¿ç”¨ï¼š
    pipeline.add_visualization(MyVisualization())
"""

class VisualizationStep:
    """å¯è§†åŒ–æ­¥éª¤åŸºç±»"""
    
    def __init__(self, name="å¯è§†åŒ–"):
        self.name = name
        self.fig = None
        self.ax = None
    
    def plot(self, pipeline_data: PipelineData):
        """ç”Ÿæˆå›¾è¡¨"""
        raise NotImplementedError
    
    def save(self, filepath):
        """ä¿å­˜å›¾è¡¨"""
        if self.fig:
            self.fig.savefig(filepath, dpi=300, bbox_inches='tight')
            print(f"  ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {filepath}")


class DPTableVisualization(VisualizationStep):
    """åŠ¨æ€è§„åˆ’è¡¨æ ¼å¯è§†åŒ–"""
    
    def __init__(self):
        super().__init__("DPè¡¨æ ¼çƒ­åŠ›å›¾")
    
    def plot(self, pipeline_data: PipelineData):
        if PlotStyleConfig:
            PlotStyleConfig.setup_style()
        
        output = pipeline_data.model_output
        if not output or output.get('type') != 'dynamic_programming':
            print("âš ï¸ æ— åŠ¨æ€è§„åˆ’ç»“æœå¯è§†åŒ–")
            return None
        
        result = output['result']
        dp_table = result['dp_table']
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. DPè¡¨æ ¼çƒ­åŠ›å›¾
        ax1 = axes[0]
        im = ax1.imshow(dp_table, cmap='Blues', aspect='auto')
        ax1.set_xlabel('èƒŒåŒ…å®¹é‡', fontweight='bold')
        ax1.set_ylabel('ç‰©å“ç´¢å¼•', fontweight='bold')
        ax1.set_title('åŠ¨æ€è§„åˆ’è¡¨æ ¼ (DP Table)', fontsize=14, fontweight='bold')
        plt.colorbar(im, ax=ax1, label='æœ€å¤§ä»·å€¼')
        
        # æ ‡æ³¨é€‰ä¸­è·¯å¾„
        selected = result['selected_items']
        j = result['capacity']
        for i in range(len(result['weights']), 0, -1):
            if i-1 in selected:
                ax1.plot(j, i, 'r*', markersize=15)
                j -= result['weights'][i-1]
        
        # 2. ç‰©å“é€‰æ‹©å¯¹æ¯”
        ax2 = axes[1]
        x = np.arange(len(result['weights']))
        width = 0.35
        
        colors = ['#27AE60' if i in selected else '#CCCCCC' for i in range(len(x))]
        ax2.bar(x - width/2, result['weights'], width, label='é‡é‡', color=colors, alpha=0.8)
        ax2.bar(x + width/2, result['values'], width, label='ä»·å€¼', color=colors, edgecolor='black')
        
        ax2.set_xlabel('ç‰©å“ç´¢å¼•', fontweight='bold')
        ax2.set_ylabel('æ•°å€¼', fontweight='bold')
        ax2.set_title('ç‰©å“é€‰æ‹©ç»“æœ', fontsize=14, fontweight='bold')
        ax2.set_xticks(x)
        ax2.legend()
        
        # æ·»åŠ é€‰ä¸­æ ‡è®°
        for i in selected:
            ax2.annotate('âœ“', (i, max(result['weights'][i], result['values'][i]) + 0.5),
                        ha='center', fontsize=16, color='green', fontweight='bold')
        
        plt.tight_layout()
        self.fig = fig
        self.ax = axes
        
        # æ·»åŠ ç»“æœæ–‡æœ¬
        result_text = f"æœ€å¤§ä»·å€¼: {result['max_value']} | æ€»é‡é‡: {result['total_weight']}/{result['capacity']}"
        fig.suptitle(result_text, y=1.02, fontsize=12, fontweight='bold', color='#2E86AB')
        
        return fig


class ConvergenceVisualization(VisualizationStep):
    """æ”¶æ•›æ›²çº¿å¯è§†åŒ–"""
    
    def __init__(self):
        super().__init__("æ”¶æ•›æ›²çº¿")
    
    def plot(self, pipeline_data: PipelineData):
        if PlotStyleConfig:
            PlotStyleConfig.setup_style()
        
        output = pipeline_data.model_output
        if not output or output.get('type') != 'optimization':
            print("âš ï¸ æ— ä¼˜åŒ–ç»“æœå¯è§†åŒ–")
            return None
        
        result = output['result']
        history = result['convergence_history']
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        ax.plot(history, color='#2E86AB', linewidth=2.5, label='æœ€ä¼˜é€‚åº”åº¦')
        ax.fill_between(range(len(history)), history, alpha=0.2, color='#2E86AB')
        ax.scatter([len(history)-1], [history[-1]], color='#C73E1D', s=100, zorder=5, label=f'æœ€ç»ˆ: {history[-1]:.6f}')
        
        ax.set_xlabel('è¿­ä»£æ¬¡æ•°', fontweight='bold')
        ax.set_ylabel('é€‚åº”åº¦å€¼', fontweight='bold')
        ax.set_title('ä¼˜åŒ–ç®—æ³•æ”¶æ•›æ›²çº¿', fontsize=14, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        self.fig = fig
        self.ax = ax
        return fig


class DataComparisonVisualization(VisualizationStep):
    """æ•°æ®å¯¹æ¯”å¯è§†åŒ–ï¼ˆé¢„å¤„ç†å‰åï¼‰"""
    
    def __init__(self):
        super().__init__("æ•°æ®å¯¹æ¯”")
    
    def plot(self, pipeline_data: PipelineData):
        if PlotStyleConfig:
            PlotStyleConfig.setup_style()
        
        raw = pipeline_data.raw_data
        processed = pipeline_data.processed_data
        
        if raw is None or processed is None:
            print("âš ï¸ æ— æ•°æ®å¯å¯¹æ¯”")
            return None
        
        n_cols = min(4, len(raw.select_dtypes(include=[np.number]).columns))
        fig, axes = plt.subplots(2, n_cols, figsize=(4*n_cols, 8))
        
        numeric_cols = raw.select_dtypes(include=[np.number]).columns[:n_cols]
        
        for i, col in enumerate(numeric_cols):
            # åŸå§‹æ•°æ®
            axes[0, i].hist(raw[col].dropna(), bins=20, color='#A23B72', alpha=0.7, edgecolor='white')
            axes[0, i].set_title(f'{col} (åŸå§‹)', fontweight='bold')
            axes[0, i].set_xlabel('å€¼')
            axes[0, i].set_ylabel('é¢‘æ•°')
            
            # å¤„ç†åæ•°æ®
            axes[1, i].hist(processed[col].dropna(), bins=20, color='#2E86AB', alpha=0.7, edgecolor='white')
            axes[1, i].set_title(f'{col} (å¤„ç†å)', fontweight='bold')
            axes[1, i].set_xlabel('å€¼')
            axes[1, i].set_ylabel('é¢‘æ•°')
        
        fig.suptitle('æ•°æ®é¢„å¤„ç†å‰åå¯¹æ¯”', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        self.fig = fig
        self.ax = axes
        return fig


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»å·¥ä½œæµç±» (Main Pipeline)
# ============================================================
"""
ã€ModelValidationPipeline å®Œæ•´ä½¿ç”¨æŒ‡å—ã€‘

è¿™æ˜¯å·¥ä½œæµçš„ä¸»ç±»ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤ä½¿ç”¨ï¼š

æ­¥éª¤1: åˆ›å»ºå·¥ä½œæµ
    pipeline = ModelValidationPipeline("ä»»åŠ¡åç§°")
    # "ä»»åŠ¡åç§°"ä¼šæ˜¾ç¤ºåœ¨è¾“å‡ºå’Œä¿å­˜çš„æ–‡ä»¶åä¸­

æ­¥éª¤2: åŠ è½½æ•°æ®
    pipeline.load_data(data, "æ•°æ®æè¿°")
    # data å¯ä»¥æ˜¯ï¼š
    #   - pandas DataFrameï¼ˆæ¨èï¼‰
    #   - numpy array
    #   - Python list
    #   - dict

æ­¥éª¤3: æ·»åŠ é¢„å¤„ç†ï¼ˆå¯é€‰ï¼Œå¯è·³è¿‡ï¼‰
    pipeline.add_preprocessing(MissingValueStep('mean'))
    pipeline.add_preprocessing(OutlierRemovalStep('iqr'))
    # å¯ä»¥æ·»åŠ å¤šä¸ªï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
    # ä¸éœ€è¦é¢„å¤„ç†å°±è·³è¿‡è¿™æ­¥

æ­¥éª¤4: è®¾ç½®æ¨¡å‹ï¼ˆå¿…é¡»ï¼‰
    pipeline.set_model(DynamicProgrammingAdapter())
    pipeline.configure_model(capacity=10)
    # å¿…é¡»è®¾ç½®ä¸€ä¸ªæ¨¡å‹
    # configure_model è®¾ç½®æ¨¡å‹å‚æ•°

æ­¥éª¤5: æ·»åŠ å¯è§†åŒ–ï¼ˆå¯é€‰ï¼Œå¯è·³è¿‡ï¼‰
    pipeline.add_visualization(DPTableVisualization())
    # å¯ä»¥æ·»åŠ å¤šä¸ª
    # ä¸éœ€è¦å¯è§†åŒ–å°±è·³è¿‡è¿™æ­¥

æ­¥éª¤6: è¿è¡Œ
    pipeline.run()
    # è¿™ä¸€æ­¥ä¼šä¾æ¬¡æ‰§è¡Œï¼šé¢„å¤„ç† â†’ æ¨¡å‹ â†’ å¯è§†åŒ–

æ­¥éª¤7: è·å–ç»“æœ
    result = pipeline.get_model_result()     # è·å–æ¨¡å‹ç»“æœï¼ˆå­—å…¸ï¼‰
    data = pipeline.get_processed_data()     # è·å–å¤„ç†åçš„æ•°æ®
    pipeline.show_results()                  # æ‰“å°ç»“æœæ‘˜è¦
    pipeline.show_figures()                  # æ˜¾ç¤ºå›¾è¡¨
    pipeline.save_figures('./output/')       # ä¿å­˜å›¾è¡¨
"""

class ModelValidationPipeline:
    """
    æ¨¡å‹éªŒè¯å·¥ä½œæµ - ä¸²è”æ‰€æœ‰æ¨¡å—
    
    ã€å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ã€‘
    
    ```python
    from workflow.model_validation_pipeline import *
    
    # === æ­¥éª¤1: åˆ›å»ºå·¥ä½œæµ ===
    pipeline = ModelValidationPipeline("èƒŒåŒ…é—®é¢˜éªŒè¯")
    
    # === æ­¥éª¤2: åŠ è½½æ•°æ® ===
    items_data = [[2, 6], [2, 3], [6, 5], [5, 4], [4, 6]]
    pipeline.load_data(items_data, "ç‰©å“åˆ—è¡¨")
    
    # === æ­¥éª¤3: æ·»åŠ é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰===
    # å¦‚æœæ•°æ®å¹²å‡€å¯ä»¥è·³è¿‡è¿™æ­¥
    pipeline.add_preprocessing(MissingValueStep('mean'))
    pipeline.add_preprocessing(OutlierRemovalStep('iqr'))
    
    # === æ­¥éª¤4: è®¾ç½®æ¨¡å‹ ===
    pipeline.set_model(DynamicProgrammingAdapter())
    pipeline.configure_model(capacity=10)
    
    # === æ­¥éª¤5: æ·»åŠ å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰===
    pipeline.add_visualization(DPTableVisualization())
    
    # === æ­¥éª¤6: è¿è¡Œ ===
    pipeline.run()
    
    # === æ­¥éª¤7: è·å–ç»“æœ ===
    result = pipeline.get_model_result()
    print(f"æœ€å¤§ä»·å€¼: {result['max_value']}")
    
    pipeline.show_figures()  # æ˜¾ç¤ºå›¾è¡¨
    ```
    """
    
    def __init__(self, name="æ¨¡å‹éªŒè¯å·¥ä½œæµ", save_dir='./figures'):
        """
        åˆ›å»ºå·¥ä½œæµ
        
        :param name: å·¥ä½œæµåç§°ï¼ˆä¼šæ˜¾ç¤ºåœ¨è¾“å‡ºä¸­ï¼‰
        :param save_dir: å›¾è¡¨ä¿å­˜ç›®å½•
        """
        self.name = name
        self.pipeline_data = None
        self.preprocessing_steps = []
        self.model = None
        self.visualizations = []
        self.save_dir = save_dir
        self.completed = False
        
        os.makedirs(save_dir, exist_ok=True)
        
        print("\n" + "="*60)
        print(f"ğŸš€ åˆå§‹åŒ–å·¥ä½œæµ: {name}")
        print("="*60)
    
    def load_data(self, data, name="è¾“å…¥æ•°æ®"):
        """
        ã€æ­¥éª¤2ã€‘åŠ è½½æ•°æ®
        
        :param data: æ•°æ®ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
            - pandas DataFrameï¼ˆæ¨èï¼‰
            - numpy array  
            - Python listï¼Œå¦‚ [[1,2], [3,4]]
            - dictï¼Œå¦‚ {'col1': [1,2], 'col2': [3,4]}
        :param name: æ•°æ®æè¿°ï¼ˆæ˜¾ç¤ºç”¨ï¼‰
        
        ç¤ºä¾‹ï¼š
            pipeline.load_data(my_dataframe, "å®éªŒæ•°æ®")
            pipeline.load_data([[1,2], [3,4]], "ç‰©å“åˆ—è¡¨")
        """
        self.pipeline_data = PipelineData(data, name)
        print(f"âœ… æ•°æ®å·²åŠ è½½: {name}")
        return self
    
    def add_preprocessing(self, step: PreprocessingStep):
        """
        ã€æ­¥éª¤3ã€‘æ·»åŠ é¢„å¤„ç†æ­¥éª¤ï¼ˆå¯é€‰ï¼Œå¯å¤šæ¬¡è°ƒç”¨ï¼‰
        
        :param step: é¢„å¤„ç†æ­¥éª¤ï¼Œå¯é€‰ï¼š
            - MissingValueStep('mean')      å‡å€¼å¡«å……ç¼ºå¤±å€¼
            - MissingValueStep('median')    ä¸­ä½æ•°å¡«å……
            - MissingValueStep('drop')      åˆ é™¤ç¼ºå¤±è¡Œ
            - OutlierRemovalStep('iqr')     IQRå¼‚å¸¸å€¼å¤„ç†
            - OutlierRemovalStep('zscore')  Z-scoreå¼‚å¸¸å€¼å¤„ç†
            - NormalizationStep('zscore')   Z-scoreæ ‡å‡†åŒ–
            - NormalizationStep('minmax')   Min-Maxå½’ä¸€åŒ–
        
        ç¤ºä¾‹ï¼š
            pipeline.add_preprocessing(MissingValueStep('mean'))
            pipeline.add_preprocessing(OutlierRemovalStep('iqr', 1.5))
        """
        self.preprocessing_steps.append(step)
        print(f"  â• é¢„å¤„ç†æ­¥éª¤: {step.name}")
        return self
    
    def set_model(self, model: ModelAdapter):
        """
        ã€æ­¥éª¤4ã€‘è®¾ç½®æ¨¡å‹ï¼ˆå¿…é¡»ï¼‰
        
        :param model: æ¨¡å‹é€‚é…å™¨ï¼Œå¯é€‰ï¼š
            - DynamicProgrammingAdapter()  åŠ¨æ€è§„åˆ’ï¼ˆèƒŒåŒ…é—®é¢˜ï¼‰
            - OptimizationAdapter('pso')   ç²’å­ç¾¤ä¼˜åŒ–
            - è‡ªå®šä¹‰æ¨¡å‹ï¼ˆç»§æ‰¿ModelAdapterï¼‰
        
        ç¤ºä¾‹ï¼š
            pipeline.set_model(DynamicProgrammingAdapter())
            
            # ä¼˜åŒ–é—®é¢˜éœ€è¦å…ˆè®¾ç½®ç›®æ ‡å‡½æ•°
            model = OptimizationAdapter('pso')
            model.set_objective(my_func)
            pipeline.set_model(model)
        """
        self.model = model
        print(f"âœ… æ¨¡å‹å·²è®¾ç½®: {model.name}")
        return self
    
    def configure_model(self, **kwargs):
        """
        ã€æ­¥éª¤4ç»­ã€‘é…ç½®æ¨¡å‹å‚æ•°
        
        :param kwargs: æ¨¡å‹å‚æ•°ï¼Œå–å†³äºä½¿ç”¨çš„æ¨¡å‹ï¼š
        
        DynamicProgrammingAdapter å‚æ•°ï¼š
            - capacity: èƒŒåŒ…å®¹é‡ï¼ˆæ•´æ•°ï¼‰
        
        OptimizationAdapter å‚æ•°ï¼š
            - bounds: æœç´¢èŒƒå›´ï¼Œå¦‚ (-5, 5)
            - n_dims: å˜é‡ç»´åº¦
            - max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            - n_particles: ç²’å­æ•°é‡ï¼ˆä»…PSOï¼‰
        
        ç¤ºä¾‹ï¼š
            pipeline.configure_model(capacity=15)
            pipeline.configure_model(bounds=(-10, 10), n_dims=3, max_iter=100)
        """
        if self.model:
            self.model.set_params(**kwargs)
            print(f"  âš™ï¸ æ¨¡å‹å‚æ•°æ›´æ–°: {kwargs}")
        return self
    
    def add_visualization(self, viz: VisualizationStep):
        """
        ã€æ­¥éª¤5ã€‘æ·»åŠ å¯è§†åŒ–ï¼ˆå¯é€‰ï¼Œå¯å¤šæ¬¡è°ƒç”¨ï¼‰
        
        :param viz: å¯è§†åŒ–æ­¥éª¤ï¼Œå¯é€‰ï¼š
            - DPTableVisualization()        åŠ¨æ€è§„åˆ’è¡¨æ ¼çƒ­åŠ›å›¾
            - ConvergenceVisualization()    ä¼˜åŒ–æ”¶æ•›æ›²çº¿
            - DataComparisonVisualization() é¢„å¤„ç†å‰åå¯¹æ¯”
            - è‡ªå®šä¹‰å¯è§†åŒ–ï¼ˆç»§æ‰¿VisualizationStepï¼‰
        
        ç¤ºä¾‹ï¼š
            pipeline.add_visualization(DPTableVisualization())
        """
        self.visualizations.append(viz)
        print(f"  ğŸ“Š å¯è§†åŒ–: {viz.name}")
        return self
    
    def run(self):
        """
        ã€æ­¥éª¤6ã€‘è¿è¡Œå·¥ä½œæµ
        
        æ‰§è¡Œé¡ºåºï¼šé¢„å¤„ç† â†’ æ¨¡å‹ â†’ å¯è§†åŒ–
        
        è¿è¡Œåå¯ä»¥ï¼š
            - get_model_result()    è·å–æ¨¡å‹ç»“æœ
            - get_processed_data()  è·å–å¤„ç†åçš„æ•°æ®
            - show_results()        æ‰“å°ç»“æœæ‘˜è¦
            - show_figures()        æ˜¾ç¤ºå›¾è¡¨
            - save_figures(path)    ä¿å­˜å›¾è¡¨
        """
        print("\n" + "-"*60)
        print("â–¶ï¸ å¼€å§‹è¿è¡Œå·¥ä½œæµ...")
        print("-"*60)
        
        if self.pipeline_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®ï¼")
        
        # 1. é¢„å¤„ç†
        print("\nğŸ“¦ Step 1: æ•°æ®é¢„å¤„ç†")
        for step in self.preprocessing_steps:
            self.pipeline_data = step.apply(self.pipeline_data)
            print(f"    âœ“ {step.name} å®Œæˆ")
        
        # 2. è¿è¡Œæ¨¡å‹
        print("\nğŸ”§ Step 2: è¿è¡Œæ¨¡å‹")
        if self.model:
            self.pipeline_data = self.model.run(self.pipeline_data)
            print(f"    âœ“ {self.model.name} å®Œæˆ")
        
        # 3. ç”Ÿæˆå¯è§†åŒ–
        print("\nğŸ“Š Step 3: ç”Ÿæˆå¯è§†åŒ–")
        for viz in self.visualizations:
            viz.plot(self.pipeline_data)
            print(f"    âœ“ {viz.name} ç”Ÿæˆå®Œæˆ")
        
        self.completed = True
        print("\n" + "="*60)
        print("âœ… å·¥ä½œæµè¿è¡Œå®Œæˆ!")
        print("="*60)
        return self
    
    def show_results(self):
        """
        ã€æ­¥éª¤7ã€‘æ˜¾ç¤ºç»“æœæ‘˜è¦
        æ‰“å°æ•°æ®ä¿¡æ¯å’Œæ¨¡å‹ç»“æœ
        """
        if not self.completed:
            print("âš ï¸ è¯·å…ˆè¿è¡Œå·¥ä½œæµ (run())")
            return
        
        self.pipeline_data.summary()
        
        if self.model and self.model.result:
            print("\nğŸ“‹ æ¨¡å‹ç»“æœ:")
            for k, v in self.model.result.items():
                if isinstance(v, np.ndarray):
                    print(f"    {k}: shape={v.shape}")
                elif isinstance(v, list) and len(v) > 10:
                    print(f"    {k}: list(len={len(v)})")
                else:
                    print(f"    {k}: {v}")
    
    def show_figures(self):
        """æ˜¾ç¤ºæ‰€æœ‰å›¾è¡¨"""
        for viz in self.visualizations:
            if viz.fig:
                plt.figure(viz.fig.number)
                plt.show()
    
    def save_figures(self, directory=None):
        """ä¿å­˜æ‰€æœ‰å›¾è¡¨"""
        save_dir = directory or self.save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        for i, viz in enumerate(self.visualizations):
            if viz.fig:
                filename = f"{self.name}_{viz.name}_{timestamp}.png"
                filepath = os.path.join(save_dir, filename)
                viz.save(filepath)
    
    def get_model_result(self):
        """
        ã€æ­¥éª¤7ã€‘è·å–æ¨¡å‹ç»“æœ
        
        :return: dictï¼Œæ¨¡å‹è¾“å‡ºçš„ç»“æœå­—å…¸
        
        ç¤ºä¾‹ï¼š
            result = pipeline.get_model_result()
            print(result['max_value'])  # åŠ¨æ€è§„åˆ’çš„æœ€å¤§ä»·å€¼
            print(result['best_position'])  # ä¼˜åŒ–çš„æœ€ä¼˜è§£
        """
        return self.model.result if self.model else None
    
    def get_processed_data(self):
        """
        ã€æ­¥éª¤7ã€‘è·å–é¢„å¤„ç†åçš„æ•°æ®
        
        :return: pandas DataFrame
        
        ç¤ºä¾‹ï¼š
            clean_data = pipeline.get_processed_data()
            clean_data.to_csv('clean_data.csv')
        """
        return self.pipeline_data.get_dataframe() if self.pipeline_data else None


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå¿«é€Ÿå·¥å‚å‡½æ•° (Quick Factory Functions)
# ============================================================
"""
ã€å¿«é€Ÿå‡½æ•°ã€‘
å¦‚æœä½ åªæ˜¯æƒ³å¿«é€ŸéªŒè¯ï¼Œä¸éœ€è¦è‡ªå®šä¹‰é…ç½®ï¼Œå¯ä»¥ç›´æ¥ç”¨è¿™äº›å‡½æ•°ï¼š

1. quick_dp_validation(items_data, capacity=10)
   - å¿«é€Ÿè¿è¡ŒåŠ¨æ€è§„åˆ’èƒŒåŒ…é—®é¢˜
   - items_data: [[é‡é‡, ä»·å€¼], ...] æ ¼å¼çš„æ•°æ®
   - capacity: èƒŒåŒ…å®¹é‡
   
   ç¤ºä¾‹ï¼š
       items = [[2, 6], [2, 3], [6, 5], [5, 4], [4, 6]]
       result = quick_dp_validation(items, capacity=10)
       print(f"æœ€å¤§ä»·å€¼: {result['max_value']}")

2. quick_optimization_validation(objective_func, bounds, n_dims, max_iter)
   - å¿«é€Ÿè¿è¡Œä¼˜åŒ–ç®—æ³•æ±‚å‡½æ•°æœ€å°å€¼
   
   ç¤ºä¾‹ï¼š
       def sphere(x): return sum(xi**2 for xi in x)
       result = quick_optimization_validation(sphere, bounds=(-5, 5), n_dims=2)
       print(f"æœ€ä¼˜è§£: {result['best_position']}")
"""

def quick_dp_validation(items_data, capacity=10, save_dir='./figures'):
    """
    å¿«é€ŸåŠ¨æ€è§„åˆ’éªŒè¯
    
    :param items_data: ç‰©å“æ•°æ® [[é‡é‡, ä»·å€¼], ...]
    :param capacity: èƒŒåŒ…å®¹é‡
    :param save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    
    ç¤ºä¾‹ï¼š
        items = [[2, 6], [2, 3], [6, 5], [5, 4], [4, 6]]
        result = quick_dp_validation(items, capacity=10)
    """
    pipeline = ModelValidationPipeline("èƒŒåŒ…é—®é¢˜", save_dir)
    pipeline.load_data(items_data, "ç‰©å“æ•°æ®")
    pipeline.set_model(DynamicProgrammingAdapter())
    pipeline.configure_model(capacity=capacity)
    pipeline.add_visualization(DPTableVisualization())
    pipeline.run()
    pipeline.show_results()
    pipeline.show_figures()
    return pipeline.get_model_result()


def quick_optimization_validation(objective_func, bounds=(-5, 5), n_dims=2, 
                                   max_iter=100, save_dir='./figures'):
    """
    å¿«é€Ÿä¼˜åŒ–ç®—æ³•éªŒè¯
    
    :param objective_func: ç›®æ ‡å‡½æ•°
    :param bounds: å˜é‡èŒƒå›´
    :param n_dims: ç»´åº¦
    :param max_iter: è¿­ä»£æ¬¡æ•°
    
    ç¤ºä¾‹ï¼š
        def sphere(x): return np.sum(x**2)
        result = quick_optimization_validation(sphere, bounds=(-5, 5), n_dims=3)
    """
    pipeline = ModelValidationPipeline("ä¼˜åŒ–éªŒè¯", save_dir)
    pipeline.pipeline_data = PipelineData(name="ä¼˜åŒ–é—®é¢˜")  # ä¼˜åŒ–é—®é¢˜å¯èƒ½ä¸éœ€è¦å¤–éƒ¨æ•°æ®
    
    model = OptimizationAdapter('pso')
    model.set_objective(objective_func)
    model.set_params(bounds=bounds, n_dims=n_dims, max_iter=max_iter)
    
    pipeline.set_model(model)
    pipeline.add_visualization(ConvergenceVisualization())
    pipeline.run()
    pipeline.show_results()
    pipeline.show_figures()
    return pipeline.get_model_result()


# ============================================================
# æ¼”ç¤º
# ============================================================
"""
ã€è¿è¡Œæ¼”ç¤ºã€‘
ç›´æ¥è¿è¡Œè¿™ä¸ªæ–‡ä»¶å¯ä»¥çœ‹åˆ°å·¥ä½œæµçš„æ•ˆæœï¼š
    python model_validation_pipeline.py

ã€å¸¸è§é—®é¢˜ã€‘

Q: æˆ‘ä¸éœ€è¦é¢„å¤„ç†ï¼Œå¯ä»¥è·³è¿‡å—ï¼Ÿ
A: å¯ä»¥ï¼Œä¸è°ƒç”¨ add_preprocessing() å°±è¡Œã€‚

Q: æˆ‘ä¸éœ€è¦å¯è§†åŒ–ï¼Œå¯ä»¥è·³è¿‡å—ï¼Ÿ
A: å¯ä»¥ï¼Œä¸è°ƒç”¨ add_visualization() å°±è¡Œã€‚

Q: æˆ‘æƒ³åªåšæ•°æ®æ¸…æ´—ï¼Œä¸éœ€è¦æ¨¡å‹ï¼Ÿ
A: å¯ä»¥ï¼Œä¸è°ƒç”¨ set_model()ï¼Œrun() åç”¨ get_processed_data() è·å–æ•°æ®ã€‚

Q: æˆ‘çš„æ¨¡å‹éœ€è¦ç‰¹æ®Šæ ¼å¼çš„æ•°æ®æ€ä¹ˆåŠï¼Ÿ
A: åœ¨ä½ çš„ ModelAdapter.run() æ–¹æ³•ä¸­ï¼Œç”¨ pipeline_data.get_xxx() è·å–æ•°æ®åè‡ªå·±è½¬æ¢ã€‚

Q: æ€ä¹ˆå¯¹æ¯”ä¸åŒå‚æ•°çš„æ•ˆæœï¼Ÿ
A: åˆ›å»ºå¤šä¸ª pipelineï¼Œæ¯ä¸ªç”¨ä¸åŒå‚æ•°ï¼Œåˆ†åˆ«è¿è¡Œã€‚

Q: æ€ä¹ˆä¿å­˜ä¸­é—´ç»“æœï¼Ÿ
A: pipeline_data.metadata['my_key'] = value  # ä¿å­˜
   value = pipeline_data.metadata['my_key']  # è¯»å–
"""

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ¯ æ¨¡å‹éªŒè¯å·¥ä½œæµæ¼”ç¤º - ä¸€ä¸ªå‚æ•°åˆ‡æ¢æ¨¡å‹")
    print("="*70)
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    list_models()
    
    # ============================================================
    # ç¤ºä¾‹1: èšç±»åˆ†æ
    # ============================================================
    print("\n\nğŸ“Œ ç¤ºä¾‹1: K-Meansèšç±»")
    print("-"*50)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    cluster_data = np.vstack([
        np.random.randn(30, 2) + [0, 0],
        np.random.randn(30, 2) + [5, 5],
        np.random.randn(30, 2) + [10, 0],
    ])
    
    pipeline = ModelValidationPipeline("èšç±»åˆ†æ", save_dir='./modelCode/figures')
    pipeline.load_data(cluster_data, "èšç±»æ•°æ®")
    pipeline.set_model(get_model("kmeans"))       # â† åªéœ€æ”¹è¿™é‡Œï¼
    pipeline.configure_model(n_clusters=3)
    pipeline.run()
    
    result = pipeline.get_model_result()
    print(f"èšç±»æ ‡ç­¾: {result['labels'][:10]}...")
    
    # ============================================================
    # ç¤ºä¾‹2: TOPSISè¯„ä»·
    # ============================================================
    print("\n\nğŸ“Œ ç¤ºä¾‹2: TOPSISç»¼åˆè¯„ä»·")
    print("-"*50)
    
    eval_data = pd.DataFrame({
        'è´¨é‡': [90, 85, 70, 95, 80],
        'ä»·æ ¼': [100, 150, 80, 200, 120],  # æˆæœ¬å‹
        'äº¤è´§æœŸ': [5, 10, 3, 15, 7],       # æˆæœ¬å‹
        'æœåŠ¡': [85, 80, 90, 75, 88],
    })
    
    pipeline2 = ModelValidationPipeline("ä¾›åº”å•†è¯„ä»·", save_dir='./modelCode/figures')
    pipeline2.load_data(eval_data, "ä¾›åº”å•†æ•°æ®")
    pipeline2.set_model(get_model("topsis"))     # â† åªéœ€æ”¹è¿™é‡Œï¼
    pipeline2.configure_model(is_benefit=[True, False, False, True])
    pipeline2.run()
    
    result2 = pipeline2.get_model_result()
    print(f"è¯„åˆ†: {result2['scores']}")
    print(f"æ’å: {result2['ranking']}")
    
    # ============================================================
    # ç¤ºä¾‹3: ç°è‰²é¢„æµ‹
    # ============================================================
    print("\n\nğŸ“Œ ç¤ºä¾‹3: ç°è‰²é¢„æµ‹GM(1,1)")
    print("-"*50)
    
    time_series = pd.DataFrame({'å€¼': [100, 112, 125, 138, 150, 165]})
    
    pipeline3 = ModelValidationPipeline("é”€é‡é¢„æµ‹", save_dir='./modelCode/figures')
    pipeline3.load_data(time_series, "å†å²é”€é‡")
    pipeline3.set_model(get_model("grey"))        # â† åªéœ€æ”¹è¿™é‡Œï¼
    pipeline3.configure_model(n_predict=3)
    pipeline3.run()
    
    result3 = pipeline3.get_model_result()
    print(f"é¢„æµ‹å€¼: {result3['predictions']}")
    
    # ============================================================
    # ç¤ºä¾‹4: åŠ¨æ€è§„åˆ’èƒŒåŒ…é—®é¢˜
    # ============================================================
    print("\n\nğŸ“Œ ç¤ºä¾‹4: åŠ¨æ€è§„åˆ’èƒŒåŒ…é—®é¢˜")
    print("-"*50)
    
    items = pd.DataFrame({'é‡é‡': [2, 2, 6, 5, 4], 'ä»·å€¼': [6, 3, 5, 4, 6]})
    
    pipeline4 = ModelValidationPipeline("èƒŒåŒ…é—®é¢˜", save_dir='./modelCode/figures')
    pipeline4.load_data(items, "ç‰©å“åˆ—è¡¨")
    pipeline4.set_model(get_model("dp"))          # â† åªéœ€æ”¹è¿™é‡Œï¼
    pipeline4.configure_model(capacity=10)
    pipeline4.add_visualization(DPTableVisualization())
    pipeline4.run()
    
    result4 = pipeline4.get_model_result()
    print(f"æœ€å¤§ä»·å€¼: {result4['max_value']}")
    print(f"é€‰ä¸­ç‰©å“: {result4['selected_items']}")
    
    # ============================================================
    # ç¤ºä¾‹5: åˆ†ç±»ï¼ˆéšæœºæ£®æ—ï¼‰
    # ============================================================
    print("\n\nğŸ“Œ ç¤ºä¾‹5: éšæœºæ£®æ—åˆ†ç±»")
    print("-"*50)
    
    from sklearn.datasets import load_iris
    iris = load_iris()
    clf_data = pd.DataFrame(iris.data, columns=iris.feature_names)
    clf_data['label'] = iris.target
    
    pipeline5 = ModelValidationPipeline("é¸¢å°¾èŠ±åˆ†ç±»", save_dir='./modelCode/figures')
    pipeline5.load_data(clf_data, "é¸¢å°¾èŠ±æ•°æ®")
    pipeline5.set_model(get_model("random_forest"))  # â† åªéœ€æ”¹è¿™é‡Œï¼
    pipeline5.configure_model(n_estimators=50)
    pipeline5.run()
    
    result5 = pipeline5.get_model_result()
    print(f"å‡†ç¡®ç‡: {result5['accuracy']:.4f}")
    
    print("\n\n" + "="*70)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("="*70)
    print("""
ã€ä½¿ç”¨æ–¹æ³•æ€»ç»“ã€‘

    pipeline.set_model(get_model("æ¨¡å‹å"))
    
    æŠŠ "æ¨¡å‹å" æ¢æˆä½ éœ€è¦çš„æ¨¡å‹å³å¯ï¼š
    
    èšç±»: "kmeans", "hierarchical"
    åˆ†ç±»: "decision_tree", "knn", "naive_bayes", "random_forest", "svm"
    å›å½’: "linear", "ridge", "lasso", "polynomial"
    é¢„æµ‹: "grey", "arima", "exp_smoothing"
    è¯„ä»·: "topsis", "entropy", "ahp"
    ä¼˜åŒ–: "dp", "pso", "ga", "sa", "linear_prog"
    é™ç»´: "pca"
    æ¨¡æ‹Ÿ: "monte_carlo"
    """)
    
    plt.show()
