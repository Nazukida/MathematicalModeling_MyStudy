"""
============================================================
æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹å·¥å…·é›† (Data Preprocessing & Feature Engineering)
åŒ…å«ï¼šPCAé™ç»´ + IQRå¼‚å¸¸å€¼æ£€æµ‹ + æ ‡å‡†åŒ– + ç¼ºå¤±å€¼å¤„ç† + ç‰¹å¾é€‰æ‹©
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šæ•°æ®æ¸…æ´—ã€å¼‚å¸¸å€¼å¤„ç†ã€é™ç»´ã€ç‰¹å¾å·¥ç¨‹
ç‰¹ç‚¹ï¼šå®Œæ•´çš„å‚æ•°è®¾ç½®ã€å¯è§†åŒ–ä¸ç¾åŒ–ã€ç»“æœè§£é‡Š
ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ
============================================================

ä½¿ç”¨åœºæ™¯ï¼š
- æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†
- å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
- é™ç»´ä¸ç‰¹å¾æå–
- æ•°æ®è´¨é‡è¯„ä¼°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
import warnings
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

warnings.filterwarnings('ignore')


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå…¨å±€é…ç½®ä¸ç¾åŒ–è®¾ç½® (Global Configuration)
# ============================================================

class PlotStyleConfig:
    """å›¾è¡¨ç¾åŒ–é…ç½®ç±»"""
    
    COLORS = {
        'primary': '#2E86AB',
        'secondary': '#A23B72',
        'accent': '#F18F01',
        'success': '#27AE60',
        'danger': '#C73E1D',
        'neutral': '#3B3B3B'
    }
    
    PALETTE = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6B4C9A', '#1B998B']
    
    @staticmethod
    def setup_style():
        plt.style.use('seaborn-v0_8-whitegrid')
        rcParams['figure.figsize'] = (12, 8)
        rcParams['figure.dpi'] = 100
        rcParams['savefig.dpi'] = 300
        rcParams['font.size'] = 11
        rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        rcParams['axes.unicode_minus'] = False

PlotStyleConfig.setup_style()


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ ·æœ¬æ•°æ®ç”Ÿæˆå™¨ (Sample Data Generator)
# ============================================================

class SampleDataGenerator:
    """æ ·æœ¬æ•°æ®ç”Ÿæˆå™¨ - ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    
    def __init__(self, random_seed=42):
        self.random_seed = random_seed
        np.random.seed(random_seed)
    
    def generate_with_missing_and_outliers(self, n_samples=200, 
                                            n_features=6, 
                                            missing_rate=0.1,
                                            outlier_rate=0.05):
        """
        ç”Ÿæˆå¸¦æœ‰ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼çš„æ•°æ®
        
        :param n_samples: æ ·æœ¬æ•°é‡
        :param n_features: ç‰¹å¾æ•°é‡
        :param missing_rate: ç¼ºå¤±ç‡
        :param outlier_rate: å¼‚å¸¸å€¼æ¯”ä¾‹
        """
        # ç”ŸæˆåŸºç¡€æ•°æ®
        feature_names = [f'ç‰¹å¾{i+1}' for i in range(n_features)]
        data = pd.DataFrame(
            np.random.randn(n_samples, n_features) * 10 + 50,
            columns=feature_names
        )
        
        # æ·»åŠ ç¼ºå¤±å€¼
        n_missing = int(n_samples * n_features * missing_rate)
        for _ in range(n_missing):
            row = np.random.randint(0, n_samples)
            col = np.random.randint(0, n_features)
            data.iloc[row, col] = np.nan
        
        # æ·»åŠ å¼‚å¸¸å€¼
        n_outliers = int(n_samples * n_features * outlier_rate)
        for _ in range(n_outliers):
            row = np.random.randint(0, n_samples)
            col = np.random.randint(0, n_features)
            # å¼‚å¸¸å€¼ä¸ºæ­£å¸¸å€¼çš„3-5å€
            data.iloc[row, col] = data.iloc[row, col] * np.random.choice([-1, 1]) * np.random.uniform(3, 5)
        
        return {
            'data': data,
            'feature_names': feature_names,
            'n_samples': n_samples,
            'n_features': n_features,
            'missing_rate': missing_rate,
            'outlier_rate': outlier_rate
        }
    
    def generate_high_dimensional(self, n_samples=300, n_features=20,
                                   n_informative=5, n_redundant=5):
        """
        ç”Ÿæˆé«˜ç»´æ•°æ®ï¼ˆé€‚åˆPCAé™ç»´ï¼‰
        
        :param n_samples: æ ·æœ¬æ•°
        :param n_features: æ€»ç‰¹å¾æ•°
        :param n_informative: ä¿¡æ¯ç‰¹å¾æ•°
        :param n_redundant: å†—ä½™ç‰¹å¾æ•°
        """
        # ç”Ÿæˆä¿¡æ¯ç‰¹å¾
        informative = np.random.randn(n_samples, n_informative)
        
        # ç”Ÿæˆå†—ä½™ç‰¹å¾ï¼ˆä¿¡æ¯ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼‰
        redundant = np.zeros((n_samples, n_redundant))
        for i in range(n_redundant):
            weights = np.random.randn(n_informative)
            redundant[:, i] = informative @ weights + np.random.randn(n_samples) * 0.1
        
        # ç”Ÿæˆå™ªå£°ç‰¹å¾
        n_noise = n_features - n_informative - n_redundant
        noise = np.random.randn(n_samples, n_noise)
        
        # åˆå¹¶
        data = np.hstack([informative, redundant, noise])
        
        feature_names = [f'ç‰¹å¾{i+1}' for i in range(n_features)]
        df = pd.DataFrame(data, columns=feature_names)
        
        return {
            'data': df,
            'feature_names': feature_names,
            'n_informative': n_informative,
            'n_redundant': n_redundant,
            'n_noise': n_noise
        }


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼ºå¤±å€¼å¤„ç†å™¨ (Missing Value Handler)
# ============================================================

class MissingValueHandler:
    """
    ç¼ºå¤±å€¼å¤„ç†å·¥å…·
    
    æ–¹æ³•ï¼š
    - åˆ é™¤æ³•ï¼šåˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ/åˆ—
    - å¡«å……æ³•ï¼šå‡å€¼/ä¸­ä½æ•°/ä¼—æ•°/å¸¸æ•°å¡«å……
    - æ’å€¼æ³•ï¼šKNNæ’è¡¥
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.original_missing_info = None
        self.handled_data = None
    
    def analyze_missing(self, data):
        """åˆ†æç¼ºå¤±å€¼æƒ…å†µ"""
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        missing_count = data.isnull().sum()
        missing_rate = (data.isnull().sum() / len(data) * 100).round(2)
        
        self.original_missing_info = pd.DataFrame({
            'ç¼ºå¤±æ•°é‡': missing_count,
            'ç¼ºå¤±ç‡(%)': missing_rate
        })
        
        total_missing = data.isnull().sum().sum()
        total_cells = data.shape[0] * data.shape[1]
        
        if self.verbose:
            print("\n" + "="*60)
            print("ğŸ“Š ç¼ºå¤±å€¼åˆ†ææŠ¥å‘Š")
            print("="*60)
            print(f"  æ•°æ®å½¢çŠ¶: {data.shape}")
            print(f"  æ€»ç¼ºå¤±å€¼: {total_missing} / {total_cells} ({total_missing/total_cells*100:.2f}%)")
            print(f"\n  å„åˆ—ç¼ºå¤±æƒ…å†µ:")
            print(self.original_missing_info[self.original_missing_info['ç¼ºå¤±æ•°é‡'] > 0])
            print("="*60)
        
        return self.original_missing_info
    
    def fill_missing(self, data, method='mean', constant=0, n_neighbors=5):
        """
        å¡«å……ç¼ºå¤±å€¼
        
        :param data: æ•°æ®
        :param method: å¡«å……æ–¹æ³•
            - 'mean': å‡å€¼å¡«å……
            - 'median': ä¸­ä½æ•°å¡«å……
            - 'mode': ä¼—æ•°å¡«å……
            - 'constant': å¸¸æ•°å¡«å……
            - 'knn': KNNæ’è¡¥
        :param constant: å¸¸æ•°å¡«å……çš„å€¼
        :param n_neighbors: KNNçš„è¿‘é‚»æ•°
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        data = data.copy()
        
        if method == 'mean':
            imputer = SimpleImputer(strategy='mean')
            filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
        
        elif method == 'median':
            imputer = SimpleImputer(strategy='median')
            filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
        
        elif method == 'mode':
            imputer = SimpleImputer(strategy='most_frequent')
            filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
        
        elif method == 'constant':
            imputer = SimpleImputer(strategy='constant', fill_value=constant)
            filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
        
        elif method == 'knn':
            imputer = KNNImputer(n_neighbors=n_neighbors)
            filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¡«å……æ–¹æ³•: {method}")
        
        self.handled_data = filled
        
        if self.verbose:
            print(f"\nâœ… ç¼ºå¤±å€¼å¡«å……å®Œæˆ (æ–¹æ³•: {method})")
            print(f"  å¡«å……å‰ç¼ºå¤±: {data.isnull().sum().sum()}")
            print(f"  å¡«å……åç¼ºå¤±: {filled.isnull().sum().sum()}")
        
        return filled
    
    def drop_missing(self, data, axis=0, thresh=None):
        """
        åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ/åˆ—
        
        :param axis: 0åˆ é™¤è¡Œï¼Œ1åˆ é™¤åˆ—
        :param thresh: éç¼ºå¤±å€¼çš„æœ€å°æ•°é‡
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        original_shape = data.shape
        
        if thresh is not None:
            cleaned = data.dropna(axis=axis, thresh=thresh)
        else:
            cleaned = data.dropna(axis=axis)
        
        self.handled_data = cleaned
        
        if self.verbose:
            if axis == 0:
                print(f"\nâœ… åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ")
                print(f"  åˆ é™¤å‰: {original_shape[0]} è¡Œ")
                print(f"  åˆ é™¤å: {cleaned.shape[0]} è¡Œ")
            else:
                print(f"\nâœ… åˆ é™¤å«ç¼ºå¤±å€¼çš„åˆ—")
                print(f"  åˆ é™¤å‰: {original_shape[1]} åˆ—")
                print(f"  åˆ é™¤å: {cleaned.shape[1]} åˆ—")
        
        return cleaned


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¼‚å¸¸å€¼æ£€æµ‹å™¨ (Outlier Detector)
# ============================================================

class OutlierDetector:
    """
    å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†å·¥å…·
    
    æ–¹æ³•ï¼š
    - IQRæ–¹æ³•ï¼šå››åˆ†ä½æ•°é—´è·
    - Z-scoreæ–¹æ³•ï¼šæ ‡å‡†åŒ–å¾—åˆ†
    - ç®±çº¿å›¾æ³•ï¼šå¯è§†åŒ–æ£€æµ‹
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.outlier_info = {}
        self.outlier_mask = None
    
    def detect_iqr(self, data, column=None, factor=1.5):
        """
        IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        
        IQR = Q3 - Q1
        å¼‚å¸¸å€¼: < Q1 - factor*IQR æˆ– > Q3 + factor*IQR
        
        :param data: æ•°æ®
        :param column: æŒ‡å®šåˆ—ï¼ˆNoneåˆ™æ£€æµ‹æ‰€æœ‰åˆ—ï¼‰
        :param factor: IQRå€æ•°ï¼ˆé€šå¸¸1.5ä¸ºè½»åº¦å¼‚å¸¸ï¼Œ3ä¸ºæåº¦å¼‚å¸¸ï¼‰
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        if column is not None:
            columns = [column]
        else:
            columns = data.select_dtypes(include=[np.number]).columns
        
        outlier_mask = pd.DataFrame(False, index=data.index, columns=columns)
        self.outlier_info = {}
        
        for col in columns:
            values = data[col].dropna()
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - factor * IQR
            upper_bound = Q3 + factor * IQR
            
            is_outlier = (data[col] < lower_bound) | (data[col] > upper_bound)
            outlier_mask[col] = is_outlier
            
            n_outliers = is_outlier.sum()
            
            self.outlier_info[col] = {
                'Q1': Q1,
                'Q3': Q3,
                'IQR': IQR,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound,
                'n_outliers': n_outliers,
                'outlier_rate': n_outliers / len(data) * 100
            }
        
        self.outlier_mask = outlier_mask
        
        if self.verbose:
            self._print_iqr_results()
        
        return outlier_mask
    
    def _print_iqr_results(self):
        """æ‰“å°IQRæ£€æµ‹ç»“æœ"""
        print("\n" + "="*70)
        print("ğŸ” IQRå¼‚å¸¸å€¼æ£€æµ‹ç»“æœ")
        print("="*70)
        
        for col, info in self.outlier_info.items():
            if info['n_outliers'] > 0:
                print(f"\n  ğŸ“Œ {col}:")
                print(f"      Q1={info['Q1']:.2f}, Q3={info['Q3']:.2f}, IQR={info['IQR']:.2f}")
                print(f"      æ­£å¸¸èŒƒå›´: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]")
                print(f"      å¼‚å¸¸å€¼: {info['n_outliers']} ä¸ª ({info['outlier_rate']:.2f}%)")
        
        total_outliers = self.outlier_mask.sum().sum()
        print(f"\n  æ€»å¼‚å¸¸å€¼æ•°é‡: {total_outliers}")
        print("="*70)
    
    def detect_zscore(self, data, column=None, threshold=3.0):
        """
        Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        
        Z = (x - mean) / std
        å¼‚å¸¸å€¼: |Z| > threshold
        
        :param threshold: Zåˆ†æ•°é˜ˆå€¼ï¼ˆé€šå¸¸2æˆ–3ï¼‰
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        if column is not None:
            columns = [column]
        else:
            columns = data.select_dtypes(include=[np.number]).columns
        
        outlier_mask = pd.DataFrame(False, index=data.index, columns=columns)
        
        for col in columns:
            values = data[col].dropna()
            mean = values.mean()
            std = values.std()
            
            if std == 0:
                continue
            
            z_scores = (data[col] - mean) / std
            is_outlier = np.abs(z_scores) > threshold
            outlier_mask[col] = is_outlier
        
        self.outlier_mask = outlier_mask
        
        if self.verbose:
            total = outlier_mask.sum().sum()
            print(f"\nâœ… Z-scoreæ£€æµ‹å®Œæˆ (é˜ˆå€¼={threshold})")
            print(f"  æ€»å¼‚å¸¸å€¼: {total}")
        
        return outlier_mask
    
    def handle_outliers(self, data, method='clip', outlier_mask=None):
        """
        å¤„ç†å¼‚å¸¸å€¼
        
        :param method:
            - 'remove': åˆ é™¤å¼‚å¸¸å€¼æ‰€åœ¨è¡Œ
            - 'clip': æˆªæ–­åˆ°è¾¹ç•Œå€¼
            - 'replace_mean': æ›¿æ¢ä¸ºå‡å€¼
            - 'replace_median': æ›¿æ¢ä¸ºä¸­ä½æ•°
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        if outlier_mask is None:
            outlier_mask = self.outlier_mask
        
        data = data.copy()
        
        if method == 'remove':
            # åˆ é™¤ä»»æ„åˆ—æœ‰å¼‚å¸¸å€¼çš„è¡Œ
            mask = outlier_mask.any(axis=1)
            handled = data[~mask]
            
        elif method == 'clip':
            handled = data.copy()
            for col in outlier_mask.columns:
                if col in self.outlier_info:
                    lower = self.outlier_info[col]['lower_bound']
                    upper = self.outlier_info[col]['upper_bound']
                    handled[col] = handled[col].clip(lower, upper)
        
        elif method == 'replace_mean':
            handled = data.copy()
            for col in outlier_mask.columns:
                mean_val = data.loc[~outlier_mask[col], col].mean()
                handled.loc[outlier_mask[col], col] = mean_val
        
        elif method == 'replace_median':
            handled = data.copy()
            for col in outlier_mask.columns:
                median_val = data.loc[~outlier_mask[col], col].median()
                handled.loc[outlier_mask[col], col] = median_val
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¤„ç†æ–¹æ³•: {method}")
        
        if self.verbose:
            print(f"\nâœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆ (æ–¹æ³•: {method})")
            print(f"  å¤„ç†å‰è¡Œæ•°: {len(data)}")
            print(f"  å¤„ç†åè¡Œæ•°: {len(handled)}")
        
        return handled


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®æ ‡å‡†åŒ–å™¨ (Data Scaler)
# ============================================================

class DataScaler:
    """
    æ•°æ®æ ‡å‡†åŒ–å·¥å…·
    
    æ–¹æ³•ï¼š
    - Z-scoreæ ‡å‡†åŒ–
    - Min-Maxæ ‡å‡†åŒ–
    - Robustæ ‡å‡†åŒ–
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.scaler = None
        self.original_stats = None
    
    def fit_transform(self, data, method='standard'):
        """
        æ ‡å‡†åŒ–æ•°æ®
        
        :param method:
            - 'standard': Z-scoreæ ‡å‡†åŒ– (x-mean)/std
            - 'minmax': Min-Maxæ ‡å‡†åŒ– åˆ°[0,1]
            - 'robust': é²æ£’æ ‡å‡†åŒ–ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        # è®°å½•åŸå§‹ç»Ÿè®¡ä¿¡æ¯
        self.original_stats = {
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max()
        }
        
        if method == 'standard':
            self.scaler = StandardScaler()
        elif method == 'minmax':
            self.scaler = MinMaxScaler()
        elif method == 'robust':
            self.scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")
        
        scaled = pd.DataFrame(
            self.scaler.fit_transform(data),
            columns=data.columns,
            index=data.index
        )
        
        if self.verbose:
            print(f"\nâœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ (æ–¹æ³•: {method})")
            print(f"  æ ‡å‡†åŒ–å‰å‡å€¼: {data.mean().mean():.4f}")
            print(f"  æ ‡å‡†åŒ–åå‡å€¼: {scaled.mean().mean():.4f}")
        
        return scaled
    
    def inverse_transform(self, data):
        """é€†å˜æ¢"""
        if self.scaler is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨fit_transform")
        
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        original = pd.DataFrame(
            self.scaler.inverse_transform(data),
            columns=data.columns,
            index=data.index
        )
        
        return original


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šPCAé™ç»´å·¥å…· (PCA Dimensionality Reduction)
# ============================================================

class PCAReducer:
    """
    PCAä¸»æˆåˆ†åˆ†æé™ç»´
    
    åŸç†ï¼š
    é€šè¿‡æ­£äº¤å˜æ¢å°†ç›¸å…³å˜é‡è½¬æ¢ä¸ºçº¿æ€§ä¸ç›¸å…³çš„ä¸»æˆåˆ†
    æŒ‰æ–¹å·®å¤§å°æ’åºï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯
    
    åº”ç”¨ï¼š
    - é«˜ç»´æ•°æ®å¯è§†åŒ–
    - ç‰¹å¾å‹ç¼©
    - å»é™¤å™ªå£°
    """
    
    def __init__(self, n_components=None, verbose=True):
        """
        å‚æ•°é…ç½®
        
        :param n_components:
            - int: ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡
            - float (0-1): ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹
            - None: ä¿ç•™æ‰€æœ‰ä¸»æˆåˆ†
        """
        self.n_components = n_components
        self.verbose = verbose
        self.pca = None
        self.scaler = None
        self.explained_variance_ratio = None
        self.components = None
        self.feature_names = None
    
    def fit_transform(self, data, scale=True):
        """
        æ‰§è¡ŒPCAé™ç»´
        
        :param data: åŸå§‹æ•°æ®
        :param scale: æ˜¯å¦å…ˆæ ‡å‡†åŒ–
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        self.feature_names = list(data.columns)
        
        # æ ‡å‡†åŒ–
        if scale:
            self.scaler = StandardScaler()
            scaled_data = self.scaler.fit_transform(data)
        else:
            scaled_data = data.values
        
        # PCA
        self.pca = PCA(n_components=self.n_components)
        transformed = self.pca.fit_transform(scaled_data)
        
        self.explained_variance_ratio = self.pca.explained_variance_ratio_
        self.components = self.pca.components_
        
        # åˆ›å»ºç»“æœDataFrame
        n_pcs = transformed.shape[1]
        pc_names = [f'PC{i+1}' for i in range(n_pcs)]
        result = pd.DataFrame(transformed, columns=pc_names, index=data.index)
        
        if self.verbose:
            self._print_results()
        
        return result
    
    def _print_results(self):
        """æ‰“å°PCAç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š PCAé™ç»´åˆ†æç»“æœ")
        print("="*60)
        print(f"  åŸå§‹ç‰¹å¾æ•°: {len(self.feature_names)}")
        print(f"  ä¿ç•™ä¸»æˆåˆ†æ•°: {len(self.explained_variance_ratio)}")
        print(f"\n  å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šç‡:")
        
        cumulative = 0
        for i, ratio in enumerate(self.explained_variance_ratio):
            cumulative += ratio
            print(f"    PC{i+1}: {ratio*100:6.2f}% (ç´¯è®¡: {cumulative*100:6.2f}%)")
        
        print(f"\n  æ€»æ–¹å·®è§£é‡Šç‡: {self.explained_variance_ratio.sum()*100:.2f}%")
        print("="*60)
    
    def get_loadings(self):
        """è·å–è½½è·çŸ©é˜µï¼ˆä¸»æˆåˆ†ä¸åŸå§‹å˜é‡çš„ç›¸å…³ç³»æ•°ï¼‰"""
        if self.pca is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨fit_transform")
        
        loadings = pd.DataFrame(
            self.components.T,
            index=self.feature_names,
            columns=[f'PC{i+1}' for i in range(len(self.explained_variance_ratio))]
        )
        
        return loadings
    
    def select_n_components(self, data, target_variance=0.95, scale=True):
        """
        è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°é‡
        
        :param target_variance: ç›®æ ‡æ–¹å·®è§£é‡Šç‡
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        if scale:
            scaler = StandardScaler()
            scaled = scaler.fit_transform(data)
        else:
            scaled = data.values
        
        pca_full = PCA()
        pca_full.fit(scaled)
        
        cumulative = np.cumsum(pca_full.explained_variance_ratio_)
        n_components = np.argmax(cumulative >= target_variance) + 1
        
        if self.verbose:
            print(f"\nâœ… è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°é‡")
            print(f"  ç›®æ ‡æ–¹å·®è§£é‡Šç‡: {target_variance*100}%")
            print(f"  æ¨èä¸»æˆåˆ†æ•°: {n_components}")
            print(f"  å®é™…æ–¹å·®è§£é‡Šç‡: {cumulative[n_components-1]*100:.2f}%")
        
        return n_components, cumulative


# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç‰¹å¾é€‰æ‹©å™¨ (Feature Selector)
# ============================================================

class FeatureSelector:
    """
    ç‰¹å¾é€‰æ‹©å·¥å…·
    
    æ–¹æ³•ï¼š
    - æ–¹å·®é˜ˆå€¼
    - ç›¸å…³æ€§è¿‡æ»¤
    - ç»Ÿè®¡æ£€éªŒï¼ˆFæ£€éªŒã€äº’ä¿¡æ¯ï¼‰
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.selected_features = None
        self.feature_scores = None
    
    def select_by_variance(self, data, threshold=0.01):
        """
        æ–¹å·®é˜ˆå€¼é€‰æ‹©
        ä½æ–¹å·®ç‰¹å¾é€šå¸¸ä¿¡æ¯é‡å°
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        variances = data.var()
        selected = variances[variances > threshold].index.tolist()
        dropped = variances[variances <= threshold].index.tolist()
        
        self.selected_features = selected
        
        if self.verbose:
            print(f"\nâœ… æ–¹å·®é˜ˆå€¼ç‰¹å¾é€‰æ‹© (é˜ˆå€¼={threshold})")
            print(f"  ä¿ç•™ç‰¹å¾: {len(selected)}")
            print(f"  åˆ é™¤ç‰¹å¾: {len(dropped)}")
            if dropped:
                print(f"  åˆ é™¤çš„ä½æ–¹å·®ç‰¹å¾: {dropped}")
        
        return data[selected]
    
    def select_by_correlation(self, data, threshold=0.9):
        """
        ç›¸å…³æ€§è¿‡æ»¤
        åˆ é™¤é«˜åº¦ç›¸å…³çš„å†—ä½™ç‰¹å¾
        """
        if isinstance(data, np.ndarray):
            data = pd.DataFrame(data)
        
        corr_matrix = data.corr().abs()
        
        # ä¸Šä¸‰è§’çŸ©é˜µ
        upper = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # æ‰¾å‡ºé«˜ç›¸å…³ç‰¹å¾
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        
        selected = [col for col in data.columns if col not in to_drop]
        self.selected_features = selected
        
        if self.verbose:
            print(f"\nâœ… ç›¸å…³æ€§è¿‡æ»¤ (é˜ˆå€¼={threshold})")
            print(f"  ä¿ç•™ç‰¹å¾: {len(selected)}")
            print(f"  åˆ é™¤ç‰¹å¾: {len(to_drop)}")
            if to_drop:
                print(f"  åˆ é™¤çš„å†—ä½™ç‰¹å¾: {to_drop}")
        
        return data[selected]
    
    def select_k_best(self, X, y, k=5, method='f_classif'):
        """
        ç»Ÿè®¡æ£€éªŒé€‰æ‹©Top Kç‰¹å¾
        
        :param method:
            - 'f_classif': Fæ£€éªŒï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
            - 'mutual_info': äº’ä¿¡æ¯ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
        """
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)
        
        if method == 'f_classif':
            selector = SelectKBest(f_classif, k=k)
        elif method == 'mutual_info':
            selector = SelectKBest(mutual_info_classif, k=k)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        selector.fit(X, y)
        
        scores = pd.Series(selector.scores_, index=X.columns)
        self.feature_scores = scores.sort_values(ascending=False)
        
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        self.selected_features = selected
        
        if self.verbose:
            print(f"\nâœ… SelectKBestç‰¹å¾é€‰æ‹© (k={k}, æ–¹æ³•={method})")
            print(f"  é€‰ä¸­ç‰¹å¾: {selected}")
            print(f"\n  ç‰¹å¾è¯„åˆ†:")
            for name, score in self.feature_scores.head(10).items():
                print(f"    {name}: {score:.4f}")
        
        return X[selected], self.feature_scores


# ============================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šå¯è§†åŒ–æ¨¡å— (Visualization)
# ============================================================

class PreprocessingVisualizer:
    """æ•°æ®é¢„å¤„ç†å¯è§†åŒ–ç±»"""
    
    def __init__(self):
        self.colors = PlotStyleConfig.PALETTE
    
    def plot_missing_heatmap(self, data, title="ç¼ºå¤±å€¼åˆ†å¸ƒçƒ­åŠ›å›¾", save_path=None):
        """ç»˜åˆ¶ç¼ºå¤±å€¼çƒ­åŠ›å›¾"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        missing = data.isnull().astype(int)
        im = ax.imshow(missing.T, aspect='auto', cmap='YlOrRd', interpolation='nearest')
        
        ax.set_yticks(range(len(data.columns)))
        ax.set_yticklabels(data.columns)
        ax.set_xlabel('æ ·æœ¬ç´¢å¼•', fontweight='bold')
        ax.set_ylabel('ç‰¹å¾', fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold')
        
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_ticks([0, 1])
        cbar.set_ticklabels(['éç¼ºå¤±', 'ç¼ºå¤±'])
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_boxplot(self, data, title="ç®±çº¿å›¾ - å¼‚å¸¸å€¼å¯è§†åŒ–", save_path=None):
        """ç»˜åˆ¶ç®±çº¿å›¾"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        data.boxplot(ax=ax, patch_artist=True,
                    boxprops=dict(facecolor=self.colors[0], alpha=0.7),
                    medianprops=dict(color='red', linewidth=2),
                    flierprops=dict(marker='o', markerfacecolor='red', markersize=5))
        
        ax.set_xlabel('ç‰¹å¾', fontweight='bold')
        ax.set_ylabel('å€¼', fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_pca_variance(self, explained_variance_ratio, 
                          title="PCAæ–¹å·®è§£é‡Šç‡", save_path=None):
        """ç»˜åˆ¶PCAæ–¹å·®è§£é‡Šå›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        n_components = len(explained_variance_ratio)
        x = range(1, n_components + 1)
        cumulative = np.cumsum(explained_variance_ratio)
        
        # å•ç‹¬æ–¹å·®
        ax1 = axes[0]
        bars = ax1.bar(x, explained_variance_ratio * 100, 
                      color=self.colors[0], edgecolor='white', linewidth=2)
        ax1.set_xlabel('ä¸»æˆåˆ†', fontweight='bold')
        ax1.set_ylabel('æ–¹å·®è§£é‡Šç‡ (%)', fontweight='bold')
        ax1.set_title('(a) å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šç‡', fontweight='bold')
        ax1.set_xticks(x)
        
        for bar, val in zip(bars, explained_variance_ratio):
            if val > 0.02:
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{val*100:.1f}%', ha='center', fontsize=9)
        
        # ç´¯è®¡æ–¹å·®
        ax2 = axes[1]
        ax2.plot(x, cumulative * 100, 'o-', color=self.colors[1], 
                linewidth=2.5, markersize=8)
        ax2.axhline(y=95, color='red', linestyle='--', label='95%é˜ˆå€¼')
        ax2.axhline(y=90, color='orange', linestyle='--', label='90%é˜ˆå€¼')
        ax2.fill_between(x, cumulative * 100, alpha=0.3, color=self.colors[1])
        ax2.set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontweight='bold')
        ax2.set_ylabel('ç´¯è®¡æ–¹å·®è§£é‡Šç‡ (%)', fontweight='bold')
        ax2.set_title('(b) ç´¯è®¡æ–¹å·®è§£é‡Šç‡', fontweight='bold')
        ax2.set_xticks(x)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.suptitle(title, fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_pca_scatter(self, pca_result, labels=None, 
                         title="PCAé™ç»´æ•£ç‚¹å›¾", save_path=None):
        """ç»˜åˆ¶PCAé™ç»´åçš„æ•£ç‚¹å›¾ï¼ˆ2Dæˆ–3Dï¼‰"""
        n_components = min(pca_result.shape[1], 3)
        
        if n_components == 2:
            fig, ax = plt.subplots(figsize=(10, 8))
            
            if labels is not None:
                unique_labels = np.unique(labels)
                for i, label in enumerate(unique_labels):
                    mask = labels == label
                    ax.scatter(pca_result.iloc[mask, 0], pca_result.iloc[mask, 1],
                              s=50, alpha=0.7, c=self.colors[i % len(self.colors)],
                              label=f'ç±»åˆ« {label}', edgecolors='white')
                ax.legend()
            else:
                ax.scatter(pca_result.iloc[:, 0], pca_result.iloc[:, 1],
                          s=50, alpha=0.7, c=self.colors[0], edgecolors='white')
            
            ax.set_xlabel('PC1', fontweight='bold')
            ax.set_ylabel('PC2', fontweight='bold')
            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        elif n_components >= 3:
            fig = plt.figure(figsize=(12, 9))
            ax = fig.add_subplot(111, projection='3d')
            
            if labels is not None:
                unique_labels = np.unique(labels)
                for i, label in enumerate(unique_labels):
                    mask = labels == label
                    ax.scatter(pca_result.iloc[mask, 0], 
                              pca_result.iloc[mask, 1],
                              pca_result.iloc[mask, 2],
                              s=50, alpha=0.7, c=self.colors[i % len(self.colors)],
                              label=f'ç±»åˆ« {label}')
                ax.legend()
            else:
                ax.scatter(pca_result.iloc[:, 0], 
                          pca_result.iloc[:, 1],
                          pca_result.iloc[:, 2],
                          s=50, alpha=0.7, c=self.colors[0])
            
            ax.set_xlabel('PC1')
            ax.set_ylabel('PC2')
            ax.set_zlabel('PC3')
            ax.set_title(title, fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_correlation_heatmap(self, data, title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾", save_path=None):
        """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        corr = data.corr()
        im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)
        
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=45, ha='right')
        ax.set_yticklabels(corr.columns)
        
        # æ·»åŠ æ•°å€¼
        for i in range(len(corr)):
            for j in range(len(corr)):
                text = ax.text(j, i, f'{corr.iloc[i, j]:.2f}',
                              ha='center', va='center',
                              color='white' if abs(corr.iloc[i, j]) > 0.5 else 'black',
                              fontsize=8)
        
        ax.set_title(title, fontsize=14, fontweight='bold')
        plt.colorbar(im, ax=ax, label='ç›¸å…³ç³»æ•°')
        
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()


# ============================================================
# ç¬¬ä¹éƒ¨åˆ†ï¼šä¸»ç¨‹åºä¸å®Œæ•´ç¤ºä¾‹ (Main Program)
# ============================================================

if __name__ == "__main__":
    
    print("\n" + "="*70)
    print("   DATA PREPROCESSING & FEATURE ENGINEERING FOR MCM/ICM")
    print("   æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹å·¥å…·é›†")
    print("   Extended Version with Visualization")
    print("="*70)
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    ğŸ”§ æ•°æ®é¢„å¤„ç†æµç¨‹                              â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                  â•‘
    â•‘   [ç¼ºå¤±å€¼å¤„ç†]                                                    â•‘
    â•‘      â”œâ”€ å‡å€¼/ä¸­ä½æ•°/ä¼—æ•°å¡«å……                                      â•‘
    â•‘      â”œâ”€ KNNæ’è¡¥                                                  â•‘
    â•‘      â””â”€ åˆ é™¤æ³•                                                   â•‘
    â•‘                                                                  â•‘
    â•‘   [å¼‚å¸¸å€¼æ£€æµ‹]                                                    â•‘
    â•‘      â”œâ”€ IQRæ–¹æ³•: å››åˆ†ä½è·                                        â•‘
    â•‘      â”œâ”€ Z-scoreæ–¹æ³•: æ ‡å‡†åŒ–å¾—åˆ†                                  â•‘
    â•‘      â””â”€ å¤„ç†: åˆ é™¤/æˆªæ–­/æ›¿æ¢                                      â•‘
    â•‘                                                                  â•‘
    â•‘   [æ ‡å‡†åŒ–]                                                        â•‘
    â•‘      â”œâ”€ Z-score: (x-mean)/std                                    â•‘
    â•‘      â”œâ”€ Min-Max: ç¼©æ”¾åˆ°[0,1]                                     â•‘
    â•‘      â””â”€ Robust: é²æ£’æ ‡å‡†åŒ–                                       â•‘
    â•‘                                                                  â•‘
    â•‘   [é™ç»´]                                                          â•‘
    â•‘      â””â”€ PCA: ä¸»æˆåˆ†åˆ†æ                                          â•‘
    â•‘                                                                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    generator = SampleDataGenerator(random_seed=2026)
    visualizer = PreprocessingVisualizer()
    
    # ================================================================
    # ç¤ºä¾‹1ï¼šç¼ºå¤±å€¼å¤„ç†
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 1: ç¼ºå¤±å€¼æ£€æµ‹ä¸å¤„ç†")
    print("="*70)
    
    data_info = generator.generate_with_missing_and_outliers(
        n_samples=200, n_features=6, missing_rate=0.1, outlier_rate=0.05
    )
    data = data_info['data']
    
    print(f"\nç”Ÿæˆæ•°æ®:")
    print(f"  æ ·æœ¬æ•°: {data_info['n_samples']}")
    print(f"  ç‰¹å¾æ•°: {data_info['n_features']}")
    
    handler = MissingValueHandler(verbose=True)
    handler.analyze_missing(data)
    
    visualizer.plot_missing_heatmap(data, title="ç¼ºå¤±å€¼åˆ†å¸ƒçƒ­åŠ›å›¾")
    
    # å¡«å……ç¼ºå¤±å€¼
    filled_data = handler.fill_missing(data, method='knn', n_neighbors=5)
    
    # ================================================================
    # ç¤ºä¾‹2ï¼šå¼‚å¸¸å€¼æ£€æµ‹
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 2: å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†")
    print("="*70)
    
    detector = OutlierDetector(verbose=True)
    outlier_mask = detector.detect_iqr(filled_data, factor=1.5)
    
    visualizer.plot_boxplot(filled_data, title="å¼‚å¸¸å€¼æ£€æµ‹ç®±çº¿å›¾")
    
    # å¤„ç†å¼‚å¸¸å€¼
    cleaned_data = detector.handle_outliers(filled_data, method='clip')
    
    visualizer.plot_boxplot(cleaned_data, title="å¤„ç†åçš„æ•°æ®ç®±çº¿å›¾")
    
    # ================================================================
    # ç¤ºä¾‹3ï¼šæ•°æ®æ ‡å‡†åŒ–
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 3: æ•°æ®æ ‡å‡†åŒ–")
    print("="*70)
    
    scaler = DataScaler(verbose=True)
    
    # Z-scoreæ ‡å‡†åŒ–
    scaled_standard = scaler.fit_transform(cleaned_data, method='standard')
    print(f"\nZ-scoreæ ‡å‡†åŒ–åç»Ÿè®¡:")
    print(f"  å‡å€¼èŒƒå›´: [{scaled_standard.mean().min():.4f}, {scaled_standard.mean().max():.4f}]")
    print(f"  æ ‡å‡†å·®èŒƒå›´: [{scaled_standard.std().min():.4f}, {scaled_standard.std().max():.4f}]")
    
    # Min-Maxæ ‡å‡†åŒ–
    scaled_minmax = scaler.fit_transform(cleaned_data, method='minmax')
    print(f"\nMin-Maxæ ‡å‡†åŒ–åèŒƒå›´:")
    print(f"  æœ€å°å€¼: {scaled_minmax.min().min():.4f}")
    print(f"  æœ€å¤§å€¼: {scaled_minmax.max().max():.4f}")
    
    # ================================================================
    # ç¤ºä¾‹4ï¼šPCAé™ç»´
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 4: PCAä¸»æˆåˆ†åˆ†æ")
    print("="*70)
    
    # ç”Ÿæˆé«˜ç»´æ•°æ®
    high_dim_data = generator.generate_high_dimensional(
        n_samples=300, n_features=20, n_informative=5, n_redundant=5
    )
    
    print(f"\né«˜ç»´æ•°æ®:")
    print(f"  æ ·æœ¬æ•°: 300")
    print(f"  ç‰¹å¾æ•°: 20 (ä¿¡æ¯ç‰¹å¾5, å†—ä½™ç‰¹å¾5, å™ªå£°10)")
    
    # è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°
    pca = PCAReducer(verbose=True)
    n_optimal, cumulative = pca.select_n_components(
        high_dim_data['data'], target_variance=0.95
    )
    
    # æ‰§è¡ŒPCA
    pca = PCAReducer(n_components=n_optimal, verbose=True)
    pca_result = pca.fit_transform(high_dim_data['data'])
    
    visualizer.plot_pca_variance(
        pca.explained_variance_ratio,
        title="PCAæ–¹å·®è§£é‡Šç‡åˆ†æ"
    )
    
    visualizer.plot_pca_scatter(
        pca_result,
        title="PCAé™ç»´ç»“æœ (2D)"
    )
    
    # è½½è·çŸ©é˜µ
    loadings = pca.get_loadings()
    print("\nä¸»æˆåˆ†è½½è·çŸ©é˜µ (å‰5ç‰¹å¾):")
    print(loadings.head())
    
    # ================================================================
    # ç¤ºä¾‹5ï¼šç‰¹å¾é€‰æ‹©
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“ EXAMPLE 5: ç‰¹å¾é€‰æ‹©")
    print("="*70)
    
    # ç›¸å…³æ€§çƒ­åŠ›å›¾
    visualizer.plot_correlation_heatmap(
        cleaned_data,
        title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"
    )
    
    selector = FeatureSelector(verbose=True)
    
    # æ–¹å·®é˜ˆå€¼
    selected_var = selector.select_by_variance(cleaned_data, threshold=1.0)
    
    # ç›¸å…³æ€§è¿‡æ»¤
    selected_corr = selector.select_by_correlation(cleaned_data, threshold=0.8)
    
    # ================================================================
    # ä½¿ç”¨è¯´æ˜
    # ================================================================
    print("\n" + "="*70)
    print("ğŸ“– ä½¿ç”¨è¯´æ˜ (Usage Guide)")
    print("="*70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   æ•°æ®é¢„å¤„ç†å·¥å…·ä½¿ç”¨æŒ‡å—                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ã€ç¼ºå¤±å€¼å¤„ç†ã€‘
    
    handler = MissingValueHandler()
    handler.analyze_missing(data)               # åˆ†æ
    filled = handler.fill_missing(data, 'knn')  # KNNå¡«å……
    cleaned = handler.drop_missing(data)        # åˆ é™¤
    
    ã€å¼‚å¸¸å€¼å¤„ç†ã€‘
    
    detector = OutlierDetector()
    mask = detector.detect_iqr(data, factor=1.5)   # IQRæ£€æµ‹
    mask = detector.detect_zscore(data, threshold=3)  # Z-scoreæ£€æµ‹
    
    cleaned = detector.handle_outliers(data, 'clip')  # æˆªæ–­å¤„ç†
    cleaned = detector.handle_outliers(data, 'remove')  # åˆ é™¤å¤„ç†
    
    ã€æ•°æ®æ ‡å‡†åŒ–ã€‘
    
    scaler = DataScaler()
    scaled = scaler.fit_transform(data, 'standard')  # Z-score
    scaled = scaler.fit_transform(data, 'minmax')    # Min-Max
    scaled = scaler.fit_transform(data, 'robust')    # Robust
    
    ã€PCAé™ç»´ã€‘
    
    pca = PCAReducer(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
    pca = PCAReducer(n_components=3)     # ä¿ç•™3ä¸ªä¸»æˆåˆ†
    result = pca.fit_transform(data)
    loadings = pca.get_loadings()        # è·å–è½½è·çŸ©é˜µ
    
    ã€ç‰¹å¾é€‰æ‹©ã€‘
    
    selector = FeatureSelector()
    selected = selector.select_by_variance(data, threshold=0.01)
    selected = selector.select_by_correlation(data, threshold=0.9)
    selected, scores = selector.select_k_best(X, y, k=5)
    
    ã€è®ºæ–‡å›¾è¡¨å»ºè®®ã€‘
    
    Figure 1: ç¼ºå¤±å€¼åˆ†å¸ƒçƒ­åŠ›å›¾
    Figure 2: ç®±çº¿å›¾ï¼ˆå¼‚å¸¸å€¼ï¼‰
    Figure 3: PCAæ–¹å·®è§£é‡Šç‡å›¾
    Figure 4: PCAé™ç»´æ•£ç‚¹å›¾
    Figure 5: ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
    
    Table 1: æ•°æ®åŸºæœ¬ä¿¡æ¯
    Table 2: ç¼ºå¤±å€¼ç»Ÿè®¡
    Table 3: PCAä¸»æˆåˆ†è´¡çŒ®ç‡
    """)
    
    print("\n" + "="*70)
    print("   âœ… All examples completed successfully!")
    print("   ğŸ’¡ Use the above code templates for your MCM/ICM paper")
    print("="*70)
