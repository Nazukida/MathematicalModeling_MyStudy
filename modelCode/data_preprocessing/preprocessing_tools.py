"""
============================================================
æ•°æ®é¢„å¤„ç†å·¥å…·åŒ… (Data Preprocessing Toolkit)
é€‚ç”¨äºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ› (MCM/ICM)
============================================================
åŠŸèƒ½ï¼šæ•°æ®æ¸…æ´—ã€æ ‡å‡†åŒ–ã€å¼‚å¸¸å€¼å¤„ç†ã€ç¼ºå¤±å€¼å¡«å……
ä½œè€…ï¼šMCM/ICM Team
æ—¥æœŸï¼š2026å¹´1æœˆ
============================================================
"""

import numpy as np
import pandas as pd
import warnings
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import SimpleImputer, KNNImputer

warnings.filterwarnings('ignore')


class DataCleaner:
    """
    æ•°æ®æ¸…æ´—å·¥å…·ç±»
    
    åŠŸèƒ½ï¼š
    - ç¼ºå¤±å€¼æ£€æµ‹ä¸å¡«å……
    - é‡å¤å€¼å¤„ç†
    - æ•°æ®ç±»å‹è½¬æ¢
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.cleaning_report = {}
        
    def check_quality(self, df):
        """
        æ£€æŸ¥æ•°æ®è´¨é‡
        
        :param df: DataFrame
        :return: è´¨é‡æŠ¥å‘Šå­—å…¸
        """
        report = {
            'shape': df.shape,
            'missing_count': df.isnull().sum().to_dict(),
            'missing_percent': (df.isnull().sum() / len(df) * 100).to_dict(),
            'duplicates': df.duplicated().sum(),
            'dtypes': df.dtypes.to_dict()
        }
        
        if self.verbose:
            print("\n" + "="*50)
            print("ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
            print("="*50)
            print(f"\n  æ•°æ®ç»´åº¦: {report['shape'][0]} è¡Œ Ã— {report['shape'][1]} åˆ—")
            print(f"  é‡å¤è¡Œæ•°: {report['duplicates']}")
            print("\n  ç¼ºå¤±å€¼ç»Ÿè®¡:")
            for col, cnt in report['missing_count'].items():
                if cnt > 0:
                    pct = report['missing_percent'][col]
                    print(f"    {col}: {cnt} ({pct:.1f}%)")
            print("="*50)
            
        self.cleaning_report = report
        return report
    
    def fill_missing(self, df, method='auto', columns=None):
        """
        å¡«å……ç¼ºå¤±å€¼
        
        :param df: DataFrame
        :param method: 'auto'/'mean'/'median'/'mode'/'ffill'/'bfill'/'knn'
        :param columns: æŒ‡å®šåˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—
        :return: å¡«å……åçš„DataFrame
        """
        df_filled = df.copy()
        
        if columns is None:
            columns = df.columns[df.isnull().any()].tolist()
            
        for col in columns:
            if col not in df.columns:
                continue
                
            if method == 'auto':
                # æ•°å€¼å‹ç”¨ä¸­ä½æ•°ï¼Œåˆ†ç±»å‹ç”¨ä¼—æ•°
                if pd.api.types.is_numeric_dtype(df[col]):
                    df_filled[col].fillna(df[col].median(), inplace=True)
                else:
                    df_filled[col].fillna(df[col].mode()[0], inplace=True)
            elif method == 'mean':
                df_filled[col].fillna(df[col].mean(), inplace=True)
            elif method == 'median':
                df_filled[col].fillna(df[col].median(), inplace=True)
            elif method == 'mode':
                df_filled[col].fillna(df[col].mode()[0], inplace=True)
            elif method == 'ffill':
                df_filled[col].fillna(method='ffill', inplace=True)
            elif method == 'bfill':
                df_filled[col].fillna(method='bfill', inplace=True)
            elif method == 'knn':
                # KNNå¡«å……ï¼ˆä»…æ•°å€¼åˆ—ï¼‰
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if col in numeric_cols:
                    imputer = KNNImputer(n_neighbors=5)
                    df_filled[numeric_cols] = imputer.fit_transform(df[numeric_cols])
                    
        if self.verbose:
            remaining = df_filled.isnull().sum().sum()
            print(f"\n  âœ… ç¼ºå¤±å€¼å¡«å……å®Œæˆï¼Œå‰©ä½™ç¼ºå¤±: {remaining}")
            
        return df_filled
    
    def remove_duplicates(self, df, keep='first'):
        """ç§»é™¤é‡å¤è¡Œ"""
        n_before = len(df)
        df_clean = df.drop_duplicates(keep=keep)
        n_removed = n_before - len(df_clean)
        
        if self.verbose:
            print(f"\n  âœ… ç§»é™¤é‡å¤è¡Œ: {n_removed} è¡Œ")
            
        return df_clean


class DataScaler:
    """
    æ•°æ®æ ‡å‡†åŒ–/å½’ä¸€åŒ–å·¥å…·ç±»
    
    æ”¯æŒæ–¹æ³•ï¼š
    - standard: Z-scoreæ ‡å‡†åŒ– (x - mean) / std
    - minmax: æœ€å°æœ€å¤§å½’ä¸€åŒ– [0, 1]
    - robust: é²æ£’æ ‡å‡†åŒ–ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
    """
    
    def __init__(self, method='standard', verbose=True):
        """
        :param method: 'standard' / 'minmax' / 'robust'
        """
        self.method = method
        self.verbose = verbose
        
        if method == 'standard':
            self.scaler = StandardScaler()
        elif method == 'minmax':
            self.scaler = MinMaxScaler()
        elif method == 'robust':
            self.scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
            
        self.is_fitted = False
        self.feature_names = None
        
    def fit_transform(self, X):
        """æ‹Ÿåˆå¹¶è½¬æ¢"""
        if isinstance(X, pd.DataFrame):
            self.feature_names = list(X.columns)
            X_scaled = self.scaler.fit_transform(X)
            X_scaled = pd.DataFrame(X_scaled, columns=self.feature_names, index=X.index)
        else:
            X_scaled = self.scaler.fit_transform(X)
            
        self.is_fitted = True
        
        if self.verbose:
            print(f"\n  âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ (æ–¹æ³•: {self.method})")
            
        return X_scaled
    
    def transform(self, X):
        """è½¬æ¢æ–°æ•°æ®"""
        if not self.is_fitted:
            raise ValueError("è¯·å…ˆè°ƒç”¨ fit_transform()")
            
        if isinstance(X, pd.DataFrame):
            X_scaled = self.scaler.transform(X)
            return pd.DataFrame(X_scaled, columns=self.feature_names, index=X.index)
        return self.scaler.transform(X)
    
    def inverse_transform(self, X):
        """é€†è½¬æ¢"""
        if isinstance(X, pd.DataFrame):
            X_inv = self.scaler.inverse_transform(X)
            return pd.DataFrame(X_inv, columns=self.feature_names, index=X.index)
        return self.scaler.inverse_transform(X)


class OutlierDetector:
    """
    å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†ç±»
    
    æ”¯æŒæ–¹æ³•ï¼š
    - IQR: å››åˆ†ä½è·æ³•
    - zscore: Zåˆ†æ•°æ³•
    - isolation: å­¤ç«‹æ£®æ—ï¼ˆéœ€è¦sklearnï¼‰
    """
    
    def __init__(self, method='iqr', threshold=1.5, verbose=True):
        """
        :param method: 'iqr' / 'zscore' / 'isolation'
        :param threshold: IQRçš„kå€¼ æˆ– zscoreé˜ˆå€¼
        """
        self.method = method
        self.threshold = threshold
        self.verbose = verbose
        self.outlier_info = {}
        
    def detect(self, data, column=None):
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        
        :param data: DataFrame, Series æˆ– array
        :param column: åˆ—åï¼ˆDataFrameæ—¶ä½¿ç”¨ï¼‰
        :return: å¸ƒå°”æ©ç ï¼ˆTrue=å¼‚å¸¸å€¼ï¼‰
        """
        # å¤„ç†è¾“å…¥
        if isinstance(data, pd.DataFrame):
            if column is None:
                column = data.columns[0]
            values = data[column].values
        elif isinstance(data, pd.Series):
            values = data.values
            column = data.name or 'value'
        else:
            values = np.array(data)
            column = 'value'
            
        # æ£€æµ‹
        if self.method == 'iqr':
            Q1, Q3 = np.percentile(values, [25, 75])
            IQR = Q3 - Q1
            lower = Q1 - self.threshold * IQR
            upper = Q3 + self.threshold * IQR
            mask = (values < lower) | (values > upper)
            self.outlier_info = {'Q1': Q1, 'Q3': Q3, 'IQR': IQR, 
                                'lower': lower, 'upper': upper}
                                
        elif self.method == 'zscore':
            from scipy import stats
            z_scores = np.abs(stats.zscore(values))
            mask = z_scores > self.threshold
            self.outlier_info = {'threshold': self.threshold}
            
        elif self.method == 'isolation':
            from sklearn.ensemble import IsolationForest
            iso = IsolationForest(contamination=0.1, random_state=42)
            predictions = iso.fit_predict(values.reshape(-1, 1))
            mask = predictions == -1
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")
            
        if self.verbose:
            n_outliers = mask.sum()
            pct = n_outliers / len(values) * 100
            print(f"\n  ğŸ” å¼‚å¸¸å€¼æ£€æµ‹ ({self.method}): å‘ç° {n_outliers} ä¸ª ({pct:.1f}%)")
            
        return mask
    
    def remove(self, data, column=None):
        """ç§»é™¤å¼‚å¸¸å€¼"""
        mask = self.detect(data, column)
        
        if isinstance(data, (pd.DataFrame, pd.Series)):
            return data[~mask]
        return data[~mask]
    
    def replace(self, data, column=None, method='median'):
        """
        æ›¿æ¢å¼‚å¸¸å€¼
        
        :param method: 'median' / 'mean' / 'clip'
        """
        mask = self.detect(data, column)
        
        if isinstance(data, pd.DataFrame):
            df = data.copy()
            col = column or data.columns[0]
            values = df[col].values.copy()
        else:
            values = np.array(data).copy()
            
        if method == 'median':
            replacement = np.median(values[~mask])
            values[mask] = replacement
        elif method == 'mean':
            replacement = np.mean(values[~mask])
            values[mask] = replacement
        elif method == 'clip':
            if self.method == 'iqr':
                values = np.clip(values, self.outlier_info['lower'], self.outlier_info['upper'])
            else:
                values[mask] = np.median(values[~mask])
                
        if isinstance(data, pd.DataFrame):
            df[col] = values
            return df
        return values


class FeatureSelector:
    """
    ç‰¹å¾é€‰æ‹©å·¥å…·ç±»
    
    æ”¯æŒæ–¹æ³•ï¼š
    - variance: æ–¹å·®è¿‡æ»¤
    - correlation: ç›¸å…³æ€§è¿‡æ»¤
    - mutual_info: äº’ä¿¡æ¯
    """
    
    def __init__(self, method='correlation', threshold=0.9, verbose=True):
        """
        :param method: 'variance' / 'correlation' / 'mutual_info'
        :param threshold: è¿‡æ»¤é˜ˆå€¼
        """
        self.method = method
        self.threshold = threshold
        self.verbose = verbose
        self.selected_features = None
        self.dropped_features = None
        
    def fit_transform(self, X, y=None):
        """
        é€‰æ‹©ç‰¹å¾
        
        :param X: ç‰¹å¾DataFrame
        :param y: æ ‡ç­¾ï¼ˆäº’ä¿¡æ¯æ—¶ä½¿ç”¨ï¼‰
        :return: ç­›é€‰åçš„ç‰¹å¾
        """
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=[f'f{i}' for i in range(X.shape[1])])
            
        if self.method == 'variance':
            # ç§»é™¤ä½æ–¹å·®ç‰¹å¾
            variances = X.var()
            mask = variances > self.threshold
            self.selected_features = list(X.columns[mask])
            self.dropped_features = list(X.columns[~mask])
            
        elif self.method == 'correlation':
            # ç§»é™¤é«˜ç›¸å…³ç‰¹å¾
            corr_matrix = X.corr().abs()
            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
            to_drop = [col for col in upper.columns if any(upper[col] > self.threshold)]
            self.dropped_features = to_drop
            self.selected_features = [col for col in X.columns if col not in to_drop]
            
        elif self.method == 'mutual_info':
            if y is None:
                raise ValueError("äº’ä¿¡æ¯æ–¹æ³•éœ€è¦æä¾› y")
            from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
            
            # è‡ªåŠ¨åˆ¤æ–­åˆ†ç±»/å›å½’
            if len(np.unique(y)) < 10:
                mi = mutual_info_classif(X, y)
            else:
                mi = mutual_info_regression(X, y)
                
            mask = mi > self.threshold
            self.selected_features = list(X.columns[mask])
            self.dropped_features = list(X.columns[~mask])
            
        if self.verbose:
            print(f"\n  âœ… ç‰¹å¾é€‰æ‹© ({self.method})")
            print(f"     ä¿ç•™ç‰¹å¾: {len(self.selected_features)}")
            print(f"     ç§»é™¤ç‰¹å¾: {len(self.dropped_features)}")
            if self.dropped_features:
                print(f"     ç§»é™¤åˆ—: {self.dropped_features[:5]}{'...' if len(self.dropped_features) > 5 else ''}")
                
        return X[self.selected_features]


# ä¾¿æ·å‡½æ•°
def quick_preprocess(df, fill_missing='auto', scale='standard', remove_outliers=True):
    """
    å¿«é€Ÿæ•°æ®é¢„å¤„ç†
    
    :param df: åŸå§‹DataFrame
    :param fill_missing: ç¼ºå¤±å€¼å¡«å……æ–¹æ³•
    :param scale: æ ‡å‡†åŒ–æ–¹æ³• ('standard'/'minmax'/None)
    :param remove_outliers: æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼
    :return: é¢„å¤„ç†åçš„DataFrame
    """
    print("\n" + "="*50)
    print("ğŸ”§ å¿«é€Ÿæ•°æ®é¢„å¤„ç†")
    print("="*50)
    
    # 1. æ•°æ®æ¸…æ´—
    cleaner = DataCleaner(verbose=True)
    cleaner.check_quality(df)
    df = cleaner.fill_missing(df, method=fill_missing)
    df = cleaner.remove_duplicates(df)
    
    # 2. å¼‚å¸¸å€¼å¤„ç†
    if remove_outliers:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        detector = OutlierDetector(method='iqr', verbose=False)
        for col in numeric_cols:
            df = detector.replace(df, column=col, method='clip')
        print(f"\n  âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")
    
    # 3. æ ‡å‡†åŒ–
    if scale:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        scaler = DataScaler(method=scale, verbose=True)
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
        
    print("\n" + "="*50)
    print("âœ… é¢„å¤„ç†å®Œæˆ!")
    print("="*50)
    
    return df


if __name__ == "__main__":
    # æ¼”ç¤º
    print("="*60)
    print("ğŸ“Š æ•°æ®é¢„å¤„ç†å·¥å…·æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n = 100
    df = pd.DataFrame({
        'ç‰¹å¾1': np.random.randn(n) * 10 + 50,
        'ç‰¹å¾2': np.random.randn(n) * 5 + 30,
        'ç‰¹å¾3': np.random.randn(n) * 15 + 100,
    })
    
    # æ·»åŠ ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    df.loc[5:10, 'ç‰¹å¾1'] = np.nan
    df.loc[0, 'ç‰¹å¾2'] = 200  # å¼‚å¸¸å€¼
    
    # å¿«é€Ÿé¢„å¤„ç†
    df_clean = quick_preprocess(df)
    print("\nå¤„ç†åæ•°æ®é¢„è§ˆ:")
    print(df_clean.head())
